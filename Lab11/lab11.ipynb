{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "316a7144-5882-4e5d-85e4-25a791c90283",
   "metadata": {},
   "source": [
    "# Laboratorium 11: Strojenie hiperparametrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45067640-7c5d-4180-ae9c-af483ce8527e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15b3ae6-fdcf-4e36-89b7-74b9bea1eefa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Poszukiwania reczne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251db0c9-f905-4a65-bafa-0bf946892069",
   "metadata": {},
   "source": [
    "Zestaw danych **Boston Housing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78c2beae-df52-4a21-badb-73e66dac2125",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = keras.datasets.boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d16a7ba-3507-4f9b-b924-817de6c91bce",
   "metadata": {},
   "source": [
    "funkcja budujaca model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcfe1908-3124-4826-a7a4-3de32781505b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_hidden = 1, n_neurons = 25, optimizer = \"sgd\",\n",
    "                learning_rate = 1e-5, momentum=0): \n",
    "    \n",
    "    if optimizer == \"sgd\": optimizer = keras.optimizers.SGD(learning_rate = learning_rate)\n",
    "    elif optimizer == \"nesterov\": optimizer = keras.optimizers.SGD(learning_rate=learning_rate,\n",
    "                                                                     nesterov = True)\n",
    "    elif optimizer == \"momentum\" : optimizer = keras.optimizers.SGD(learning_rate=learning_rate,\n",
    "                                                                      momentum=momentum)\n",
    "    elif optimizer == \"adam\": optimizer = keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "                                                                     \n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(keras.layers.Flatten(input_shape=X_train.shape[1:]))\n",
    "    for i in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "        \n",
    "    model.compile(loss=\"mse\", optimizer = optimizer, metrics=[\"mse\", \"mae\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2407430-d245-4885-9c88-130634573874",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_list = [1e-6, 1e-5, 1e-4]\n",
    "hl_list = [0, 1, 2, 3]\n",
    "nn_list = [5, 25, 125]\n",
    "opt_list = [\"sgd\", \"nesterov\", \"momentum\", \"adam\"]\n",
    "mom_list = [0.1, 0.5, 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "135062bd-e7cb-4c49-8178-676242640b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "root_logdir = os.path.join(os.curdir, \"tb_logs\")\n",
    "\n",
    "def get_run_logdir(name, value): \n",
    "    import time\n",
    "    unixtime = int(time.time())\n",
    "    run_id = \"_\".join((str(unixtime), name, str(value)))\n",
    "    return os.path.join(root_logdir, run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11fde297-9bab-4127-b5cb-f6f9e8ffb2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_pickle(name, values):\n",
    "    import pickle\n",
    "    name = \".\".join((name,\"pkl\"))\n",
    "    with open(name,'wb') as f: pickle.dump(values, f)\n",
    "\n",
    "    #sprawdzenie zawartosci pliku\n",
    "    with open(name,'rb') as f: print(pickle.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b846501f-d6d3-4ced-bcfb-9e6b6bc65539",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### eksperymetr 1: krok uczenia (lr): 10−6, 10−5, 10−4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14cbe273-210a-4ef3-9c78-d0102693a11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-04 13:49:40.436355: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 2ms/step - loss: 2700.2764 - mse: 2700.2764 - mae: 31.1550\n",
      "Epoch 2/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 246.2086 - mse: 246.2086 - mae: 11.9515\n",
      "Epoch 3/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 189.4720 - mse: 189.4720 - mae: 10.5609\n",
      "Epoch 4/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 175.1508 - mse: 175.1508 - mae: 10.1026\n",
      "Epoch 5/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 167.5972 - mse: 167.5972 - mae: 9.9639\n",
      "Epoch 6/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 154.0362 - mse: 154.0362 - mae: 9.4851\n",
      "Epoch 7/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 147.4924 - mse: 147.4924 - mae: 9.3873\n",
      "Epoch 8/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 150.3802 - mse: 150.3802 - mae: 9.4128\n",
      "Epoch 9/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 135.4045 - mse: 135.4045 - mae: 8.7718\n",
      "Epoch 10/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 137.8131 - mse: 137.8131 - mae: 8.9101\n",
      "Epoch 11/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 131.7026 - mse: 131.7026 - mae: 8.5841\n",
      "Epoch 12/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 124.7457 - mse: 124.7457 - mae: 8.2919\n",
      "Epoch 13/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 126.5832 - mse: 126.5832 - mae: 8.4254\n",
      "Epoch 14/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 118.3163 - mse: 118.3163 - mae: 8.0485\n",
      "Epoch 15/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 118.1022 - mse: 118.1022 - mae: 8.1325\n",
      "Epoch 16/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 114.8265 - mse: 114.8265 - mae: 7.8007\n",
      "Epoch 17/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 111.6644 - mse: 111.6644 - mae: 7.7476\n",
      "Epoch 18/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 109.4295 - mse: 109.4295 - mae: 7.7427\n",
      "Epoch 19/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 105.5968 - mse: 105.5968 - mae: 7.5306\n",
      "Epoch 20/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 104.7179 - mse: 104.7179 - mae: 7.4836\n",
      "Epoch 21/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 110.3975 - mse: 110.3975 - mae: 7.7143\n",
      "Epoch 22/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 105.3935 - mse: 105.3935 - mae: 7.5429\n",
      "Epoch 23/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 100.4650 - mse: 100.4650 - mae: 7.3403\n",
      "Epoch 24/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 104.7742 - mse: 104.7742 - mae: 7.5095\n",
      "Epoch 25/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 98.5442 - mse: 98.5442 - mae: 7.1825\n",
      "Epoch 26/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 100.3322 - mse: 100.3322 - mae: 7.1872\n",
      "Epoch 27/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 98.6378 - mse: 98.6378 - mae: 7.1914\n",
      "Epoch 28/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 97.9508 - mse: 97.9508 - mae: 7.3227\n",
      "Epoch 29/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 96.1474 - mse: 96.1474 - mae: 7.1057\n",
      "Epoch 30/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 94.7156 - mse: 94.7156 - mae: 6.9807\n",
      "Epoch 31/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 93.1845 - mse: 93.1845 - mae: 6.9716\n",
      "Epoch 32/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 92.1447 - mse: 92.1447 - mae: 6.9325\n",
      "Epoch 33/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 92.4683 - mse: 92.4683 - mae: 6.8624\n",
      "Epoch 34/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 93.1582 - mse: 93.1582 - mae: 7.0072\n",
      "Epoch 35/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 90.9753 - mse: 90.9753 - mae: 6.8307\n",
      "Epoch 36/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 88.8884 - mse: 88.8884 - mae: 6.7291\n",
      "Epoch 37/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 91.0245 - mse: 91.0245 - mae: 6.8429\n",
      "Epoch 38/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 91.4363 - mse: 91.4363 - mae: 6.7758\n",
      "Epoch 39/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 90.6643 - mse: 90.6643 - mae: 6.7506\n",
      "Epoch 40/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 93.2220 - mse: 93.2220 - mae: 6.9624\n",
      "Epoch 41/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 91.9573 - mse: 91.9573 - mae: 6.8036\n",
      "Epoch 42/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 87.8183 - mse: 87.8183 - mae: 6.8101\n",
      "Epoch 43/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 89.7829 - mse: 89.7829 - mae: 6.7460\n",
      "Epoch 44/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 88.2174 - mse: 88.2174 - mae: 6.6701\n",
      "Epoch 45/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 86.6316 - mse: 86.6316 - mae: 6.5987\n",
      "Epoch 46/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 88.4890 - mse: 88.4890 - mae: 6.7195\n",
      "Epoch 47/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 92.0158 - mse: 92.0158 - mae: 6.8426\n",
      "Epoch 48/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 88.9455 - mse: 88.9455 - mae: 6.6787\n",
      "Epoch 49/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 89.1505 - mse: 89.1505 - mae: 6.6817\n",
      "Epoch 50/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 85.9303 - mse: 85.9303 - mae: 6.5408\n",
      "Epoch 51/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 83.5981 - mse: 83.5981 - mae: 6.5098\n",
      "Epoch 52/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 85.1097 - mse: 85.1097 - mae: 6.4690\n",
      "Epoch 53/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 87.2357 - mse: 87.2357 - mae: 6.7274\n",
      "Epoch 54/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 85.3174 - mse: 85.3174 - mae: 6.5227\n",
      "Epoch 55/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 85.4054 - mse: 85.4054 - mae: 6.4979\n",
      "Epoch 56/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 85.9790 - mse: 85.9790 - mae: 6.5780\n",
      "Epoch 57/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 83.0296 - mse: 83.0296 - mae: 6.3579\n",
      "Epoch 58/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 86.5088 - mse: 86.5088 - mae: 6.5503\n",
      "Epoch 59/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 85.2681 - mse: 85.2681 - mae: 6.5949\n",
      "Epoch 60/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 83.7650 - mse: 83.7650 - mae: 6.4138\n",
      "Epoch 61/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 84.7476 - mse: 84.7476 - mae: 6.5023\n",
      "Epoch 61: early stopping\n",
      "Epoch 1/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1524.2535 - mse: 1524.2535 - mae: 20.8298\n",
      "Epoch 2/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 126.9034 - mse: 126.9034 - mae: 8.4635\n",
      "Epoch 3/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 107.8146 - mse: 107.8146 - mae: 7.8811\n",
      "Epoch 4/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 96.5704 - mse: 96.5704 - mae: 7.1098\n",
      "Epoch 5/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 88.4147 - mse: 88.4147 - mae: 6.8102\n",
      "Epoch 6/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 83.9571 - mse: 83.9571 - mae: 6.6934\n",
      "Epoch 7/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 78.5842 - mse: 78.5842 - mae: 6.2570\n",
      "Epoch 8/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 76.3205 - mse: 76.3205 - mae: 6.3244\n",
      "Epoch 9/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 74.8923 - mse: 74.8923 - mae: 6.1789\n",
      "Epoch 10/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 75.5893 - mse: 75.5893 - mae: 6.2002\n",
      "Epoch 11/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 75.0033 - mse: 75.0033 - mae: 6.1574\n",
      "Epoch 12/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 73.1394 - mse: 73.1394 - mae: 6.1501\n",
      "Epoch 13/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 74.9279 - mse: 74.9279 - mae: 6.1996\n",
      "Epoch 14/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 73.0146 - mse: 73.0146 - mae: 6.0660\n",
      "Epoch 15/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 73.1447 - mse: 73.1447 - mae: 6.1985\n",
      "Epoch 16/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 72.3123 - mse: 72.3123 - mae: 6.0121\n",
      "Epoch 17/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 72.2366 - mse: 72.2366 - mae: 6.0438\n",
      "Epoch 18/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 72.2005 - mse: 72.2005 - mae: 6.1173\n",
      "Epoch 19/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 71.3581 - mse: 71.3581 - mae: 5.9604\n",
      "Epoch 20/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 71.3467 - mse: 71.3467 - mae: 6.0408\n",
      "Epoch 21/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 71.5218 - mse: 71.5218 - mae: 5.9490\n",
      "Epoch 22/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 72.4559 - mse: 72.4559 - mae: 6.1169\n",
      "Epoch 23/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 70.5268 - mse: 70.5268 - mae: 5.9448\n",
      "Epoch 24/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 71.5256 - mse: 71.5256 - mae: 6.0391\n",
      "Epoch 25/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 70.0426 - mse: 70.0426 - mae: 5.9206\n",
      "Epoch 26/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 70.1493 - mse: 70.1493 - mae: 5.9529\n",
      "Epoch 27/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 69.4945 - mse: 69.4945 - mae: 5.9240\n",
      "Epoch 28/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 69.8161 - mse: 69.8161 - mae: 5.9589\n",
      "Epoch 29/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 69.9612 - mse: 69.9612 - mae: 5.9576\n",
      "Epoch 30/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 70.0430 - mse: 70.0430 - mae: 5.8966\n",
      "Epoch 31/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 68.9811 - mse: 68.9811 - mae: 5.8839\n",
      "Epoch 32/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 68.4325 - mse: 68.4325 - mae: 5.9038\n",
      "Epoch 33/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 67.7940 - mse: 67.7940 - mae: 5.7837\n",
      "Epoch 34/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 69.1919 - mse: 69.1919 - mae: 5.9848\n",
      "Epoch 35/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 68.0314 - mse: 68.0314 - mae: 5.8487\n",
      "Epoch 36/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 67.3443 - mse: 67.3443 - mae: 5.8387\n",
      "Epoch 37/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 67.8728 - mse: 67.8728 - mae: 5.8416\n",
      "Epoch 38/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 68.4668 - mse: 68.4668 - mae: 5.7831\n",
      "Epoch 39/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 67.6345 - mse: 67.6345 - mae: 5.8422\n",
      "Epoch 40/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 68.5339 - mse: 68.5339 - mae: 5.8298\n",
      "Epoch 41/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 66.6280 - mse: 66.6280 - mae: 5.6573\n",
      "Epoch 42/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 66.8349 - mse: 66.8349 - mae: 5.8833\n",
      "Epoch 43/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 65.8785 - mse: 65.8785 - mae: 5.7703\n",
      "Epoch 44/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 65.9889 - mse: 65.9889 - mae: 5.7853\n",
      "Epoch 45/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 65.6257 - mse: 65.6257 - mae: 5.7237\n",
      "Epoch 46/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 65.9952 - mse: 65.9952 - mae: 5.8150\n",
      "Epoch 47/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 66.4315 - mse: 66.4315 - mae: 5.7666\n",
      "Epoch 48/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 65.9395 - mse: 65.9395 - mae: 5.7119\n",
      "Epoch 49/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 66.3552 - mse: 66.3552 - mae: 5.7616\n",
      "Epoch 50/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 65.0863 - mse: 65.0863 - mae: 5.7470\n",
      "Epoch 51/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 64.6140 - mse: 64.6140 - mae: 5.7742\n",
      "Epoch 52/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 64.8338 - mse: 64.8338 - mae: 5.6567\n",
      "Epoch 53/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 65.8851 - mse: 65.8851 - mae: 5.8725\n",
      "Epoch 54/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 65.3151 - mse: 65.3151 - mae: 5.6995\n",
      "Epoch 55/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 64.3356 - mse: 64.3356 - mae: 5.7699\n",
      "Epoch 56/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 64.3643 - mse: 64.3643 - mae: 5.6946\n",
      "Epoch 57/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 64.0579 - mse: 64.0579 - mae: 5.6409\n",
      "Epoch 58/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 64.0276 - mse: 64.0276 - mae: 5.6530\n",
      "Epoch 59/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 63.8599 - mse: 63.8599 - mae: 5.6610\n",
      "Epoch 60/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 63.2993 - mse: 63.2993 - mae: 5.7268\n",
      "Epoch 61/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 63.7287 - mse: 63.7287 - mae: 5.6462\n",
      "Epoch 62/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 62.7508 - mse: 62.7508 - mae: 5.6640\n",
      "Epoch 63/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 62.9453 - mse: 62.9453 - mae: 5.5266\n",
      "Epoch 64/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 62.9544 - mse: 62.9544 - mae: 5.6884\n",
      "Epoch 65/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 63.9597 - mse: 63.9597 - mae: 5.7083\n",
      "Epoch 66/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 62.8322 - mse: 62.8322 - mae: 5.6089\n",
      "Epoch 67/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 62.9549 - mse: 62.9549 - mae: 5.6963\n",
      "Epoch 68/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 62.8526 - mse: 62.8526 - mae: 5.5722\n",
      "Epoch 69/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 61.8291 - mse: 61.8291 - mae: 5.5677\n",
      "Epoch 70/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 63.4475 - mse: 63.4475 - mae: 5.6542\n",
      "Epoch 71/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 60.9820 - mse: 60.9820 - mae: 5.5510\n",
      "Epoch 72/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 61.6298 - mse: 61.6298 - mae: 5.5066\n",
      "Epoch 73/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 61.2923 - mse: 61.2923 - mae: 5.5339\n",
      "Epoch 74/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 61.8671 - mse: 61.8671 - mae: 5.6142\n",
      "Epoch 75/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 61.4521 - mse: 61.4521 - mae: 5.5080\n",
      "Epoch 76/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 61.4141 - mse: 61.4141 - mae: 5.5447\n",
      "Epoch 77/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 61.0942 - mse: 61.0942 - mae: 5.6114\n",
      "Epoch 78/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 61.4537 - mse: 61.4537 - mae: 5.5395\n",
      "Epoch 79/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 60.9669 - mse: 60.9669 - mae: 5.5714\n",
      "Epoch 79: early stopping\n",
      "Epoch 1/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 163849456.0000 - mse: 163849456.0000 - mae: 3565.9158\n",
      "Epoch 2/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 265.2574 - mse: 265.2574 - mae: 13.5373\n",
      "Epoch 3/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 264.3132 - mse: 264.3132 - mae: 13.5048\n",
      "Epoch 4/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 263.3861 - mse: 263.3861 - mae: 13.4725\n",
      "Epoch 5/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 262.4583 - mse: 262.4583 - mae: 13.4401\n",
      "Epoch 6/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 261.5302 - mse: 261.5302 - mae: 13.4081\n",
      "Epoch 7/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 260.6190 - mse: 260.6190 - mae: 13.3769\n",
      "Epoch 8/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 259.7083 - mse: 259.7083 - mae: 13.3449\n",
      "Epoch 9/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 258.8055 - mse: 258.8055 - mae: 13.3126\n",
      "Epoch 10/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 257.9019 - mse: 257.9019 - mae: 13.2813\n",
      "Epoch 11/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 257.0052 - mse: 257.0052 - mae: 13.2496\n",
      "Epoch 12/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 256.1097 - mse: 256.1097 - mae: 13.2181\n",
      "Epoch 13/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 255.2219 - mse: 255.2219 - mae: 13.1866\n",
      "Epoch 14/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 254.3413 - mse: 254.3413 - mae: 13.1555\n",
      "Epoch 15/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 253.4568 - mse: 253.4568 - mae: 13.1242\n",
      "Epoch 16/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 252.5856 - mse: 252.5856 - mae: 13.0932\n",
      "Epoch 17/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 251.7140 - mse: 251.7140 - mae: 13.0616\n",
      "Epoch 18/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 250.8472 - mse: 250.8472 - mae: 13.0309\n",
      "Epoch 19/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 249.9880 - mse: 249.9880 - mae: 12.9999\n",
      "Epoch 20/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 249.1301 - mse: 249.1301 - mae: 12.9686\n",
      "Epoch 21/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 248.2770 - mse: 248.2770 - mae: 12.9380\n",
      "Epoch 22/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 247.4217 - mse: 247.4217 - mae: 12.9074\n",
      "Epoch 23/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 246.5807 - mse: 246.5807 - mae: 12.8772\n",
      "Epoch 24/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 245.7357 - mse: 245.7357 - mae: 12.8459\n",
      "Epoch 25/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 244.9020 - mse: 244.9020 - mae: 12.8158\n",
      "Epoch 26/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 244.0745 - mse: 244.0745 - mae: 12.7860\n",
      "Epoch 27/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 243.2506 - mse: 243.2506 - mae: 12.7557\n",
      "Epoch 28/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 242.4217 - mse: 242.4217 - mae: 12.7254\n",
      "Epoch 29/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 241.6075 - mse: 241.6075 - mae: 12.6956\n",
      "Epoch 30/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 240.7920 - mse: 240.7920 - mae: 12.6657\n",
      "Epoch 31/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 239.9830 - mse: 239.9830 - mae: 12.6358\n",
      "Epoch 32/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 239.1705 - mse: 239.1705 - mae: 12.6054\n",
      "Epoch 33/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 238.3727 - mse: 238.3727 - mae: 12.5766\n",
      "Epoch 34/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 237.5720 - mse: 237.5720 - mae: 12.5459\n",
      "Epoch 35/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 236.7787 - mse: 236.7786 - mae: 12.5165\n",
      "Epoch 36/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 235.9888 - mse: 235.9888 - mae: 12.4868\n",
      "Epoch 37/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 235.2048 - mse: 235.2048 - mae: 12.4572\n",
      "Epoch 38/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 234.4265 - mse: 234.4265 - mae: 12.4284\n",
      "Epoch 39/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 233.6491 - mse: 233.6491 - mae: 12.3991\n",
      "Epoch 40/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 232.8824 - mse: 232.8824 - mae: 12.3698\n",
      "Epoch 41/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 232.1174 - mse: 232.1174 - mae: 12.3421\n",
      "Epoch 42/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 231.3458 - mse: 231.3458 - mae: 12.3124\n",
      "Epoch 43/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 230.5846 - mse: 230.5846 - mae: 12.2839\n",
      "Epoch 44/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 229.8261 - mse: 229.8261 - mae: 12.2553\n",
      "Epoch 45/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 229.0747 - mse: 229.0747 - mae: 12.2265\n",
      "Epoch 46/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 228.3198 - mse: 228.3198 - mae: 12.1978\n",
      "Epoch 47/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 227.5781 - mse: 227.5781 - mae: 12.1694\n",
      "Epoch 48/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 226.8407 - mse: 226.8407 - mae: 12.1415\n",
      "Epoch 49/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 226.1003 - mse: 226.1003 - mae: 12.1134\n",
      "Epoch 50/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 225.3633 - mse: 225.3633 - mae: 12.0857\n",
      "Epoch 51/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 224.6344 - mse: 224.6344 - mae: 12.0578\n",
      "Epoch 52/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 223.9064 - mse: 223.9064 - mae: 12.0307\n",
      "Epoch 53/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 223.1769 - mse: 223.1769 - mae: 12.0036\n",
      "Epoch 54/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 222.4623 - mse: 222.4623 - mae: 11.9757\n",
      "Epoch 55/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 221.7425 - mse: 221.7425 - mae: 11.9479\n",
      "Epoch 56/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 221.0364 - mse: 221.0364 - mae: 11.9209\n",
      "Epoch 57/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 220.3328 - mse: 220.3328 - mae: 11.8940\n",
      "Epoch 58/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 219.6302 - mse: 219.6302 - mae: 11.8666\n",
      "Epoch 59/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 218.9285 - mse: 218.9285 - mae: 11.8402\n",
      "Epoch 60/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 218.2310 - mse: 218.2310 - mae: 11.8131\n",
      "Epoch 61/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 217.5402 - mse: 217.5402 - mae: 11.7861\n",
      "Epoch 62/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 216.8509 - mse: 216.8509 - mae: 11.7592\n",
      "Epoch 63/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 216.1669 - mse: 216.1669 - mae: 11.7333\n",
      "Epoch 64/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 215.4766 - mse: 215.4766 - mae: 11.7064\n",
      "Epoch 65/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 214.8006 - mse: 214.8006 - mae: 11.6806\n",
      "Epoch 66/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 214.1236 - mse: 214.1236 - mae: 11.6549\n",
      "Epoch 67/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 213.4525 - mse: 213.4525 - mae: 11.6280\n",
      "Epoch 68/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 212.7842 - mse: 212.7842 - mae: 11.6023\n",
      "Epoch 69/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 212.1183 - mse: 212.1183 - mae: 11.5768\n",
      "Epoch 70/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 211.4572 - mse: 211.4572 - mae: 11.5516\n",
      "Epoch 71/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 210.7999 - mse: 210.7999 - mae: 11.5250\n",
      "Epoch 72/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 210.1501 - mse: 210.1501 - mae: 11.4997\n",
      "Epoch 73/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 209.5023 - mse: 209.5023 - mae: 11.4751\n",
      "Epoch 74/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 208.8511 - mse: 208.8511 - mae: 11.4493\n",
      "Epoch 75/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 208.2077 - mse: 208.2077 - mae: 11.4245\n",
      "Epoch 76/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 207.5648 - mse: 207.5648 - mae: 11.3988\n",
      "Epoch 77/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 206.9264 - mse: 206.9264 - mae: 11.3733\n",
      "Epoch 78/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 206.2955 - mse: 206.2955 - mae: 11.3487\n",
      "Epoch 79/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 205.6696 - mse: 205.6696 - mae: 11.3242\n",
      "Epoch 80/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 205.0459 - mse: 205.0459 - mae: 11.2986\n",
      "Epoch 81/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 204.4290 - mse: 204.4290 - mae: 11.2746\n",
      "Epoch 82/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 203.8100 - mse: 203.8100 - mae: 11.2496\n",
      "Epoch 83/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 203.1919 - mse: 203.1919 - mae: 11.2252\n",
      "Epoch 84/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 202.5767 - mse: 202.5767 - mae: 11.2009\n",
      "Epoch 85/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 201.9688 - mse: 201.9688 - mae: 11.1759\n",
      "Epoch 86/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 201.3619 - mse: 201.3619 - mae: 11.1522\n",
      "Epoch 87/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 200.7579 - mse: 200.7579 - mae: 11.1279\n",
      "Epoch 88/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 200.1566 - mse: 200.1566 - mae: 11.1039\n",
      "Epoch 89/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 199.5537 - mse: 199.5537 - mae: 11.0792\n",
      "Epoch 90/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 198.9581 - mse: 198.9581 - mae: 11.0555\n",
      "Epoch 91/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 198.3691 - mse: 198.3691 - mae: 11.0318\n",
      "Epoch 92/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 197.7807 - mse: 197.7807 - mae: 11.0083\n",
      "Epoch 93/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 197.1893 - mse: 197.1893 - mae: 10.9839\n",
      "Epoch 94/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 196.6036 - mse: 196.6036 - mae: 10.9607\n",
      "Epoch 95/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 196.0243 - mse: 196.0243 - mae: 10.9380\n",
      "Epoch 96/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 195.4456 - mse: 195.4456 - mae: 10.9150\n",
      "Epoch 97/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 194.8677 - mse: 194.8677 - mae: 10.8910\n",
      "Epoch 98/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 194.2925 - mse: 194.2925 - mae: 10.8679\n",
      "Epoch 99/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 193.7253 - mse: 193.7253 - mae: 10.8448\n",
      "Epoch 100/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 193.1582 - mse: 193.1582 - mae: 10.8228\n",
      "[(1e-06, 84.74761962890625, 6.502342224121094), (1e-05, 60.96689224243164, 5.5713605880737305), (0.0001, 193.1582489013672, 10.822776794433594)]\n"
     ]
    }
   ],
   "source": [
    "#Przed eksperymentami wyczyść sesję TensorFlow i ustal generatory liczb losowych:\n",
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "lr_pkl = list()\n",
    "\n",
    "for lr in lr_list:\n",
    "    tensorboard_cb = keras.callbacks.TensorBoard(get_run_logdir(\"lr\", lr))\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor=\"loss\",\n",
    "                                                   patience = 10,\n",
    "                                                   min_delta=1.0,\n",
    "                                                   verbose=1)\n",
    "    \n",
    "    model = build_model(learning_rate=lr)                      \n",
    "    history = model.fit(X_train, y_train, epochs=100,\n",
    "                    callbacks=[early_stopping, tensorboard_cb])\n",
    "    \n",
    "    lr_pkl.append((lr, history.history[\"mse\"][-1], history.history[\"mae\"][-1]))\n",
    "save_to_pickle(\"lr\",lr_pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763b3036-d5b6-4e66-b6c2-bd9c83f8d5b6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### eksperymetr 2: liczba warstw ukrytych (hl): od 0 do 3,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f440bc2-5566-4c03-9400-03c9effa4119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 24234845961206104064.0000 - mse: 24234845961206104064.0000 - mae: 1486527488.0000\n",
      "Epoch 2/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: inf - mse: inf - mae: 2260918888248115200.0000                                       \n",
      "Epoch 3/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: inf - mse: inf - mae: 4824812911053208938256269312.0000\n",
      "Epoch 4/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: nan - mse: nan - mae: nan                            \n",
      "Epoch 5/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: nan - mse: nan - mae: nan\n",
      "Epoch 6/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: nan - mse: nan - mae: nan\n",
      "Epoch 7/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: nan - mse: nan - mae: nan\n",
      "Epoch 8/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: nan - mse: nan - mae: nan\n",
      "Epoch 9/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: nan - mse: nan - mae: nan\n",
      "Epoch 10/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: nan - mse: nan - mae: nan\n",
      "Epoch 11/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: nan - mse: nan - mae: nan\n",
      "Epoch 11: early stopping\n",
      "Epoch 1/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 47152.0156 - mse: 47152.0156 - mae: 91.1419\n",
      "Epoch 2/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 574.2975 - mse: 574.2975 - mae: 21.7228\n",
      "Epoch 3/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 572.9777 - mse: 572.9777 - mae: 21.6629\n",
      "Epoch 4/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 568.9274 - mse: 568.9274 - mae: 21.5159\n",
      "Epoch 5/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 541.7530 - mse: 541.7530 - mae: 20.7596\n",
      "Epoch 6/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 514.9861 - mse: 514.9861 - mae: 19.7161\n",
      "Epoch 7/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 403.8700 - mse: 403.8700 - mae: 17.3078\n",
      "Epoch 8/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 267.1600 - mse: 267.1600 - mae: 13.6946\n",
      "Epoch 9/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 218.3973 - mse: 218.3973 - mae: 12.0847\n",
      "Epoch 10/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 200.7678 - mse: 200.7678 - mae: 11.3046\n",
      "Epoch 11/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 182.1192 - mse: 182.1192 - mae: 10.7862\n",
      "Epoch 12/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 169.6411 - mse: 169.6411 - mae: 10.2272\n",
      "Epoch 13/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 161.5987 - mse: 161.5987 - mae: 9.9242\n",
      "Epoch 14/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 151.9887 - mse: 151.9887 - mae: 9.4469\n",
      "Epoch 15/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 144.5209 - mse: 144.5209 - mae: 9.1476\n",
      "Epoch 16/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 139.0525 - mse: 139.0525 - mae: 8.8661\n",
      "Epoch 17/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 133.9417 - mse: 133.9417 - mae: 8.6760\n",
      "Epoch 18/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 127.1813 - mse: 127.1813 - mae: 8.3925\n",
      "Epoch 19/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 122.1378 - mse: 122.1378 - mae: 8.1413\n",
      "Epoch 20/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 116.6719 - mse: 116.6719 - mae: 7.9093\n",
      "Epoch 21/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 114.9573 - mse: 114.9573 - mae: 7.8115\n",
      "Epoch 22/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 112.9476 - mse: 112.9476 - mae: 7.6628\n",
      "Epoch 23/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 105.0268 - mse: 105.0268 - mae: 7.3964\n",
      "Epoch 24/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 103.3372 - mse: 103.3372 - mae: 7.3008\n",
      "Epoch 25/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 98.9713 - mse: 98.9713 - mae: 7.1692\n",
      "Epoch 26/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 97.7035 - mse: 97.7035 - mae: 7.1744\n",
      "Epoch 27/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 94.7948 - mse: 94.7948 - mae: 6.9594\n",
      "Epoch 28/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 93.6976 - mse: 93.6976 - mae: 7.0359\n",
      "Epoch 29/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 93.5279 - mse: 93.5279 - mae: 7.0087\n",
      "Epoch 30/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 93.8636 - mse: 93.8636 - mae: 7.0889\n",
      "Epoch 31/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 90.9481 - mse: 90.9481 - mae: 6.9300\n",
      "Epoch 32/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 89.5179 - mse: 89.5179 - mae: 6.9790\n",
      "Epoch 33/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 89.3334 - mse: 89.3334 - mae: 6.9478\n",
      "Epoch 34/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 89.9353 - mse: 89.9353 - mae: 6.9947\n",
      "Epoch 35/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 88.5971 - mse: 88.5971 - mae: 6.9375\n",
      "Epoch 36/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 88.2736 - mse: 88.2736 - mae: 6.9767\n",
      "Epoch 37/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 89.0502 - mse: 89.0502 - mae: 6.9456\n",
      "Epoch 38/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 90.7144 - mse: 90.7144 - mae: 6.9657\n",
      "Epoch 39/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 88.2149 - mse: 88.2149 - mae: 6.9793\n",
      "Epoch 40/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 91.8376 - mse: 91.8376 - mae: 7.0424\n",
      "Epoch 41/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 88.1510 - mse: 88.1510 - mae: 6.8367\n",
      "Epoch 42/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 88.5862 - mse: 88.5862 - mae: 7.0260\n",
      "Epoch 43/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 87.2309 - mse: 87.2309 - mae: 6.8613\n",
      "Epoch 44/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 88.8639 - mse: 88.8639 - mae: 6.9764\n",
      "Epoch 45/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 88.0212 - mse: 88.0212 - mae: 6.9118\n",
      "Epoch 46/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 88.4251 - mse: 88.4251 - mae: 6.9798\n",
      "Epoch 47/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 89.3138 - mse: 89.3138 - mae: 6.9666\n",
      "Epoch 48/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 88.7481 - mse: 88.7481 - mae: 6.9253\n",
      "Epoch 49/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 89.4008 - mse: 89.4008 - mae: 6.9591\n",
      "Epoch 50/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 87.3473 - mse: 87.3473 - mae: 6.9012\n",
      "Epoch 51/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 86.6898 - mse: 86.6898 - mae: 6.9098\n",
      "Epoch 52/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 87.2474 - mse: 87.2474 - mae: 6.8650\n",
      "Epoch 53/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 89.2219 - mse: 89.2219 - mae: 7.0699\n",
      "Epoch 53: early stopping\n",
      "Epoch 1/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 8993.6572 - mse: 8993.6572 - mae: 58.4984\n",
      "Epoch 2/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 180.3446 - mse: 180.3446 - mae: 10.5865\n",
      "Epoch 3/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 157.9608 - mse: 157.9608 - mae: 9.6177\n",
      "Epoch 4/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 142.8509 - mse: 142.8509 - mae: 9.0088\n",
      "Epoch 5/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 121.8750 - mse: 121.8750 - mae: 8.1345\n",
      "Epoch 6/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 87.7274 - mse: 87.7274 - mae: 6.6436\n",
      "Epoch 7/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 76.3687 - mse: 76.3687 - mae: 6.1315\n",
      "Epoch 8/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 72.0866 - mse: 72.0866 - mae: 5.9904\n",
      "Epoch 9/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 67.8691 - mse: 67.8691 - mae: 5.6770\n",
      "Epoch 10/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 66.6122 - mse: 66.6122 - mae: 5.6115\n",
      "Epoch 11/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 65.0547 - mse: 65.0547 - mae: 5.5415\n",
      "Epoch 12/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 63.3290 - mse: 63.3290 - mae: 5.5552\n",
      "Epoch 13/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 65.1199 - mse: 65.1199 - mae: 5.6918\n",
      "Epoch 14/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 62.8777 - mse: 62.8777 - mae: 5.4912\n",
      "Epoch 15/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 63.3235 - mse: 63.3235 - mae: 5.6498\n",
      "Epoch 16/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 62.8257 - mse: 62.8257 - mae: 5.5131\n",
      "Epoch 17/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 62.6164 - mse: 62.6164 - mae: 5.5244\n",
      "Epoch 18/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 62.4039 - mse: 62.4039 - mae: 5.5603\n",
      "Epoch 19/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 61.6751 - mse: 61.6751 - mae: 5.4687\n",
      "Epoch 20/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 62.1435 - mse: 62.1435 - mae: 5.5719\n",
      "Epoch 21/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 62.7220 - mse: 62.7220 - mae: 5.4532\n",
      "Epoch 22/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 64.6205 - mse: 64.6205 - mae: 5.7327\n",
      "Epoch 23/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 61.2882 - mse: 61.2882 - mae: 5.4348\n",
      "Epoch 24/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 63.3799 - mse: 63.3799 - mae: 5.6137\n",
      "Epoch 25/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 61.4203 - mse: 61.4203 - mae: 5.4381\n",
      "Epoch 26/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 61.8977 - mse: 61.8977 - mae: 5.4698\n",
      "Epoch 27/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 61.3764 - mse: 61.3764 - mae: 5.4524\n",
      "Epoch 28/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 63.0357 - mse: 63.0357 - mae: 5.6227\n",
      "Epoch 29/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 62.6360 - mse: 62.6360 - mae: 5.5623\n",
      "Epoch 29: early stopping\n",
      "Epoch 1/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 301.2970 - mse: 301.2970 - mae: 12.2519\n",
      "Epoch 2/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 80.9051 - mse: 80.9051 - mae: 6.4340\n",
      "Epoch 3/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 83.5490 - mse: 83.5490 - mae: 6.6281\n",
      "Epoch 4/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 78.6900 - mse: 78.6900 - mae: 6.3746\n",
      "Epoch 5/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 71.1350 - mse: 71.1350 - mae: 6.0121\n",
      "Epoch 6/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 71.9508 - mse: 71.9508 - mae: 6.0401\n",
      "Epoch 7/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 70.8848 - mse: 70.8848 - mae: 5.9054\n",
      "Epoch 8/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 72.7505 - mse: 72.7505 - mae: 6.0039\n",
      "Epoch 9/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 67.9158 - mse: 67.9158 - mae: 5.7887\n",
      "Epoch 10/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 70.7344 - mse: 70.7344 - mae: 5.8753\n",
      "Epoch 11/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 70.8935 - mse: 70.8935 - mae: 5.7776\n",
      "Epoch 12/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 64.2356 - mse: 64.2356 - mae: 5.6929\n",
      "Epoch 13/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 72.8473 - mse: 72.8473 - mae: 6.1143\n",
      "Epoch 14/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 65.5939 - mse: 65.5939 - mae: 5.6611\n",
      "Epoch 15/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 65.8304 - mse: 65.8304 - mae: 5.8370\n",
      "Epoch 16/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 65.9223 - mse: 65.9223 - mae: 5.6635\n",
      "Epoch 17/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 65.5062 - mse: 65.5062 - mae: 5.6590\n",
      "Epoch 18/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 64.1442 - mse: 64.1442 - mae: 5.6807\n",
      "Epoch 19/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 63.6537 - mse: 63.6537 - mae: 5.5821\n",
      "Epoch 20/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 65.5530 - mse: 65.5530 - mae: 5.8083\n",
      "Epoch 21/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 65.9229 - mse: 65.9229 - mae: 5.6893\n",
      "Epoch 22/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 71.7854 - mse: 71.7854 - mae: 6.0787\n",
      "Epoch 22: early stopping\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "#Przed eksperymentami wyczyść sesję TensorFlow i ustal generatory liczb losowych:\n",
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "hr_pkl = list()\n",
    "\n",
    "for hl in hl_list:\n",
    "    tensorboard_cb = keras.callbacks.TensorBoard(get_run_logdir(\"hl\", hl))\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor=\"loss\",\n",
    "                                                   patience = 10,\n",
    "                                                   min_delta=1.0,\n",
    "                                                   verbose=1)\n",
    "    \n",
    "    model = build_model(n_hidden = hl)                      \n",
    "    history = model.fit(X_train, y_train, epochs=100,\n",
    "                    callbacks=[early_stopping, tensorboard_cb])\n",
    "    \n",
    "    lr_pkl.append((hl, history.history[\"mse\"][-1], history.history[\"mae\"][-1]))\n",
    "save_to_pickle(\"hr\",hr_pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4899ccbb-3d66-4fde-844a-1152bd23184a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### eksperyment 3: liczba neuronów na warstwę (nn): 5, 25, 125,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30e926b6-349c-410b-b91e-94692089e9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 5503.8071 - mse: 5503.8071 - mae: 47.5473\n",
      "Epoch 2/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 585.7496 - mse: 585.7496 - mae: 22.3859\n",
      "Epoch 3/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 585.4879 - mse: 585.4879 - mae: 22.3800\n",
      "Epoch 4/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 585.2284 - mse: 585.2284 - mae: 22.3742\n",
      "Epoch 5/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 584.9681 - mse: 584.9681 - mae: 22.3684\n",
      "Epoch 6/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 584.7070 - mse: 584.7070 - mae: 22.3626\n",
      "Epoch 7/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 584.4481 - mse: 584.4481 - mae: 22.3568\n",
      "Epoch 8/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 584.1886 - mse: 584.1886 - mae: 22.3510\n",
      "Epoch 9/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 583.9299 - mse: 583.9299 - mae: 22.3452\n",
      "Epoch 10/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 583.6702 - mse: 583.6702 - mae: 22.3394\n",
      "Epoch 11/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 583.4113 - mse: 583.4113 - mae: 22.3336\n",
      "Epoch 12/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 583.1517 - mse: 583.1517 - mae: 22.3278\n",
      "Epoch 13/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 582.8927 - mse: 582.8927 - mae: 22.3220\n",
      "Epoch 14/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 582.6345 - mse: 582.6345 - mae: 22.3162\n",
      "Epoch 15/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 582.3748 - mse: 582.3748 - mae: 22.3104\n",
      "Epoch 16/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 582.1168 - mse: 582.1168 - mae: 22.3046\n",
      "Epoch 17/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 581.8582 - mse: 581.8582 - mae: 22.2988\n",
      "Epoch 18/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 581.5997 - mse: 581.5997 - mae: 22.2930\n",
      "Epoch 19/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 581.3418 - mse: 581.3418 - mae: 22.2872\n",
      "Epoch 20/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 581.0836 - mse: 581.0836 - mae: 22.2814\n",
      "Epoch 21/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 580.8254 - mse: 580.8254 - mae: 22.2757\n",
      "Epoch 22/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 580.5663 - mse: 580.5663 - mae: 22.2699\n",
      "Epoch 23/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 580.3091 - mse: 580.3091 - mae: 22.2641\n",
      "Epoch 24/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 580.0506 - mse: 580.0506 - mae: 22.2582\n",
      "Epoch 25/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 579.7935 - mse: 579.7935 - mae: 22.2524\n",
      "Epoch 26/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 579.5366 - mse: 579.5366 - mae: 22.2467\n",
      "Epoch 27/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 579.2799 - mse: 579.2799 - mae: 22.2409\n",
      "Epoch 28/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 579.0216 - mse: 579.0216 - mae: 22.2351\n",
      "Epoch 29/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 578.7654 - mse: 578.7654 - mae: 22.2293\n",
      "Epoch 30/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 578.5084 - mse: 578.5084 - mae: 22.2235\n",
      "Epoch 31/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 578.2518 - mse: 578.2518 - mae: 22.2178\n",
      "Epoch 32/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 577.9940 - mse: 577.9940 - mae: 22.2120\n",
      "Epoch 33/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 577.7381 - mse: 577.7381 - mae: 22.2063\n",
      "Epoch 34/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 577.4812 - mse: 577.4812 - mae: 22.2004\n",
      "Epoch 35/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 577.2250 - mse: 577.2250 - mae: 22.1946\n",
      "Epoch 36/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 576.9688 - mse: 576.9688 - mae: 22.1888\n",
      "Epoch 37/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 576.7130 - mse: 576.7130 - mae: 22.1831\n",
      "Epoch 38/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 576.4577 - mse: 576.4577 - mae: 22.1773\n",
      "Epoch 39/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 576.2019 - mse: 576.2019 - mae: 22.1716\n",
      "Epoch 40/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 575.9474 - mse: 575.9474 - mae: 22.1658\n",
      "Epoch 41/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 575.6928 - mse: 575.6928 - mae: 22.1602\n",
      "Epoch 42/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 575.4363 - mse: 575.4363 - mae: 22.1544\n",
      "Epoch 43/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 575.1812 - mse: 575.1812 - mae: 22.1486\n",
      "Epoch 44/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 574.9260 - mse: 574.9260 - mae: 22.1428\n",
      "Epoch 45/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 574.6713 - mse: 574.6713 - mae: 22.1371\n",
      "Epoch 46/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 574.4155 - mse: 574.4155 - mae: 22.1313\n",
      "Epoch 47/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 574.1616 - mse: 574.1616 - mae: 22.1255\n",
      "Epoch 48/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 573.9078 - mse: 573.9078 - mae: 22.1198\n",
      "Epoch 49/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 573.6530 - mse: 573.6530 - mae: 22.1141\n",
      "Epoch 50/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 573.3982 - mse: 573.3982 - mae: 22.1083\n",
      "Epoch 51/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 573.1442 - mse: 573.1442 - mae: 22.1025\n",
      "Epoch 52/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 572.8900 - mse: 572.8900 - mae: 22.0968\n",
      "Epoch 53/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 572.6348 - mse: 572.6348 - mae: 22.0911\n",
      "Epoch 54/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 572.3818 - mse: 572.3818 - mae: 22.0853\n",
      "Epoch 55/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 572.1274 - mse: 572.1274 - mae: 22.0795\n",
      "Epoch 56/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 571.8749 - mse: 571.8749 - mae: 22.0738\n",
      "Epoch 57/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 571.6223 - mse: 571.6223 - mae: 22.0681\n",
      "Epoch 58/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 571.3694 - mse: 571.3694 - mae: 22.0623\n",
      "Epoch 59/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 571.1161 - mse: 571.1161 - mae: 22.0567\n",
      "Epoch 60/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 570.8631 - mse: 570.8631 - mae: 22.0509\n",
      "Epoch 61/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 570.6107 - mse: 570.6107 - mae: 22.0451\n",
      "Epoch 62/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 570.3582 - mse: 570.3582 - mae: 22.0394\n",
      "Epoch 63/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 570.1060 - mse: 570.1060 - mae: 22.0337\n",
      "Epoch 64/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 569.8520 - mse: 569.8520 - mae: 22.0279\n",
      "Epoch 65/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 569.6002 - mse: 569.6002 - mae: 22.0222\n",
      "Epoch 66/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 569.3477 - mse: 569.3477 - mae: 22.0165\n",
      "Epoch 67/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 569.0959 - mse: 569.0959 - mae: 22.0107\n",
      "Epoch 68/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 568.8441 - mse: 568.8441 - mae: 22.0050\n",
      "Epoch 69/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 568.5921 - mse: 568.5921 - mae: 21.9993\n",
      "Epoch 70/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 568.3405 - mse: 568.3405 - mae: 21.9936\n",
      "Epoch 71/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 568.0892 - mse: 568.0892 - mae: 21.9878\n",
      "Epoch 72/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 567.8387 - mse: 567.8387 - mae: 21.9821\n",
      "Epoch 73/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 567.5882 - mse: 567.5882 - mae: 21.9765\n",
      "Epoch 74/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 567.3364 - mse: 567.3364 - mae: 21.9708\n",
      "Epoch 75/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 567.0857 - mse: 567.0857 - mae: 21.9651\n",
      "Epoch 76/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 566.8345 - mse: 566.8345 - mae: 21.9593\n",
      "Epoch 77/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 566.5836 - mse: 566.5836 - mae: 21.9536\n",
      "Epoch 78/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 566.3338 - mse: 566.3338 - mae: 21.9479\n",
      "Epoch 79/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 566.0844 - mse: 566.0844 - mae: 21.9422\n",
      "Epoch 80/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 565.8350 - mse: 565.8350 - mae: 21.9365\n",
      "Epoch 81/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 565.5863 - mse: 565.5863 - mae: 21.9309\n",
      "Epoch 82/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 565.3367 - mse: 565.3367 - mae: 21.9251\n",
      "Epoch 83/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 565.0869 - mse: 565.0869 - mae: 21.9195\n",
      "Epoch 84/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 564.8370 - mse: 564.8370 - mae: 21.9138\n",
      "Epoch 85/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 564.5882 - mse: 564.5882 - mae: 21.9081\n",
      "Epoch 86/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 564.3391 - mse: 564.3391 - mae: 21.9024\n",
      "Epoch 87/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 564.0900 - mse: 564.0900 - mae: 21.8967\n",
      "Epoch 88/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 563.8409 - mse: 563.8409 - mae: 21.8910\n",
      "Epoch 89/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 563.5910 - mse: 563.5910 - mae: 21.8853\n",
      "Epoch 90/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 563.3422 - mse: 563.3422 - mae: 21.8796\n",
      "Epoch 91/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 563.0941 - mse: 563.0941 - mae: 21.8740\n",
      "Epoch 92/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 562.8456 - mse: 562.8456 - mae: 21.8683\n",
      "Epoch 93/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 562.5961 - mse: 562.5961 - mae: 21.8626\n",
      "Epoch 94/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 562.3473 - mse: 562.3473 - mae: 21.8569\n",
      "Epoch 95/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 562.0993 - mse: 562.0993 - mae: 21.8513\n",
      "Epoch 96/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 561.8509 - mse: 561.8509 - mae: 21.8456\n",
      "Epoch 97/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 561.6021 - mse: 561.6021 - mae: 21.8399\n",
      "Epoch 98/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 561.3534 - mse: 561.3534 - mae: 21.8341\n",
      "Epoch 99/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 561.1060 - mse: 561.1060 - mae: 21.8285\n",
      "Epoch 100/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 560.8580 - mse: 560.8580 - mae: 21.8229\n",
      "Epoch 1/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1524.2535 - mse: 1524.2535 - mae: 20.8298\n",
      "Epoch 2/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 126.9034 - mse: 126.9034 - mae: 8.4635\n",
      "Epoch 3/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 107.8146 - mse: 107.8146 - mae: 7.8811\n",
      "Epoch 4/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 96.5704 - mse: 96.5704 - mae: 7.1098\n",
      "Epoch 5/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 88.4147 - mse: 88.4147 - mae: 6.8102\n",
      "Epoch 6/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 83.9571 - mse: 83.9571 - mae: 6.6934\n",
      "Epoch 7/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 78.5842 - mse: 78.5842 - mae: 6.2570\n",
      "Epoch 8/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 76.3205 - mse: 76.3205 - mae: 6.3244\n",
      "Epoch 9/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 74.8923 - mse: 74.8923 - mae: 6.1789\n",
      "Epoch 10/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 75.5893 - mse: 75.5893 - mae: 6.2002\n",
      "Epoch 11/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 75.0033 - mse: 75.0033 - mae: 6.1574\n",
      "Epoch 12/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 73.1394 - mse: 73.1394 - mae: 6.1501\n",
      "Epoch 13/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 74.9279 - mse: 74.9279 - mae: 6.1996\n",
      "Epoch 14/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 73.0146 - mse: 73.0146 - mae: 6.0660\n",
      "Epoch 15/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 73.1447 - mse: 73.1447 - mae: 6.1985\n",
      "Epoch 16/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 72.3123 - mse: 72.3123 - mae: 6.0121\n",
      "Epoch 17/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 72.2366 - mse: 72.2366 - mae: 6.0438\n",
      "Epoch 18/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 72.2005 - mse: 72.2005 - mae: 6.1173\n",
      "Epoch 19/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 71.3581 - mse: 71.3581 - mae: 5.9604\n",
      "Epoch 20/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 71.3467 - mse: 71.3467 - mae: 6.0408\n",
      "Epoch 21/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 71.5218 - mse: 71.5218 - mae: 5.9490\n",
      "Epoch 22/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 72.4559 - mse: 72.4559 - mae: 6.1169\n",
      "Epoch 23/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 70.5268 - mse: 70.5268 - mae: 5.9448\n",
      "Epoch 24/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 71.5256 - mse: 71.5256 - mae: 6.0391\n",
      "Epoch 25/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 70.0426 - mse: 70.0426 - mae: 5.9206\n",
      "Epoch 26/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 70.1493 - mse: 70.1493 - mae: 5.9529\n",
      "Epoch 27/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 69.4945 - mse: 69.4945 - mae: 5.9240\n",
      "Epoch 28/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 69.8161 - mse: 69.8161 - mae: 5.9589\n",
      "Epoch 29/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 69.9612 - mse: 69.9612 - mae: 5.9576\n",
      "Epoch 30/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 70.0430 - mse: 70.0430 - mae: 5.8966\n",
      "Epoch 31/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 68.9811 - mse: 68.9811 - mae: 5.8839\n",
      "Epoch 32/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 68.4325 - mse: 68.4325 - mae: 5.9038\n",
      "Epoch 33/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 67.7940 - mse: 67.7940 - mae: 5.7837\n",
      "Epoch 34/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 69.1919 - mse: 69.1919 - mae: 5.9848\n",
      "Epoch 35/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 68.0314 - mse: 68.0314 - mae: 5.8487\n",
      "Epoch 36/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 67.3443 - mse: 67.3443 - mae: 5.8387\n",
      "Epoch 37/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 67.8728 - mse: 67.8728 - mae: 5.8416\n",
      "Epoch 38/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 68.4668 - mse: 68.4668 - mae: 5.7831\n",
      "Epoch 39/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 67.6345 - mse: 67.6345 - mae: 5.8422\n",
      "Epoch 40/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 68.5339 - mse: 68.5339 - mae: 5.8298\n",
      "Epoch 41/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 66.6280 - mse: 66.6280 - mae: 5.6573\n",
      "Epoch 42/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 66.8349 - mse: 66.8349 - mae: 5.8833\n",
      "Epoch 43/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 65.8785 - mse: 65.8785 - mae: 5.7703\n",
      "Epoch 44/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 65.9889 - mse: 65.9889 - mae: 5.7853\n",
      "Epoch 45/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 65.6257 - mse: 65.6257 - mae: 5.7237\n",
      "Epoch 46/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 65.9952 - mse: 65.9952 - mae: 5.8150\n",
      "Epoch 47/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 66.4315 - mse: 66.4315 - mae: 5.7666\n",
      "Epoch 48/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 65.9395 - mse: 65.9395 - mae: 5.7119\n",
      "Epoch 49/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 66.3552 - mse: 66.3552 - mae: 5.7616\n",
      "Epoch 50/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 65.0863 - mse: 65.0863 - mae: 5.7470\n",
      "Epoch 51/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 64.6140 - mse: 64.6140 - mae: 5.7742\n",
      "Epoch 52/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 64.8338 - mse: 64.8338 - mae: 5.6567\n",
      "Epoch 53/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 65.8851 - mse: 65.8851 - mae: 5.8725\n",
      "Epoch 54/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 65.3151 - mse: 65.3151 - mae: 5.6995\n",
      "Epoch 55/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 64.3356 - mse: 64.3356 - mae: 5.7699\n",
      "Epoch 56/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 64.3643 - mse: 64.3643 - mae: 5.6946\n",
      "Epoch 57/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 64.0579 - mse: 64.0579 - mae: 5.6409\n",
      "Epoch 58/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 64.0276 - mse: 64.0276 - mae: 5.6530\n",
      "Epoch 59/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 63.8599 - mse: 63.8599 - mae: 5.6610\n",
      "Epoch 60/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 63.2993 - mse: 63.2993 - mae: 5.7268\n",
      "Epoch 61/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 63.7287 - mse: 63.7287 - mae: 5.6462\n",
      "Epoch 62/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 62.7508 - mse: 62.7508 - mae: 5.6640\n",
      "Epoch 63/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 62.9453 - mse: 62.9453 - mae: 5.5266\n",
      "Epoch 64/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 62.9544 - mse: 62.9544 - mae: 5.6884\n",
      "Epoch 65/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 63.9597 - mse: 63.9597 - mae: 5.7083\n",
      "Epoch 66/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 62.8322 - mse: 62.8322 - mae: 5.6089\n",
      "Epoch 67/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 62.9549 - mse: 62.9549 - mae: 5.6963\n",
      "Epoch 68/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 62.8526 - mse: 62.8526 - mae: 5.5722\n",
      "Epoch 69/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 61.8291 - mse: 61.8291 - mae: 5.5677\n",
      "Epoch 70/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 63.4475 - mse: 63.4475 - mae: 5.6542\n",
      "Epoch 71/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 60.9820 - mse: 60.9820 - mae: 5.5510\n",
      "Epoch 72/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 61.6298 - mse: 61.6298 - mae: 5.5066\n",
      "Epoch 73/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 61.2923 - mse: 61.2923 - mae: 5.5339\n",
      "Epoch 74/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 61.8671 - mse: 61.8671 - mae: 5.6142\n",
      "Epoch 75/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 61.4521 - mse: 61.4521 - mae: 5.5080\n",
      "Epoch 76/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 61.4141 - mse: 61.4141 - mae: 5.5447\n",
      "Epoch 77/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 61.0942 - mse: 61.0942 - mae: 5.6114\n",
      "Epoch 78/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 61.4537 - mse: 61.4537 - mae: 5.5395\n",
      "Epoch 79/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 60.9669 - mse: 60.9669 - mae: 5.5714\n",
      "Epoch 79: early stopping\n",
      "Epoch 1/100\n",
      "13/13 [==============================] - 1s 1ms/step - loss: 26321.4824 - mse: 26321.4824 - mae: 79.4014\n",
      "Epoch 2/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 82.7364 - mse: 82.7364 - mae: 6.8041\n",
      "Epoch 3/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 83.3891 - mse: 83.3891 - mae: 6.9428\n",
      "Epoch 4/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 82.3556 - mse: 82.3556 - mae: 6.7760\n",
      "Epoch 5/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 81.0694 - mse: 81.0694 - mae: 6.7704\n",
      "Epoch 6/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 80.7131 - mse: 80.7131 - mae: 6.7495\n",
      "Epoch 7/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 81.2082 - mse: 81.2082 - mae: 6.7297\n",
      "Epoch 8/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 80.6655 - mse: 80.6655 - mae: 6.7289\n",
      "Epoch 9/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 79.1787 - mse: 79.1787 - mae: 6.6639\n",
      "Epoch 10/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 81.6423 - mse: 81.6423 - mae: 6.7416\n",
      "Epoch 11/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 79.4970 - mse: 79.4970 - mae: 6.6024\n",
      "Epoch 12/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 78.0614 - mse: 78.0614 - mae: 6.6233\n",
      "Epoch 13/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 78.6864 - mse: 78.6864 - mae: 6.5652\n",
      "Epoch 14/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 76.5457 - mse: 76.5457 - mae: 6.4897\n",
      "Epoch 15/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 75.6641 - mse: 75.6641 - mae: 6.5189\n",
      "Epoch 16/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 74.8391 - mse: 74.8391 - mae: 6.4004\n",
      "Epoch 17/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 73.6732 - mse: 73.6732 - mae: 6.3708\n",
      "Epoch 18/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 72.1422 - mse: 72.1422 - mae: 6.2874\n",
      "Epoch 19/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 70.2011 - mse: 70.2011 - mae: 6.1023\n",
      "Epoch 20/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 68.3867 - mse: 68.3867 - mae: 6.0694\n",
      "Epoch 21/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 68.0888 - mse: 68.0888 - mae: 5.8909\n",
      "Epoch 22/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 68.4244 - mse: 68.4244 - mae: 6.0061\n",
      "Epoch 23/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 65.5776 - mse: 65.5776 - mae: 5.7293\n",
      "Epoch 24/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 67.3578 - mse: 67.3578 - mae: 5.8968\n",
      "Epoch 25/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 64.8530 - mse: 64.8530 - mae: 5.7138\n",
      "Epoch 26/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 65.2434 - mse: 65.2434 - mae: 5.7398\n",
      "Epoch 27/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 64.8871 - mse: 64.8871 - mae: 5.7484\n",
      "Epoch 28/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 64.8969 - mse: 64.8969 - mae: 5.8064\n",
      "Epoch 29/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 65.4982 - mse: 65.4982 - mae: 5.7749\n",
      "Epoch 30/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 64.9327 - mse: 64.9327 - mae: 5.7110\n",
      "Epoch 31/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 63.7454 - mse: 63.7454 - mae: 5.6877\n",
      "Epoch 32/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 62.7728 - mse: 62.7728 - mae: 5.6582\n",
      "Epoch 33/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 62.9440 - mse: 62.9440 - mae: 5.5721\n",
      "Epoch 34/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 64.0378 - mse: 64.0378 - mae: 5.7698\n",
      "Epoch 35/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 63.0832 - mse: 63.0832 - mae: 5.6331\n",
      "Epoch 36/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 62.3110 - mse: 62.3110 - mae: 5.6281\n",
      "Epoch 37/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 62.6624 - mse: 62.6624 - mae: 5.6403\n",
      "Epoch 38/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 64.2223 - mse: 64.2223 - mae: 5.6489\n",
      "Epoch 39/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 63.6540 - mse: 63.6540 - mae: 5.6851\n",
      "Epoch 40/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 64.0073 - mse: 64.0073 - mae: 5.7421\n",
      "Epoch 41/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 63.3989 - mse: 63.3989 - mae: 5.5630\n",
      "Epoch 42/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 62.4294 - mse: 62.4294 - mae: 5.6466\n",
      "Epoch 43/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 62.3597 - mse: 62.3597 - mae: 5.6020\n",
      "Epoch 44/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 62.8292 - mse: 62.8292 - mae: 5.6137\n",
      "Epoch 45/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 62.0445 - mse: 62.0445 - mae: 5.5450\n",
      "Epoch 46/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 62.7556 - mse: 62.7556 - mae: 5.6729\n",
      "Epoch 46: early stopping\n",
      "[(3, 560.8580322265625, 21.822851181030273), (3, 60.96689224243164, 5.5713605880737305), (3, 62.7556037902832, 5.672937393188477)]\n"
     ]
    }
   ],
   "source": [
    "#Przed eksperymentami wyczyść sesję TensorFlow i ustal generatory liczb losowych:\n",
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "nn_pkl = list()\n",
    "\n",
    "for nn in nn_list:\n",
    "    tensorboard_cb = keras.callbacks.TensorBoard(get_run_logdir(\"nn\", nn))\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor=\"loss\",\n",
    "                                                   patience = 10,\n",
    "                                                   min_delta=1.0,\n",
    "                                                   verbose=1)\n",
    "    \n",
    "    model = build_model(n_neurons = nn)                      \n",
    "    history = model.fit(X_train, y_train, epochs=100,\n",
    "                    callbacks=[early_stopping, tensorboard_cb])\n",
    "    \n",
    "    nn_pkl.append((hl, history.history[\"mse\"][-1], history.history[\"mae\"][-1]))\n",
    "save_to_pickle(\"nn\", nn_pkl)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a69954-2107-4439-9e38-8ac9ecbb9136",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### eksperyment 4: algorytm optymalizacji (opt): wszystkie 4 algorytmy (pęd = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22141af2-9e48-42d0-a0b1-3bdf4e638026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 83732.3828 - mse: 83732.3828 - mae: 99.9468\n",
      "Epoch 2/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 122.9807 - mse: 122.9807 - mae: 8.1736\n",
      "Epoch 3/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 93.0585 - mse: 93.0585 - mae: 7.1528\n",
      "Epoch 4/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 82.7675 - mse: 82.7675 - mae: 6.6219\n",
      "Epoch 5/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 76.3989 - mse: 76.3989 - mae: 6.4053\n",
      "Epoch 6/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 73.2667 - mse: 73.2667 - mae: 6.1773\n",
      "Epoch 7/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 71.5863 - mse: 71.5863 - mae: 5.9187\n",
      "Epoch 8/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 68.0666 - mse: 68.0666 - mae: 5.8083\n",
      "Epoch 9/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 66.3178 - mse: 66.3178 - mae: 5.6422\n",
      "Epoch 10/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 67.8043 - mse: 67.8043 - mae: 5.6757\n",
      "Epoch 11/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 66.3706 - mse: 66.3706 - mae: 5.6120\n",
      "Epoch 12/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 65.1992 - mse: 65.1992 - mae: 5.6752\n",
      "Epoch 13/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 67.3151 - mse: 67.3151 - mae: 5.7655\n",
      "Epoch 14/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 65.6250 - mse: 65.6250 - mae: 5.6331\n",
      "Epoch 15/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 66.0997 - mse: 66.0997 - mae: 5.7737\n",
      "Epoch 16/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 65.5308 - mse: 65.5308 - mae: 5.6172\n",
      "Epoch 17/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 65.7366 - mse: 65.7366 - mae: 5.6635\n",
      "Epoch 18/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 65.4474 - mse: 65.4474 - mae: 5.7013\n",
      "Epoch 19/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 64.9761 - mse: 64.9761 - mae: 5.5896\n",
      "Epoch 20/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 65.6430 - mse: 65.6430 - mae: 5.7230\n",
      "Epoch 21/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 65.6906 - mse: 65.6906 - mae: 5.5753\n",
      "Epoch 22/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 66.9982 - mse: 66.9982 - mae: 5.8257\n",
      "Epoch 22: early stopping\n",
      "Epoch 1/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1524.2535 - mse: 1524.2535 - mae: 20.8298\n",
      "Epoch 2/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 126.9034 - mse: 126.9034 - mae: 8.4635\n",
      "Epoch 3/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 107.8146 - mse: 107.8146 - mae: 7.8811\n",
      "Epoch 4/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 96.5704 - mse: 96.5704 - mae: 7.1098\n",
      "Epoch 5/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 88.4147 - mse: 88.4147 - mae: 6.8102\n",
      "Epoch 6/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 83.9571 - mse: 83.9571 - mae: 6.6934\n",
      "Epoch 7/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 78.5842 - mse: 78.5842 - mae: 6.2570\n",
      "Epoch 8/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 76.3205 - mse: 76.3205 - mae: 6.3244\n",
      "Epoch 9/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 74.8923 - mse: 74.8923 - mae: 6.1789\n",
      "Epoch 10/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 75.5893 - mse: 75.5893 - mae: 6.2002\n",
      "Epoch 11/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 75.0033 - mse: 75.0033 - mae: 6.1574\n",
      "Epoch 12/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 73.1394 - mse: 73.1394 - mae: 6.1501\n",
      "Epoch 13/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 74.9279 - mse: 74.9279 - mae: 6.1996\n",
      "Epoch 14/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 73.0146 - mse: 73.0146 - mae: 6.0660\n",
      "Epoch 15/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 73.1447 - mse: 73.1447 - mae: 6.1985\n",
      "Epoch 16/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 72.3123 - mse: 72.3123 - mae: 6.0121\n",
      "Epoch 17/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 72.2366 - mse: 72.2366 - mae: 6.0438\n",
      "Epoch 18/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 72.2005 - mse: 72.2005 - mae: 6.1173\n",
      "Epoch 19/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 71.3581 - mse: 71.3581 - mae: 5.9604\n",
      "Epoch 20/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 71.3467 - mse: 71.3467 - mae: 6.0408\n",
      "Epoch 21/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 71.5218 - mse: 71.5218 - mae: 5.9490\n",
      "Epoch 22/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 72.4559 - mse: 72.4559 - mae: 6.1169\n",
      "Epoch 23/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 70.5268 - mse: 70.5268 - mae: 5.9448\n",
      "Epoch 24/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 71.5256 - mse: 71.5256 - mae: 6.0391\n",
      "Epoch 25/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 70.0426 - mse: 70.0426 - mae: 5.9206\n",
      "Epoch 26/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 70.1493 - mse: 70.1493 - mae: 5.9529\n",
      "Epoch 27/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 69.4945 - mse: 69.4945 - mae: 5.9240\n",
      "Epoch 28/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 69.8161 - mse: 69.8161 - mae: 5.9589\n",
      "Epoch 29/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 69.9612 - mse: 69.9612 - mae: 5.9576\n",
      "Epoch 30/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 70.0430 - mse: 70.0430 - mae: 5.8966\n",
      "Epoch 31/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 68.9811 - mse: 68.9811 - mae: 5.8839\n",
      "Epoch 32/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 68.4325 - mse: 68.4325 - mae: 5.9038\n",
      "Epoch 33/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 67.7940 - mse: 67.7940 - mae: 5.7837\n",
      "Epoch 34/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 69.1919 - mse: 69.1919 - mae: 5.9848\n",
      "Epoch 35/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 68.0314 - mse: 68.0314 - mae: 5.8487\n",
      "Epoch 36/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 67.3443 - mse: 67.3443 - mae: 5.8387\n",
      "Epoch 37/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 67.8728 - mse: 67.8728 - mae: 5.8416\n",
      "Epoch 38/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 68.4668 - mse: 68.4668 - mae: 5.7831\n",
      "Epoch 39/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 67.6345 - mse: 67.6345 - mae: 5.8422\n",
      "Epoch 40/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 68.5339 - mse: 68.5339 - mae: 5.8298\n",
      "Epoch 41/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 66.6280 - mse: 66.6280 - mae: 5.6573\n",
      "Epoch 42/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 66.8349 - mse: 66.8349 - mae: 5.8833\n",
      "Epoch 43/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 65.8785 - mse: 65.8785 - mae: 5.7703\n",
      "Epoch 44/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 65.9889 - mse: 65.9889 - mae: 5.7853\n",
      "Epoch 45/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 65.6257 - mse: 65.6257 - mae: 5.7237\n",
      "Epoch 46/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 65.9952 - mse: 65.9952 - mae: 5.8150\n",
      "Epoch 47/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 66.4315 - mse: 66.4315 - mae: 5.7666\n",
      "Epoch 48/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 65.9395 - mse: 65.9395 - mae: 5.7119\n",
      "Epoch 49/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 66.3552 - mse: 66.3552 - mae: 5.7616\n",
      "Epoch 50/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 65.0863 - mse: 65.0863 - mae: 5.7470\n",
      "Epoch 51/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 64.6140 - mse: 64.6140 - mae: 5.7742\n",
      "Epoch 52/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 64.8338 - mse: 64.8338 - mae: 5.6567\n",
      "Epoch 53/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 65.8851 - mse: 65.8851 - mae: 5.8725\n",
      "Epoch 54/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 65.3151 - mse: 65.3151 - mae: 5.6995\n",
      "Epoch 55/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 64.3356 - mse: 64.3356 - mae: 5.7699\n",
      "Epoch 56/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 64.3643 - mse: 64.3643 - mae: 5.6946\n",
      "Epoch 57/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 64.0579 - mse: 64.0579 - mae: 5.6409\n",
      "Epoch 58/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 64.0276 - mse: 64.0276 - mae: 5.6530\n",
      "Epoch 59/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 63.8599 - mse: 63.8599 - mae: 5.6610\n",
      "Epoch 60/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 63.2993 - mse: 63.2993 - mae: 5.7268\n",
      "Epoch 61/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 63.7287 - mse: 63.7287 - mae: 5.6462\n",
      "Epoch 62/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 62.7508 - mse: 62.7508 - mae: 5.6640\n",
      "Epoch 63/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 62.9453 - mse: 62.9453 - mae: 5.5266\n",
      "Epoch 64/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 62.9544 - mse: 62.9544 - mae: 5.6884\n",
      "Epoch 65/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 63.9597 - mse: 63.9597 - mae: 5.7083\n",
      "Epoch 66/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 62.8322 - mse: 62.8322 - mae: 5.6089\n",
      "Epoch 67/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 62.9549 - mse: 62.9549 - mae: 5.6963\n",
      "Epoch 68/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 62.8526 - mse: 62.8526 - mae: 5.5722\n",
      "Epoch 69/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 61.8291 - mse: 61.8291 - mae: 5.5677\n",
      "Epoch 70/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 63.4475 - mse: 63.4475 - mae: 5.6542\n",
      "Epoch 71/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 60.9820 - mse: 60.9820 - mae: 5.5510\n",
      "Epoch 72/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 61.6298 - mse: 61.6298 - mae: 5.5066\n",
      "Epoch 73/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 61.2923 - mse: 61.2923 - mae: 5.5339\n",
      "Epoch 74/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 61.8671 - mse: 61.8671 - mae: 5.6142\n",
      "Epoch 75/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 61.4521 - mse: 61.4521 - mae: 5.5080\n",
      "Epoch 76/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 61.4141 - mse: 61.4141 - mae: 5.5447\n",
      "Epoch 77/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 61.0942 - mse: 61.0942 - mae: 5.6114\n",
      "Epoch 78/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 61.4537 - mse: 61.4537 - mae: 5.5395\n",
      "Epoch 79/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 60.9669 - mse: 60.9669 - mae: 5.5714\n",
      "Epoch 79: early stopping\n",
      "Epoch 1/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 125512.2188 - mse: 125512.2188 - mae: 146.4687\n",
      "Epoch 2/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 154.1075 - mse: 154.1075 - mae: 9.8314\n",
      "Epoch 3/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 94.7021 - mse: 94.7021 - mae: 7.5241\n",
      "Epoch 4/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 89.2134 - mse: 89.2134 - mae: 7.0937\n",
      "Epoch 5/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 86.3388 - mse: 86.3388 - mae: 7.0481\n",
      "Epoch 6/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 86.3474 - mse: 86.3474 - mae: 7.0211\n",
      "Epoch 7/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 88.7915 - mse: 88.7915 - mae: 7.0449\n",
      "Epoch 8/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 85.7482 - mse: 85.7482 - mae: 7.0277\n",
      "Epoch 9/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 84.6867 - mse: 84.6867 - mae: 6.8442\n",
      "Epoch 10/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 84.7501 - mse: 84.7501 - mae: 6.9096\n",
      "Epoch 11/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 81.2942 - mse: 81.2942 - mae: 6.7312\n",
      "Epoch 12/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 75.1871 - mse: 75.1871 - mae: 6.4921\n",
      "Epoch 13/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 74.6941 - mse: 74.6941 - mae: 6.3371\n",
      "Epoch 14/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 71.0615 - mse: 71.0615 - mae: 6.0885\n",
      "Epoch 15/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 71.7953 - mse: 71.7953 - mae: 6.2156\n",
      "Epoch 16/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 70.5393 - mse: 70.5393 - mae: 6.0610\n",
      "Epoch 17/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 70.4165 - mse: 70.4165 - mae: 6.0924\n",
      "Epoch 18/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 70.5126 - mse: 70.5126 - mae: 6.1544\n",
      "Epoch 19/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 69.9488 - mse: 69.9488 - mae: 6.0974\n",
      "Epoch 20/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 69.1707 - mse: 69.1707 - mae: 6.0775\n",
      "Epoch 21/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 69.4077 - mse: 69.4077 - mae: 6.0340\n",
      "Epoch 22/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 72.2287 - mse: 72.2287 - mae: 6.2810\n",
      "Epoch 23/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 69.0425 - mse: 69.0425 - mae: 6.0146\n",
      "Epoch 24/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 70.6646 - mse: 70.6646 - mae: 6.1595\n",
      "Epoch 25/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 68.9640 - mse: 68.9640 - mae: 5.9849\n",
      "Epoch 26/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 68.5331 - mse: 68.5331 - mae: 6.1027\n",
      "Epoch 27/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 69.2153 - mse: 69.2153 - mae: 6.0024\n",
      "Epoch 28/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 68.6596 - mse: 68.6596 - mae: 6.1174\n",
      "Epoch 29/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 69.9451 - mse: 69.9451 - mae: 6.0841\n",
      "Epoch 30/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 69.1153 - mse: 69.1153 - mae: 6.0359\n",
      "Epoch 31/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 67.9516 - mse: 67.9516 - mae: 5.9871\n",
      "Epoch 32/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 67.5202 - mse: 67.5202 - mae: 6.0087\n",
      "Epoch 33/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 65.8481 - mse: 65.8481 - mae: 5.8853\n",
      "Epoch 34/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 70.1299 - mse: 70.1299 - mae: 6.1669\n",
      "Epoch 35/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 67.0921 - mse: 67.0921 - mae: 5.9337\n",
      "Epoch 36/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 66.0369 - mse: 66.0369 - mae: 5.9492\n",
      "Epoch 37/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 66.7734 - mse: 66.7734 - mae: 5.9953\n",
      "Epoch 38/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 67.4604 - mse: 67.4604 - mae: 5.9126\n",
      "Epoch 39/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 67.2883 - mse: 67.2883 - mae: 5.9946\n",
      "Epoch 40/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 68.0260 - mse: 68.0260 - mae: 6.0002\n",
      "Epoch 41/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 65.6339 - mse: 65.6339 - mae: 5.7899\n",
      "Epoch 42/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 68.0558 - mse: 68.0558 - mae: 6.0607\n",
      "Epoch 43/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 65.3089 - mse: 65.3089 - mae: 5.8634\n",
      "Epoch 43: early stopping\n",
      "Epoch 1/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 54896.4219 - mse: 54896.4219 - mae: 228.5584\n",
      "Epoch 2/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 54602.3555 - mse: 54602.3555 - mae: 227.9387\n",
      "Epoch 3/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 54307.3125 - mse: 54307.3125 - mae: 227.3224\n",
      "Epoch 4/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 54017.5742 - mse: 54017.5742 - mae: 226.7054\n",
      "Epoch 5/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 53723.1445 - mse: 53723.1445 - mae: 226.0869\n",
      "Epoch 6/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 53434.4883 - mse: 53434.4883 - mae: 225.4742\n",
      "Epoch 7/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 53145.5703 - mse: 53145.5703 - mae: 224.8592\n",
      "Epoch 8/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 52859.3477 - mse: 52859.3477 - mae: 224.2467\n",
      "Epoch 9/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 52572.5898 - mse: 52572.5898 - mae: 223.6342\n",
      "Epoch 10/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 52287.7891 - mse: 52287.7891 - mae: 223.0235\n",
      "Epoch 11/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 52004.4609 - mse: 52004.4609 - mae: 222.4146\n",
      "Epoch 12/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 51721.2578 - mse: 51721.2578 - mae: 221.8038\n",
      "Epoch 13/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 51440.5156 - mse: 51440.5156 - mae: 221.1955\n",
      "Epoch 14/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 51160.5547 - mse: 51160.5547 - mae: 220.5879\n",
      "Epoch 15/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 50880.9023 - mse: 50880.9023 - mae: 219.9811\n",
      "Epoch 16/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 50603.0117 - mse: 50603.0117 - mae: 219.3774\n",
      "Epoch 17/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 50328.3008 - mse: 50328.3008 - mae: 218.7738\n",
      "Epoch 18/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 50053.0547 - mse: 50053.0547 - mae: 218.1686\n",
      "Epoch 19/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 49775.9805 - mse: 49775.9805 - mae: 217.5637\n",
      "Epoch 20/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 49504.0312 - mse: 49504.0312 - mae: 216.9631\n",
      "Epoch 21/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 49232.3633 - mse: 49232.3633 - mae: 216.3630\n",
      "Epoch 22/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 48962.9492 - mse: 48962.9492 - mae: 215.7643\n",
      "Epoch 23/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 48693.0781 - mse: 48693.0781 - mae: 215.1642\n",
      "Epoch 24/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 48423.0898 - mse: 48423.0898 - mae: 214.5654\n",
      "Epoch 25/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 48156.9492 - mse: 48156.9492 - mae: 213.9692\n",
      "Epoch 26/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 47889.3359 - mse: 47889.3359 - mae: 213.3713\n",
      "Epoch 27/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 47624.3477 - mse: 47624.3477 - mae: 212.7762\n",
      "Epoch 28/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 47359.3867 - mse: 47359.3867 - mae: 212.1803\n",
      "Epoch 29/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 47097.6719 - mse: 47097.6719 - mae: 211.5868\n",
      "Epoch 30/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 46833.9102 - mse: 46833.9102 - mae: 210.9916\n",
      "Epoch 31/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 46576.4844 - mse: 46576.4844 - mae: 210.4022\n",
      "Epoch 32/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 46315.6641 - mse: 46315.6641 - mae: 209.8092\n",
      "Epoch 33/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 46057.1055 - mse: 46057.1055 - mae: 209.2184\n",
      "Epoch 34/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 45801.3164 - mse: 45801.3164 - mae: 208.6319\n",
      "Epoch 35/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 45547.2109 - mse: 45547.2109 - mae: 208.0443\n",
      "Epoch 36/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 45288.8750 - mse: 45288.8750 - mae: 207.4547\n",
      "Epoch 37/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 45037.7227 - mse: 45037.7227 - mae: 206.8690\n",
      "Epoch 38/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 44783.3008 - mse: 44783.3008 - mae: 206.2805\n",
      "Epoch 39/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 44532.2461 - mse: 44532.2461 - mae: 205.6955\n",
      "Epoch 40/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 44281.7266 - mse: 44281.7266 - mae: 205.1134\n",
      "Epoch 41/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 44031.1875 - mse: 44031.1875 - mae: 204.5306\n",
      "Epoch 42/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 43786.7461 - mse: 43786.7461 - mae: 203.9513\n",
      "Epoch 43/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 43538.5938 - mse: 43538.5938 - mae: 203.3669\n",
      "Epoch 44/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 43290.9570 - mse: 43290.9570 - mae: 202.7859\n",
      "Epoch 45/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 43045.8203 - mse: 43045.8203 - mae: 202.2082\n",
      "Epoch 46/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 42803.0898 - mse: 42803.0898 - mae: 201.6302\n",
      "Epoch 47/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 42558.7422 - mse: 42558.7422 - mae: 201.0522\n",
      "Epoch 48/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 42317.2266 - mse: 42317.2266 - mae: 200.4761\n",
      "Epoch 49/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 42077.3398 - mse: 42077.3398 - mae: 199.8999\n",
      "Epoch 50/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 41836.9961 - mse: 41836.9961 - mae: 199.3215\n",
      "Epoch 51/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 41596.5195 - mse: 41596.5195 - mae: 198.7484\n",
      "Epoch 52/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 41360.8906 - mse: 41360.8906 - mae: 198.1774\n",
      "Epoch 53/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 41127.2852 - mse: 41127.2852 - mae: 197.6080\n",
      "Epoch 54/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 40889.7031 - mse: 40889.7031 - mae: 197.0347\n",
      "Epoch 55/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 40655.8398 - mse: 40655.8398 - mae: 196.4657\n",
      "Epoch 56/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 40423.1133 - mse: 40423.1133 - mae: 195.8968\n",
      "Epoch 57/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 40191.8828 - mse: 40191.8828 - mae: 195.3294\n",
      "Epoch 58/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 39959.5273 - mse: 39959.5273 - mae: 194.7571\n",
      "Epoch 59/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 39727.6133 - mse: 39727.6133 - mae: 194.1885\n",
      "Epoch 60/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 39499.6836 - mse: 39499.6836 - mae: 193.6234\n",
      "Epoch 61/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 39268.0547 - mse: 39268.0547 - mae: 193.0556\n",
      "Epoch 62/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 39042.7266 - mse: 39042.7266 - mae: 192.4945\n",
      "Epoch 63/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 38817.7578 - mse: 38817.7578 - mae: 191.9317\n",
      "Epoch 64/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 38590.1289 - mse: 38590.1328 - mae: 191.3654\n",
      "Epoch 65/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 38368.0977 - mse: 38368.0977 - mae: 190.8073\n",
      "Epoch 66/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 38143.0703 - mse: 38143.0703 - mae: 190.2421\n",
      "Epoch 67/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 37920.5859 - mse: 37920.5859 - mae: 189.6812\n",
      "Epoch 68/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 37698.4297 - mse: 37698.4297 - mae: 189.1206\n",
      "Epoch 69/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 37479.3789 - mse: 37479.3789 - mae: 188.5623\n",
      "Epoch 70/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 37259.1484 - mse: 37259.1484 - mae: 188.0029\n",
      "Epoch 71/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 37040.5039 - mse: 37040.5039 - mae: 187.4458\n",
      "Epoch 72/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 36823.2148 - mse: 36823.2148 - mae: 186.8877\n",
      "Epoch 73/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 36606.4102 - mse: 36606.4102 - mae: 186.3316\n",
      "Epoch 74/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 36389.9922 - mse: 36389.9922 - mae: 185.7765\n",
      "Epoch 75/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 36175.8555 - mse: 36175.8555 - mae: 185.2215\n",
      "Epoch 76/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 35962.3945 - mse: 35962.3945 - mae: 184.6683\n",
      "Epoch 77/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 35750.2812 - mse: 35750.2812 - mae: 184.1163\n",
      "Epoch 78/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 35537.8008 - mse: 35537.8008 - mae: 183.5641\n",
      "Epoch 79/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 35326.3828 - mse: 35326.3828 - mae: 183.0100\n",
      "Epoch 80/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 35115.1250 - mse: 35115.1250 - mae: 182.4576\n",
      "Epoch 81/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 34906.9492 - mse: 34906.9492 - mae: 181.9059\n",
      "Epoch 82/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 34698.5234 - mse: 34698.5234 - mae: 181.3546\n",
      "Epoch 83/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 34488.6992 - mse: 34488.6992 - mae: 180.8022\n",
      "Epoch 84/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 34283.0508 - mse: 34283.0508 - mae: 180.2550\n",
      "Epoch 85/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 34076.3633 - mse: 34076.3633 - mae: 179.7058\n",
      "Epoch 86/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 33870.7969 - mse: 33870.7969 - mae: 179.1573\n",
      "Epoch 87/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 33666.2734 - mse: 33666.2734 - mae: 178.6096\n",
      "Epoch 88/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 33461.2070 - mse: 33461.2070 - mae: 178.0631\n",
      "Epoch 89/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 33261.8281 - mse: 33261.8281 - mae: 177.5208\n",
      "Epoch 90/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 33059.1172 - mse: 33059.1172 - mae: 176.9734\n",
      "Epoch 91/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 32860.3203 - mse: 32860.3203 - mae: 176.4321\n",
      "Epoch 92/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 32659.1602 - mse: 32659.1602 - mae: 175.8868\n",
      "Epoch 93/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 32459.7480 - mse: 32459.7480 - mae: 175.3433\n",
      "Epoch 94/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 32263.1602 - mse: 32263.1602 - mae: 174.8031\n",
      "Epoch 95/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 32066.3418 - mse: 32066.3418 - mae: 174.2605\n",
      "Epoch 96/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 31870.1367 - mse: 31870.1367 - mae: 173.7194\n",
      "Epoch 97/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 31672.4902 - mse: 31672.4902 - mae: 173.1778\n",
      "Epoch 98/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 31479.4199 - mse: 31479.4199 - mae: 172.6420\n",
      "Epoch 99/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 31287.3223 - mse: 31287.3223 - mae: 172.1064\n",
      "Epoch 100/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 31094.2559 - mse: 31094.2559 - mae: 171.5651\n",
      "[('sgd', 66.99816131591797, 5.82567834854126), ('nesterov', 60.96689224243164, 5.5713605880737305), ('momentum', 65.30890655517578, 5.8633832931518555), ('adam', 31094.255859375, 171.56512451171875)]\n"
     ]
    }
   ],
   "source": [
    "#Przed eksperymentami wyczyść sesję TensorFlow i ustal generatory liczb losowych:\n",
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "opt_pkl = list()\n",
    "\n",
    "for opt in opt_list:\n",
    "    tensorboard_cb = keras.callbacks.TensorBoard(get_run_logdir(\"opt\", opt))\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor=\"loss\",\n",
    "                                                   patience = 10,\n",
    "                                                   min_delta=1.0,\n",
    "                                                   verbose=1)\n",
    "    if opt == \"momentum\": m = 0.5\n",
    "    else: m=0\n",
    "    \n",
    "    model = build_model(optimizer = opt, momentum=m)                      \n",
    "    history = model.fit(X_train, y_train, epochs=100,\n",
    "                    callbacks=[early_stopping, tensorboard_cb])\n",
    "    \n",
    "    opt_pkl.append((opt, history.history[\"mse\"][-1], history.history[\"mae\"][-1]))\n",
    "save_to_pickle(\"opt\", opt_pkl)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8c09cb-31d4-47fe-933b-deaa6e0ffdc0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### eksperyment 5: pęd (mom): 0.1, 0.5, 0.9 (dla algorytmu momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bb3eb51-2521-4ca5-abc3-815e5d20315b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 83728.2500 - mse: 83728.2500 - mae: 99.7617\n",
      "Epoch 2/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 111.8574 - mse: 111.8574 - mae: 7.7539\n",
      "Epoch 3/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 92.5026 - mse: 92.5026 - mae: 7.1335\n",
      "Epoch 4/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 86.4758 - mse: 86.4758 - mae: 6.8017\n",
      "Epoch 5/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 82.8550 - mse: 82.8550 - mae: 6.7412\n",
      "Epoch 6/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 81.9884 - mse: 81.9884 - mae: 6.6559\n",
      "Epoch 7/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 82.4515 - mse: 82.4515 - mae: 6.5522\n",
      "Epoch 8/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 80.9828 - mse: 80.9828 - mae: 6.6153\n",
      "Epoch 9/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 80.6428 - mse: 80.6428 - mae: 6.5149\n",
      "Epoch 10/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 82.5981 - mse: 82.5981 - mae: 6.5743\n",
      "Epoch 11/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 80.8589 - mse: 80.8589 - mae: 6.4981\n",
      "Epoch 12/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 79.4942 - mse: 79.4942 - mae: 6.5238\n",
      "Epoch 13/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 81.7718 - mse: 81.7718 - mae: 6.6398\n",
      "Epoch 14/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 79.9046 - mse: 79.9046 - mae: 6.4869\n",
      "Epoch 15/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 80.6827 - mse: 80.6827 - mae: 6.6561\n",
      "Epoch 16/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 79.8821 - mse: 79.8821 - mae: 6.4749\n",
      "Epoch 17/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 80.1451 - mse: 80.1451 - mae: 6.5279\n",
      "Epoch 18/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 79.8844 - mse: 79.8844 - mae: 6.5677\n",
      "Epoch 19/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 79.3588 - mse: 79.3588 - mae: 6.4619\n",
      "Epoch 20/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 80.3304 - mse: 80.3304 - mae: 6.5723\n",
      "Epoch 21/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 80.2955 - mse: 80.2955 - mae: 6.4736\n",
      "Epoch 22/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 81.7167 - mse: 81.7167 - mae: 6.7221\n",
      "Epoch 22: early stopping\n",
      "Epoch 1/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1500.7212 - mse: 1500.7212 - mae: 20.4882\n",
      "Epoch 2/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 78.3457 - mse: 78.3457 - mae: 6.2454\n",
      "Epoch 3/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 74.1972 - mse: 74.1972 - mae: 6.2878\n",
      "Epoch 4/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 73.2399 - mse: 73.2399 - mae: 6.0311\n",
      "Epoch 5/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 71.4823 - mse: 71.4823 - mae: 6.0967\n",
      "Epoch 6/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 73.1129 - mse: 73.1129 - mae: 6.1710\n",
      "Epoch 7/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 72.6330 - mse: 72.6330 - mae: 6.0132\n",
      "Epoch 8/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 71.0247 - mse: 71.0247 - mae: 6.0697\n",
      "Epoch 9/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 70.8844 - mse: 70.8844 - mae: 5.9891\n",
      "Epoch 10/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 71.5102 - mse: 71.5102 - mae: 6.0463\n",
      "Epoch 11/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 71.4554 - mse: 71.4554 - mae: 6.0461\n",
      "Epoch 12/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 68.5808 - mse: 68.5808 - mae: 5.9657\n",
      "Epoch 13/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 70.0523 - mse: 70.0523 - mae: 6.0490\n",
      "Epoch 14/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 68.3629 - mse: 68.3629 - mae: 5.8742\n",
      "Epoch 15/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 69.3037 - mse: 69.3037 - mae: 6.0251\n",
      "Epoch 16/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 68.3639 - mse: 68.3639 - mae: 5.8333\n",
      "Epoch 17/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 68.0695 - mse: 68.0695 - mae: 5.9067\n",
      "Epoch 18/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 68.2673 - mse: 68.2673 - mae: 5.9490\n",
      "Epoch 19/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 67.6901 - mse: 67.6901 - mae: 5.8555\n",
      "Epoch 20/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 67.0328 - mse: 67.0328 - mae: 5.9008\n",
      "Epoch 21/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 67.4181 - mse: 67.4181 - mae: 5.8405\n",
      "Epoch 22/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 69.4538 - mse: 69.4538 - mae: 6.0479\n",
      "Epoch 23/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 66.7847 - mse: 66.7847 - mae: 5.8510\n",
      "Epoch 24/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 67.7744 - mse: 67.7744 - mae: 5.8986\n",
      "Epoch 25/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 66.4013 - mse: 66.4013 - mae: 5.7794\n",
      "Epoch 26/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 66.6114 - mse: 66.6114 - mae: 5.8960\n",
      "Epoch 27/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 66.2393 - mse: 66.2393 - mae: 5.7932\n",
      "Epoch 28/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 66.2330 - mse: 66.2330 - mae: 5.8746\n",
      "Epoch 29/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 66.6572 - mse: 66.6572 - mae: 5.8640\n",
      "Epoch 30/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 66.4320 - mse: 66.4320 - mae: 5.7575\n",
      "Epoch 30: early stopping\n",
      "Epoch 1/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 132329.8281 - mse: 132329.8281 - mae: 152.9747\n",
      "Epoch 2/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 171.6995 - mse: 171.6995 - mae: 10.9370\n",
      "Epoch 3/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 132.9952 - mse: 132.9952 - mae: 9.0336\n",
      "Epoch 4/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 129.2398 - mse: 129.2398 - mae: 8.6191\n",
      "Epoch 5/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 101.4306 - mse: 101.4306 - mae: 7.7226\n",
      "Epoch 6/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 83.9644 - mse: 83.9644 - mae: 6.9783\n",
      "Epoch 7/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 94.9236 - mse: 94.9236 - mae: 7.2226\n",
      "Epoch 8/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 95.5479 - mse: 95.5479 - mae: 7.5223\n",
      "Epoch 9/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 115.7825 - mse: 115.7825 - mae: 8.1618\n",
      "Epoch 10/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 92.5546 - mse: 92.5546 - mae: 7.5566\n",
      "Epoch 11/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 85.3452 - mse: 85.3452 - mae: 6.7685\n",
      "Epoch 12/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 81.6809 - mse: 81.6809 - mae: 6.8650\n",
      "Epoch 13/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 83.7059 - mse: 83.7059 - mae: 6.9034\n",
      "Epoch 14/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 77.5821 - mse: 77.5821 - mae: 6.4778\n",
      "Epoch 15/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 84.3020 - mse: 84.3020 - mae: 7.0305\n",
      "Epoch 16/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 78.9982 - mse: 78.9982 - mae: 6.4737\n",
      "Epoch 17/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 74.6747 - mse: 74.6747 - mae: 6.4302\n",
      "Epoch 18/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 71.7094 - mse: 71.7094 - mae: 6.1822\n",
      "Epoch 19/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 71.3598 - mse: 71.3598 - mae: 6.2173\n",
      "Epoch 20/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 72.8437 - mse: 72.8437 - mae: 6.3049\n",
      "Epoch 21/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 69.2573 - mse: 69.2573 - mae: 5.9938\n",
      "Epoch 22/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 69.9400 - mse: 69.9400 - mae: 6.3149\n",
      "Epoch 23/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 68.3567 - mse: 68.3567 - mae: 5.9770\n",
      "Epoch 24/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 78.5231 - mse: 78.5231 - mae: 6.6479\n",
      "Epoch 25/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 74.2674 - mse: 74.2674 - mae: 6.2541\n",
      "Epoch 26/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 74.7793 - mse: 74.7793 - mae: 6.5780\n",
      "Epoch 27/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 76.8357 - mse: 76.8357 - mae: 6.4967\n",
      "Epoch 28/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 69.1284 - mse: 69.1284 - mae: 6.3107\n",
      "Epoch 29/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 72.6877 - mse: 72.6877 - mae: 6.1467\n",
      "Epoch 30/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 66.8524 - mse: 66.8524 - mae: 6.1205\n",
      "Epoch 31/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 69.5409 - mse: 69.5409 - mae: 6.0271\n",
      "Epoch 32/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 73.7057 - mse: 73.7057 - mae: 6.4443\n",
      "Epoch 33/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 64.2732 - mse: 64.2732 - mae: 5.7303\n",
      "Epoch 34/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 73.3435 - mse: 73.3435 - mae: 6.4212\n",
      "Epoch 35/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 67.4653 - mse: 67.4653 - mae: 6.1788\n",
      "Epoch 36/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 71.1196 - mse: 71.1196 - mae: 6.3436\n",
      "Epoch 37/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 72.6826 - mse: 72.6826 - mae: 6.3817\n",
      "Epoch 38/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 65.1148 - mse: 65.1148 - mae: 5.7429\n",
      "Epoch 39/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 73.0974 - mse: 73.0974 - mae: 6.5081\n",
      "Epoch 40/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 68.5402 - mse: 68.5402 - mae: 6.2377\n",
      "Epoch 41/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 63.1750 - mse: 63.1750 - mae: 5.8122\n",
      "Epoch 42/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 75.6622 - mse: 75.6622 - mae: 6.5140\n",
      "Epoch 43/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 72.6444 - mse: 72.6444 - mae: 6.3890\n",
      "Epoch 44/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 62.6395 - mse: 62.6395 - mae: 5.8245\n",
      "Epoch 45/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 63.4122 - mse: 63.4122 - mae: 6.0021\n",
      "Epoch 46/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 59.7939 - mse: 59.7939 - mae: 5.6778\n",
      "Epoch 47/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 63.7989 - mse: 63.7989 - mae: 5.8716\n",
      "Epoch 48/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 67.4970 - mse: 67.4970 - mae: 6.1259\n",
      "Epoch 49/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 71.4219 - mse: 71.4219 - mae: 6.2986\n",
      "Epoch 50/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 73.2697 - mse: 73.2697 - mae: 6.3521\n",
      "Epoch 51/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 63.4217 - mse: 63.4217 - mae: 5.9783\n",
      "Epoch 52/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 70.1339 - mse: 70.1339 - mae: 6.3134\n",
      "Epoch 53/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 63.0319 - mse: 63.0319 - mae: 5.9238\n",
      "Epoch 54/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 63.0576 - mse: 63.0576 - mae: 5.9097\n",
      "Epoch 55/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 62.4765 - mse: 62.4765 - mae: 5.8460\n",
      "Epoch 56/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 59.4753 - mse: 59.4753 - mae: 5.8455\n",
      "Epoch 56: early stopping\n",
      "[(0.1, 81.71673583984375, 6.722068786621094), (0.5, 66.43197631835938, 5.7575483322143555), (0.9, 59.475284576416016, 5.845536231994629)]\n"
     ]
    }
   ],
   "source": [
    "#Przed eksperymentami wyczyść sesję TensorFlow i ustal generatory liczb losowych:\n",
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "mom_pkl = list()\n",
    "\n",
    "for mom in mom_list:\n",
    "    tensorboard_cb = keras.callbacks.TensorBoard(get_run_logdir(\"mom\", mom))\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor=\"loss\",\n",
    "                                                   patience = 10,\n",
    "                                                   min_delta=1.0,\n",
    "                                                   verbose=1)\n",
    "    \n",
    "    model = build_model(optimizer = \"momentum\", momentum=mom)                      \n",
    "    history = model.fit(X_train, y_train, epochs=100,\n",
    "                    callbacks=[early_stopping, tensorboard_cb])\n",
    "    \n",
    "    mom_pkl.append((mom, history.history[\"mse\"][-1], history.history[\"mae\"][-1]))\n",
    "save_to_pickle(\"mom\", mom_pkl) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aab3457-4022-41ac-b1a2-4bffb6af4b24",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Autonomiczne przeszukiwanie przestrzeni hiperparametrów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b0e3430-c6d6-4e1b-ae52-48106c972bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#slownik zawierajacy przeszukiwanie wartosci parametrow\n",
    "param_distribs = {\n",
    "    \"model__n_hidden\": [0, 1, 2, 3],\n",
    "    \"model__n_neurons\": [5, 25, 125],\n",
    "    \"model__learning_rate\": [1e-6, 1e-5, 1e-4],\n",
    "    \"model__optimizer\": [\"sgd\", \"nesterov\", \"momentum\", \"adam\"],\n",
    "    \"model__momentum\": [0.1, 0.5, 0.9]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "847f0f3f-f518-40d6-bbed-3c5da3fe20cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikeras\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(patience=10, min_delta=1.0, verbose=1)\n",
    "\n",
    "keras_reg = KerasRegressor(build_model, callbacks=[es])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81d0965-ea6b-4859-aa8b-fa3be5549295",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a03063e9-cc41-4bbb-9cce-31bb2629f551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 28ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 10: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.9, model__n_hidden=3, model__n_neurons=25, model__optimizer=sgd; total time=   1.2s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 22ms/step - loss: 7657043506052988928.0000 - mse: 7657043506052988928.0000 - mae: 1007819584.0000 - val_loss: 2308499570688.0000 - val_mse: 2308499570688.0000 - val_mae: 1519374.6250\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2305448214528.0000 - mse: 2305448214528.0000 - mae: 1518370.1250 - val_loss: 2301122838528.0000 - val_mse: 2301122838528.0000 - val_mae: 1516945.1250\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2298081443840.0000 - mse: 2298081443840.0000 - mae: 1515942.2500 - val_loss: 2293769961472.0000 - val_mse: 2293769961472.0000 - val_mae: 1514519.7500\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 2290738528256.0000 - mse: 2290738528256.0000 - mae: 1513518.5000 - val_loss: 2286441201664.0000 - val_mse: 2286441201664.0000 - val_mae: 1512098.2500\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2283419205632.0000 - mse: 2283419205632.0000 - mae: 1511098.6250 - val_loss: 2279136034816.0000 - val_mse: 2279136034816.0000 - val_mae: 1509680.7500\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2276123475968.0000 - mse: 2276123475968.0000 - mae: 1508682.6250 - val_loss: 2271853412352.0000 - val_mse: 2271853412352.0000 - val_mae: 1507266.8750\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2268850290688.0000 - mse: 2268850290688.0000 - mae: 1506270.2500 - val_loss: 2264594120704.0000 - val_mse: 2264594120704.0000 - val_mae: 1504856.8750\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2261600960512.0000 - mse: 2261600960512.0000 - mae: 1503862.0000 - val_loss: 2257358684160.0000 - val_mse: 2257358684160.0000 - val_mae: 1502450.8750\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2254375223296.0000 - mse: 2254375223296.0000 - mae: 1501457.6250 - val_loss: 2250146054144.0000 - val_mse: 2250146054144.0000 - val_mae: 1500048.6250\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2247171768320.0000 - mse: 2247171768320.0000 - mae: 1499056.7500 - val_loss: 2242955706368.0000 - val_mse: 2242955706368.0000 - val_mae: 1497650.1250\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2239991382016.0000 - mse: 2239991382016.0000 - mae: 1496660.0000 - val_loss: 2235789213696.0000 - val_mse: 2235789213696.0000 - val_mae: 1495255.5000\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2232834588672.0000 - mse: 2232834588672.0000 - mae: 1494267.1250 - val_loss: 2228645789696.0000 - val_mse: 2228645789696.0000 - val_mae: 1492865.0000\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2225700077568.0000 - mse: 2225700077568.0000 - mae: 1491877.8750 - val_loss: 2221524647936.0000 - val_mse: 2221524647936.0000 - val_mae: 1490478.1250\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2218588635136.0000 - mse: 2218588635136.0000 - mae: 1489492.5000 - val_loss: 2214426574848.0000 - val_mse: 2214426574848.0000 - val_mae: 1488095.1250\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2211499999232.0000 - mse: 2211499999232.0000 - mae: 1487111.1250 - val_loss: 2207350784000.0000 - val_mse: 2207350784000.0000 - val_mae: 1485715.5000\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2204433383424.0000 - mse: 2204433383424.0000 - mae: 1484733.3750 - val_loss: 2200297799680.0000 - val_mse: 2200297799680.0000 - val_mae: 1483340.0000\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2197389967360.0000 - mse: 2197389967360.0000 - mae: 1482359.5000 - val_loss: 2193268015104.0000 - val_mse: 2193268015104.0000 - val_mae: 1480968.6250\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2190368702464.0000 - mse: 2190368702464.0000 - mae: 1479989.2500 - val_loss: 2186259857408.0000 - val_mse: 2186259857408.0000 - val_mae: 1478600.6250\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2183370113024.0000 - mse: 2183370113024.0000 - mae: 1477622.8750 - val_loss: 2179274637312.0000 - val_mse: 2179274637312.0000 - val_mae: 1476236.6250\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2176393805824.0000 - mse: 2176393805824.0000 - mae: 1475260.3750 - val_loss: 2172310781952.0000 - val_mse: 2172310781952.0000 - val_mae: 1473876.0000\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2169439518720.0000 - mse: 2169439518720.0000 - mae: 1472901.8750 - val_loss: 2165369864192.0000 - val_mse: 2165369864192.0000 - val_mae: 1471519.5000\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2162508038144.0000 - mse: 2162508038144.0000 - mae: 1470546.6250 - val_loss: 2158451228672.0000 - val_mse: 2158451228672.0000 - val_mae: 1469166.8750\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2155598184448.0000 - mse: 2155598184448.0000 - mae: 1468195.5000 - val_loss: 2151554613248.0000 - val_mse: 2151554613248.0000 - val_mae: 1466817.8750\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2148710875136.0000 - mse: 2148710875136.0000 - mae: 1465848.1250 - val_loss: 2144680017920.0000 - val_mse: 2144680017920.0000 - val_mae: 1464472.6250\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2141845192704.0000 - mse: 2141845192704.0000 - mae: 1463504.2500 - val_loss: 2137827311616.0000 - val_mse: 2137827311616.0000 - val_mae: 1462131.0000\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2135001661440.0000 - mse: 2135001661440.0000 - mae: 1461164.2500 - val_loss: 2130996625408.0000 - val_mse: 2130996625408.0000 - val_mae: 1459793.3750\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2128179757056.0000 - mse: 2128179757056.0000 - mae: 1458828.1250 - val_loss: 2124187828224.0000 - val_mse: 2124187828224.0000 - val_mae: 1457459.3750\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2121380003840.0000 - mse: 2121380003840.0000 - mae: 1456495.6250 - val_loss: 2117400395776.0000 - val_mse: 2117400395776.0000 - val_mae: 1455129.0000\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2114601746432.0000 - mse: 2114601746432.0000 - mae: 1454166.8750 - val_loss: 2110635114496.0000 - val_mse: 2110635114496.0000 - val_mae: 1452802.5000\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2107845378048.0000 - mse: 2107845378048.0000 - mae: 1451841.8750 - val_loss: 2103891066880.0000 - val_mse: 2103891066880.0000 - val_mae: 1450479.5000\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2101109981184.0000 - mse: 2101109981184.0000 - mae: 1449520.5000 - val_loss: 2097168777216.0000 - val_mse: 2097168777216.0000 - val_mae: 1448160.6250\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2094396604416.0000 - mse: 2094396604416.0000 - mae: 1447202.7500 - val_loss: 2090467721216.0000 - val_mse: 2090467721216.0000 - val_mae: 1445845.0000\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2087704592384.0000 - mse: 2087704592384.0000 - mae: 1444889.1250 - val_loss: 2083788685312.0000 - val_mse: 2083788685312.0000 - val_mae: 1443533.3750\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2081034076160.0000 - mse: 2081034076160.0000 - mae: 1442578.7500 - val_loss: 2077130620928.0000 - val_mse: 2077130620928.0000 - val_mae: 1441225.3750\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2074385055744.0000 - mse: 2074385055744.0000 - mae: 1440272.3750 - val_loss: 2070493659136.0000 - val_mse: 2070493659136.0000 - val_mae: 1438921.0000\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2067756875776.0000 - mse: 2067756875776.0000 - mae: 1437969.7500 - val_loss: 2063878062080.0000 - val_mse: 2063878062080.0000 - val_mae: 1436620.5000\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2061149667328.0000 - mse: 2061149667328.0000 - mae: 1435670.3750 - val_loss: 2057283436544.0000 - val_mse: 2057283436544.0000 - val_mae: 1434323.3750\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2054564085760.0000 - mse: 2054564085760.0000 - mae: 1433375.0000 - val_loss: 2050709913600.0000 - val_mse: 2050709913600.0000 - val_mae: 1432030.1250\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2047999213568.0000 - mse: 2047999213568.0000 - mae: 1431083.1250 - val_loss: 2044157886464.0000 - val_mse: 2044157886464.0000 - val_mae: 1429740.6250\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2041455706112.0000 - mse: 2041455706112.0000 - mae: 1428795.1250 - val_loss: 2037626306560.0000 - val_mse: 2037626306560.0000 - val_mae: 1427454.5000\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2034932776960.0000 - mse: 2034932776960.0000 - mae: 1426510.5000 - val_loss: 2031115436032.0000 - val_mse: 2031115436032.0000 - val_mae: 1425172.0000\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2028430688256.0000 - mse: 2028430688256.0000 - mae: 1424229.5000 - val_loss: 2024625930240.0000 - val_mse: 2024625930240.0000 - val_mae: 1422893.5000\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2021949571072.0000 - mse: 2021949571072.0000 - mae: 1421952.6250 - val_loss: 2018156740608.0000 - val_mse: 2018156740608.0000 - val_mae: 1420618.5000\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2015488770048.0000 - mse: 2015488770048.0000 - mae: 1419679.1250 - val_loss: 2011708129280.0000 - val_mse: 2011708129280.0000 - val_mae: 1418347.0000\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2009049071616.0000 - mse: 2009049071616.0000 - mae: 1417409.2500 - val_loss: 2005280620544.0000 - val_mse: 2005280620544.0000 - val_mae: 1416079.2500\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2002629558272.0000 - mse: 2002629558272.0000 - mae: 1415142.8750 - val_loss: 1998873296896.0000 - val_mse: 1998873296896.0000 - val_mae: 1413815.1250\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1996230885376.0000 - mse: 1996230885376.0000 - mae: 1412880.2500 - val_loss: 1992486420480.0000 - val_mse: 1992486420480.0000 - val_mae: 1411554.5000\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1989852528640.0000 - mse: 1989852528640.0000 - mae: 1410621.1250 - val_loss: 1986120122368.0000 - val_mse: 1986120122368.0000 - val_mae: 1409297.7500\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1983494619136.0000 - mse: 1983494619136.0000 - mae: 1408365.8750 - val_loss: 1979773747200.0000 - val_mse: 1979773747200.0000 - val_mae: 1407044.5000\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1977156894720.0000 - mse: 1977156894720.0000 - mae: 1406114.0000 - val_loss: 1973448212480.0000 - val_mse: 1973448212480.0000 - val_mae: 1404794.6250\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1970839486464.0000 - mse: 1970839486464.0000 - mae: 1403865.7500 - val_loss: 1967142600704.0000 - val_mse: 1967142600704.0000 - val_mae: 1402548.6250\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1964542132224.0000 - mse: 1964542132224.0000 - mae: 1401621.0000 - val_loss: 1960857305088.0000 - val_mse: 1960857305088.0000 - val_mae: 1400306.1250\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1958264963072.0000 - mse: 1958264963072.0000 - mae: 1399380.1250 - val_loss: 1954591932416.0000 - val_mse: 1954591932416.0000 - val_mae: 1398067.2500\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1952007979008.0000 - mse: 1952007979008.0000 - mae: 1397142.6250 - val_loss: 1948346482688.0000 - val_mse: 1948346482688.0000 - val_mae: 1395831.8750\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1945770917888.0000 - mse: 1945770917888.0000 - mae: 1394908.8750 - val_loss: 1942121349120.0000 - val_mse: 1942121349120.0000 - val_mae: 1393600.0000\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1939553910784.0000 - mse: 1939553910784.0000 - mae: 1392678.5000 - val_loss: 1935915614208.0000 - val_mse: 1935915614208.0000 - val_mae: 1391371.8750\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1933356302336.0000 - mse: 1933356302336.0000 - mae: 1390451.7500 - val_loss: 1929729933312.0000 - val_mse: 1929729933312.0000 - val_mae: 1389147.2500\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1927178878976.0000 - mse: 1927178878976.0000 - mae: 1388228.6250 - val_loss: 1923564044288.0000 - val_mse: 1923564044288.0000 - val_mae: 1386926.1250\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1921020854272.0000 - mse: 1921020854272.0000 - mae: 1386008.8750 - val_loss: 1917417947136.0000 - val_mse: 1917417947136.0000 - val_mae: 1384708.6250\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1914882883584.0000 - mse: 1914882883584.0000 - mae: 1383792.7500 - val_loss: 1911291510784.0000 - val_mse: 1911291510784.0000 - val_mae: 1382494.5000\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1908764442624.0000 - mse: 1908764442624.0000 - mae: 1381580.1250 - val_loss: 1905184079872.0000 - val_mse: 1905184079872.0000 - val_mae: 1380284.0000\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1902665662464.0000 - mse: 1902665662464.0000 - mae: 1379371.2500 - val_loss: 1899096834048.0000 - val_mse: 1899096834048.0000 - val_mae: 1378077.1250\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1896586149888.0000 - mse: 1896586149888.0000 - mae: 1377165.8750 - val_loss: 1893028724736.0000 - val_mse: 1893028724736.0000 - val_mae: 1375873.8750\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1890526035968.0000 - mse: 1890526035968.0000 - mae: 1374964.0000 - val_loss: 1886980276224.0000 - val_mse: 1886980276224.0000 - val_mae: 1373674.1250\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1884485582848.0000 - mse: 1884485582848.0000 - mae: 1372765.3750 - val_loss: 1880950833152.0000 - val_mse: 1880950833152.0000 - val_mae: 1371477.5000\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1878464266240.0000 - mse: 1878464266240.0000 - mae: 1370570.6250 - val_loss: 1874941181952.0000 - val_mse: 1874941181952.0000 - val_mae: 1369284.8750\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1872462217216.0000 - mse: 1872462217216.0000 - mae: 1368379.2500 - val_loss: 1868950011904.0000 - val_mse: 1868950011904.0000 - val_mae: 1367095.5000\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1866479173632.0000 - mse: 1866479173632.0000 - mae: 1366191.3750 - val_loss: 1862978764800.0000 - val_mse: 1862978764800.0000 - val_mae: 1364909.6250\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1860515528704.0000 - mse: 1860515528704.0000 - mae: 1364007.0000 - val_loss: 1857025867776.0000 - val_mse: 1857025867776.0000 - val_mae: 1362727.3750\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1854570758144.0000 - mse: 1854570758144.0000 - mae: 1361826.1250 - val_loss: 1851092500480.0000 - val_mse: 1851092500480.0000 - val_mae: 1360548.6250\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1848645386240.0000 - mse: 1848645386240.0000 - mae: 1359648.7500 - val_loss: 1845177745408.0000 - val_mse: 1845177745408.0000 - val_mae: 1358373.1250\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1842738364416.0000 - mse: 1842738364416.0000 - mae: 1357474.8750 - val_loss: 1839282126848.0000 - val_mse: 1839282126848.0000 - val_mae: 1356201.3750\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1836850610176.0000 - mse: 1836850610176.0000 - mae: 1355304.5000 - val_loss: 1833405251584.0000 - val_mse: 1833405251584.0000 - val_mae: 1354033.0000\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1830981206016.0000 - mse: 1830981206016.0000 - mae: 1353137.5000 - val_loss: 1827547250688.0000 - val_mse: 1827547250688.0000 - val_mae: 1351868.0000\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1825131069440.0000 - mse: 1825131069440.0000 - mae: 1350974.1250 - val_loss: 1821707730944.0000 - val_mse: 1821707730944.0000 - val_mae: 1349706.5000\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1819299282944.0000 - mse: 1819299282944.0000 - mae: 1348813.8750 - val_loss: 1815886823424.0000 - val_mse: 1815886823424.0000 - val_mae: 1347548.6250\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1813485977600.0000 - mse: 1813485977600.0000 - mae: 1346657.2500 - val_loss: 1810084921344.0000 - val_mse: 1810084921344.0000 - val_mae: 1345394.1250\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1807691808768.0000 - mse: 1807691808768.0000 - mae: 1344504.1250 - val_loss: 1804301369344.0000 - val_mse: 1804301369344.0000 - val_mae: 1343242.8750\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1801915858944.0000 - mse: 1801915858944.0000 - mae: 1342354.5000 - val_loss: 1798536298496.0000 - val_mse: 1798536298496.0000 - val_mae: 1341095.2500\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1796158521344.0000 - mse: 1796158521344.0000 - mae: 1340208.2500 - val_loss: 1792789839872.0000 - val_mse: 1792789839872.0000 - val_mae: 1338951.1250\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1790419533824.0000 - mse: 1790419533824.0000 - mae: 1338065.5000 - val_loss: 1787061469184.0000 - val_mse: 1787061469184.0000 - val_mae: 1336810.2500\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1784698765312.0000 - mse: 1784698765312.0000 - mae: 1335926.1250 - val_loss: 1781351710720.0000 - val_mse: 1781351710720.0000 - val_mae: 1334672.8750\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1778996346880.0000 - mse: 1778996346880.0000 - mae: 1333790.1250 - val_loss: 1775660302336.0000 - val_mse: 1775660302336.0000 - val_mae: 1332539.0000\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1773312409600.0000 - mse: 1773312409600.0000 - mae: 1331657.5000 - val_loss: 1769986326528.0000 - val_mse: 1769986326528.0000 - val_mae: 1330408.2500\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1767646167040.0000 - mse: 1767646167040.0000 - mae: 1329528.3750 - val_loss: 1764330569728.0000 - val_mse: 1764330569728.0000 - val_mae: 1328281.0000\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1761997881344.0000 - mse: 1761997881344.0000 - mae: 1327402.6250 - val_loss: 1758693294080.0000 - val_mse: 1758693294080.0000 - val_mae: 1326157.3750\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1756367814656.0000 - mse: 1756367814656.0000 - mae: 1325280.1250 - val_loss: 1753073975296.0000 - val_mse: 1753073975296.0000 - val_mae: 1324037.0000\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1750756229120.0000 - mse: 1750756229120.0000 - mae: 1323161.2500 - val_loss: 1747472875520.0000 - val_mse: 1747472875520.0000 - val_mae: 1321920.0000\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1745162207232.0000 - mse: 1745162207232.0000 - mae: 1321045.8750 - val_loss: 1741889208320.0000 - val_mse: 1741889208320.0000 - val_mae: 1319806.5000\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1739586142208.0000 - mse: 1739586142208.0000 - mae: 1318933.5000 - val_loss: 1736323497984.0000 - val_mse: 1736323497984.0000 - val_mae: 1317696.1250\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1734027640832.0000 - mse: 1734027640832.0000 - mae: 1316824.6250 - val_loss: 1730775613440.0000 - val_mse: 1730775613440.0000 - val_mae: 1315589.5000\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1728487096320.0000 - mse: 1728487096320.0000 - mae: 1314719.2500 - val_loss: 1725245423616.0000 - val_mse: 1725245423616.0000 - val_mae: 1313486.1250\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 1722964377600.0000 - mse: 1722964377600.0000 - mae: 1312617.2500 - val_loss: 1719732797440.0000 - val_mse: 1719732797440.0000 - val_mae: 1311385.8750\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1717459091456.0000 - mse: 1717459091456.0000 - mae: 1310518.5000 - val_loss: 1714237997056.0000 - val_mse: 1714237997056.0000 - val_mae: 1309289.0000\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1711971368960.0000 - mse: 1711971368960.0000 - mae: 1308423.1250 - val_loss: 1708760629248.0000 - val_mse: 1708760629248.0000 - val_mae: 1307195.7500\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1706501210112.0000 - mse: 1706501210112.0000 - mae: 1306331.1250 - val_loss: 1703300694016.0000 - val_mse: 1703300694016.0000 - val_mae: 1305105.5000\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1701048614912.0000 - mse: 1701048614912.0000 - mae: 1304242.3750 - val_loss: 1697858453504.0000 - val_mse: 1697858453504.0000 - val_mae: 1303019.0000\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1695613452288.0000 - mse: 1695613452288.0000 - mae: 1302157.2500 - val_loss: 1692433383424.0000 - val_mse: 1692433383424.0000 - val_mae: 1300935.5000\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1690195722240.0000 - mse: 1690195722240.0000 - mae: 1300075.1250 - val_loss: 1687026008064.0000 - val_mse: 1687026008064.0000 - val_mae: 1298855.5000\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1684795293696.0000 - mse: 1684795293696.0000 - mae: 1297996.5000 - val_loss: 1681635672064.0000 - val_mse: 1681635672064.0000 - val_mae: 1296779.0000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.9, model__n_hidden=3, model__n_neurons=25, model__optimizer=sgd; total time=   5.3s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 22ms/step - loss: inf - mse: inf - mae: 358512953802130194432.0000 - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 10: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.9, model__n_hidden=3, model__n_neurons=25, model__optimizer=sgd; total time=   1.0s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 21ms/step - loss: 21785.2227 - mse: 21785.2227 - mae: 146.3550 - val_loss: 20512.6641 - val_mse: 20512.6641 - val_mae: 142.1184\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21782.3613 - mse: 21782.3613 - mae: 146.3454 - val_loss: 20509.8711 - val_mse: 20509.8711 - val_mae: 142.1087\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21779.5137 - mse: 21779.5137 - mae: 146.3358 - val_loss: 20507.0664 - val_mse: 20507.0664 - val_mae: 142.0990\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21776.6660 - mse: 21776.6660 - mae: 146.3262 - val_loss: 20504.2734 - val_mse: 20504.2734 - val_mae: 142.0893\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21773.8379 - mse: 21773.8379 - mae: 146.3167 - val_loss: 20501.4668 - val_mse: 20501.4668 - val_mae: 142.0796\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21770.9648 - mse: 21770.9648 - mae: 146.3070 - val_loss: 20498.6875 - val_mse: 20498.6875 - val_mae: 142.0700\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21768.1367 - mse: 21768.1367 - mae: 146.2975 - val_loss: 20495.8848 - val_mse: 20495.8848 - val_mae: 142.0603\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21765.2754 - mse: 21765.2754 - mae: 146.2879 - val_loss: 20493.1152 - val_mse: 20493.1152 - val_mae: 142.0507\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21762.4570 - mse: 21762.4570 - mae: 146.2784 - val_loss: 20490.3164 - val_mse: 20490.3164 - val_mae: 142.0410\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21759.6074 - mse: 21759.6074 - mae: 146.2688 - val_loss: 20487.5332 - val_mse: 20487.5332 - val_mae: 142.0313\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21756.7852 - mse: 21756.7852 - mae: 146.2592 - val_loss: 20484.7461 - val_mse: 20484.7461 - val_mae: 142.0217\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21753.9492 - mse: 21753.9492 - mae: 146.2497 - val_loss: 20481.9902 - val_mse: 20481.9902 - val_mae: 142.0121\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21751.1152 - mse: 21751.1152 - mae: 146.2402 - val_loss: 20479.2246 - val_mse: 20479.2246 - val_mae: 142.0025\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21748.3047 - mse: 21748.3047 - mae: 146.2307 - val_loss: 20476.4355 - val_mse: 20476.4355 - val_mae: 141.9929\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 21745.4531 - mse: 21745.4531 - mae: 146.2211 - val_loss: 20473.6621 - val_mse: 20473.6621 - val_mae: 141.9833\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21742.6152 - mse: 21742.6152 - mae: 146.2116 - val_loss: 20470.8730 - val_mse: 20470.8730 - val_mae: 141.9736\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21739.7832 - mse: 21739.7832 - mae: 146.2020 - val_loss: 20468.0762 - val_mse: 20468.0762 - val_mae: 141.9639\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21736.9551 - mse: 21736.9551 - mae: 146.1925 - val_loss: 20465.2832 - val_mse: 20465.2832 - val_mae: 141.9542\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21734.0996 - mse: 21734.0996 - mae: 146.1829 - val_loss: 20462.5156 - val_mse: 20462.5156 - val_mae: 141.9446\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21731.2812 - mse: 21731.2812 - mae: 146.1734 - val_loss: 20459.7539 - val_mse: 20459.7539 - val_mae: 141.9350\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21728.4531 - mse: 21728.4531 - mae: 146.1639 - val_loss: 20456.9727 - val_mse: 20456.9727 - val_mae: 141.9254\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21725.6309 - mse: 21725.6309 - mae: 146.1543 - val_loss: 20454.1777 - val_mse: 20454.1777 - val_mae: 141.9157\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21722.7930 - mse: 21722.7930 - mae: 146.1448 - val_loss: 20451.3828 - val_mse: 20451.3828 - val_mae: 141.9060\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21719.9453 - mse: 21719.9453 - mae: 146.1352 - val_loss: 20448.6133 - val_mse: 20448.6133 - val_mae: 141.8964\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21717.1211 - mse: 21717.1211 - mae: 146.1256 - val_loss: 20445.8477 - val_mse: 20445.8477 - val_mae: 141.8868\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21714.3066 - mse: 21714.3066 - mae: 146.1162 - val_loss: 20443.0703 - val_mse: 20443.0703 - val_mae: 141.8771\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 21711.4707 - mse: 21711.4707 - mae: 146.1066 - val_loss: 20440.3047 - val_mse: 20440.3047 - val_mae: 141.8676\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21708.6562 - mse: 21708.6562 - mae: 146.0971 - val_loss: 20437.5332 - val_mse: 20437.5332 - val_mae: 141.8579\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21705.8594 - mse: 21705.8594 - mae: 146.0877 - val_loss: 20434.7363 - val_mse: 20434.7363 - val_mae: 141.8482\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21702.9883 - mse: 21702.9883 - mae: 146.0780 - val_loss: 20431.9766 - val_mse: 20431.9766 - val_mae: 141.8387\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21700.1797 - mse: 21700.1797 - mae: 146.0686 - val_loss: 20429.2090 - val_mse: 20429.2090 - val_mae: 141.8290\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21697.3535 - mse: 21697.3535 - mae: 146.0590 - val_loss: 20426.4102 - val_mse: 20426.4102 - val_mae: 141.8193\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21694.5293 - mse: 21694.5293 - mae: 146.0494 - val_loss: 20423.6152 - val_mse: 20423.6152 - val_mae: 141.8096\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21691.6738 - mse: 21691.6738 - mae: 146.0399 - val_loss: 20420.8652 - val_mse: 20420.8652 - val_mae: 141.8001\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21688.8633 - mse: 21688.8633 - mae: 146.0304 - val_loss: 20418.1133 - val_mse: 20418.1133 - val_mae: 141.7905\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21686.0547 - mse: 21686.0547 - mae: 146.0209 - val_loss: 20415.3301 - val_mse: 20415.3301 - val_mae: 141.7809\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21683.2559 - mse: 21683.2559 - mae: 146.0114 - val_loss: 20412.5273 - val_mse: 20412.5273 - val_mae: 141.7711\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 21680.3867 - mse: 21680.3867 - mae: 146.0018 - val_loss: 20409.7676 - val_mse: 20409.7676 - val_mae: 141.7616\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 21677.5781 - mse: 21677.5781 - mae: 145.9923 - val_loss: 20406.9980 - val_mse: 20406.9980 - val_mae: 141.7519\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21674.7480 - mse: 21674.7480 - mae: 145.9828 - val_loss: 20404.2422 - val_mse: 20404.2422 - val_mae: 141.7424\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21671.9395 - mse: 21671.9395 - mae: 145.9733 - val_loss: 20401.4746 - val_mse: 20401.4746 - val_mae: 141.7327\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21669.1133 - mse: 21669.1133 - mae: 145.9638 - val_loss: 20398.7246 - val_mse: 20398.7246 - val_mae: 141.7232\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21666.3086 - mse: 21666.3086 - mae: 145.9543 - val_loss: 20395.9746 - val_mse: 20395.9746 - val_mae: 141.7136\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21663.5078 - mse: 21663.5078 - mae: 145.9449 - val_loss: 20393.1992 - val_mse: 20393.1992 - val_mae: 141.7040\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 21660.6641 - mse: 21660.6641 - mae: 145.9353 - val_loss: 20390.4453 - val_mse: 20390.4453 - val_mae: 141.6944\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 21657.8730 - mse: 21657.8730 - mae: 145.9258 - val_loss: 20387.6641 - val_mse: 20387.6641 - val_mae: 141.6848\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21655.0488 - mse: 21655.0488 - mae: 145.9163 - val_loss: 20384.8887 - val_mse: 20384.8887 - val_mae: 141.6751\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 21652.2168 - mse: 21652.2168 - mae: 145.9068 - val_loss: 20382.1484 - val_mse: 20382.1484 - val_mae: 141.6656\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 21649.4277 - mse: 21649.4277 - mae: 145.8973 - val_loss: 20379.3965 - val_mse: 20379.3965 - val_mae: 141.6560\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21646.6172 - mse: 21646.6172 - mae: 145.8879 - val_loss: 20376.6426 - val_mse: 20376.6426 - val_mae: 141.6465\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21643.8105 - mse: 21643.8105 - mae: 145.8784 - val_loss: 20373.8789 - val_mse: 20373.8789 - val_mae: 141.6369\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21640.9980 - mse: 21640.9980 - mae: 145.8689 - val_loss: 20371.1113 - val_mse: 20371.1113 - val_mae: 141.6272\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21638.1875 - mse: 21638.1875 - mae: 145.8594 - val_loss: 20368.3398 - val_mse: 20368.3398 - val_mae: 141.6176\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21635.3672 - mse: 21635.3672 - mae: 145.8499 - val_loss: 20365.5957 - val_mse: 20365.5957 - val_mae: 141.6081\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 21632.5762 - mse: 21632.5762 - mae: 145.8404 - val_loss: 20362.8203 - val_mse: 20362.8203 - val_mae: 141.5984\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21629.7344 - mse: 21629.7344 - mae: 145.8309 - val_loss: 20360.0781 - val_mse: 20360.0781 - val_mae: 141.5889\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21626.9453 - mse: 21626.9453 - mae: 145.8215 - val_loss: 20357.3105 - val_mse: 20357.3105 - val_mae: 141.5793\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21624.1211 - mse: 21624.1211 - mae: 145.8119 - val_loss: 20354.5645 - val_mse: 20354.5645 - val_mae: 141.5697\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21621.3086 - mse: 21621.3086 - mae: 145.8024 - val_loss: 20351.8105 - val_mse: 20351.8105 - val_mae: 141.5602\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21618.5254 - mse: 21618.5254 - mae: 145.7930 - val_loss: 20349.0215 - val_mse: 20349.0215 - val_mae: 141.5504\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 21615.6738 - mse: 21615.6738 - mae: 145.7834 - val_loss: 20346.2754 - val_mse: 20346.2754 - val_mae: 141.5409\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21612.8809 - mse: 21612.8809 - mae: 145.7739 - val_loss: 20343.5352 - val_mse: 20343.5352 - val_mae: 141.5314\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21610.0762 - mse: 21610.0762 - mae: 145.7645 - val_loss: 20340.7773 - val_mse: 20340.7773 - val_mae: 141.5218\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21607.2773 - mse: 21607.2773 - mae: 145.7550 - val_loss: 20338.0215 - val_mse: 20338.0215 - val_mae: 141.5122\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21604.4609 - mse: 21604.4609 - mae: 145.7455 - val_loss: 20335.2617 - val_mse: 20335.2617 - val_mae: 141.5026\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21601.6523 - mse: 21601.6523 - mae: 145.7360 - val_loss: 20332.4980 - val_mse: 20332.4980 - val_mae: 141.4930\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21598.8457 - mse: 21598.8457 - mae: 145.7265 - val_loss: 20329.7246 - val_mse: 20329.7246 - val_mae: 141.4833\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21596.0195 - mse: 21596.0195 - mae: 145.7170 - val_loss: 20326.9648 - val_mse: 20326.9648 - val_mae: 141.4737\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21593.2402 - mse: 21593.2402 - mae: 145.7075 - val_loss: 20324.1953 - val_mse: 20324.1953 - val_mae: 141.4641\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 21590.4004 - mse: 21590.4004 - mae: 145.6980 - val_loss: 20321.4512 - val_mse: 20321.4512 - val_mae: 141.4545\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 21587.6016 - mse: 21587.6016 - mae: 145.6885 - val_loss: 20318.7129 - val_mse: 20318.7129 - val_mae: 141.4450\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 21584.8066 - mse: 21584.8066 - mae: 145.6791 - val_loss: 20315.9590 - val_mse: 20315.9590 - val_mae: 141.4354\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21582.0039 - mse: 21582.0039 - mae: 145.6696 - val_loss: 20313.2109 - val_mse: 20313.2109 - val_mae: 141.4258\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21579.1855 - mse: 21579.1855 - mae: 145.6601 - val_loss: 20310.4551 - val_mse: 20310.4551 - val_mae: 141.4162\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21576.3965 - mse: 21576.3965 - mae: 145.6506 - val_loss: 20307.6777 - val_mse: 20307.6777 - val_mae: 141.4066\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21573.5566 - mse: 21573.5566 - mae: 145.6410 - val_loss: 20304.9336 - val_mse: 20304.9336 - val_mae: 141.3970\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21570.7461 - mse: 21570.7461 - mae: 145.6315 - val_loss: 20302.1777 - val_mse: 20302.1777 - val_mae: 141.3874\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21567.9219 - mse: 21567.9219 - mae: 145.6219 - val_loss: 20299.4102 - val_mse: 20299.4102 - val_mae: 141.3778\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21565.0918 - mse: 21565.0918 - mae: 145.6124 - val_loss: 20296.6250 - val_mse: 20296.6250 - val_mae: 141.3681\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21562.2734 - mse: 21562.2734 - mae: 145.6029 - val_loss: 20293.7676 - val_mse: 20293.7676 - val_mae: 141.3581\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21559.4492 - mse: 21559.4492 - mae: 145.5933 - val_loss: 20290.9297 - val_mse: 20290.9297 - val_mae: 141.3482\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21556.6230 - mse: 21556.6230 - mae: 145.5838 - val_loss: 20288.0977 - val_mse: 20288.0977 - val_mae: 141.3384\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21553.8262 - mse: 21553.8262 - mae: 145.5743 - val_loss: 20285.2520 - val_mse: 20285.2520 - val_mae: 141.3284\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21551.0000 - mse: 21551.0000 - mae: 145.5647 - val_loss: 20282.4258 - val_mse: 20282.4258 - val_mae: 141.3186\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 21548.2012 - mse: 21548.2012 - mae: 145.5553 - val_loss: 20279.5957 - val_mse: 20279.5957 - val_mae: 141.3087\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21545.3828 - mse: 21545.3828 - mae: 145.5457 - val_loss: 20276.7793 - val_mse: 20276.7793 - val_mae: 141.2989\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21542.5820 - mse: 21542.5820 - mae: 145.5363 - val_loss: 20273.9473 - val_mse: 20273.9473 - val_mae: 141.2890\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21539.7832 - mse: 21539.7832 - mae: 145.5268 - val_loss: 20271.1172 - val_mse: 20271.1172 - val_mae: 141.2792\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21536.9785 - mse: 21536.9785 - mae: 145.5173 - val_loss: 20268.2598 - val_mse: 20268.2598 - val_mae: 141.2692\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 21534.1484 - mse: 21534.1484 - mae: 145.5077 - val_loss: 20265.4395 - val_mse: 20265.4395 - val_mae: 141.2594\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21531.3477 - mse: 21531.3477 - mae: 145.4982 - val_loss: 20262.6152 - val_mse: 20262.6152 - val_mae: 141.2495\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21528.5234 - mse: 21528.5234 - mae: 145.4887 - val_loss: 20259.7969 - val_mse: 20259.7969 - val_mae: 141.2397\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21525.7012 - mse: 21525.7012 - mae: 145.4791 - val_loss: 20256.9668 - val_mse: 20256.9668 - val_mae: 141.2298\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21522.8867 - mse: 21522.8867 - mae: 145.4696 - val_loss: 20254.1152 - val_mse: 20254.1152 - val_mae: 141.2198\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21520.0586 - mse: 21520.0586 - mae: 145.4600 - val_loss: 20251.2402 - val_mse: 20251.2402 - val_mae: 141.2098\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 21517.2070 - mse: 21517.2070 - mae: 145.4504 - val_loss: 20248.3906 - val_mse: 20248.3906 - val_mae: 141.1999\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 21514.3887 - mse: 21514.3887 - mae: 145.4408 - val_loss: 20245.5293 - val_mse: 20245.5293 - val_mae: 141.1899\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21511.5625 - mse: 21511.5625 - mae: 145.4312 - val_loss: 20242.6660 - val_mse: 20242.6660 - val_mae: 141.1799\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21508.7012 - mse: 21508.7012 - mae: 145.4215 - val_loss: 20239.8281 - val_mse: 20239.8281 - val_mae: 141.1700\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21505.8848 - mse: 21505.8848 - mae: 145.4120 - val_loss: 20236.9902 - val_mse: 20236.9902 - val_mae: 141.1601\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.5, model__n_hidden=2, model__n_neurons=5, model__optimizer=adam; total time=   4.9s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 526.5300 - mse: 526.5300 - mae: 17.1512 - val_loss: 448.9519 - val_mse: 448.9519 - val_mae: 16.0224\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 526.0556 - mse: 526.0556 - mae: 17.1407 - val_loss: 448.4960 - val_mse: 448.4960 - val_mae: 16.0128\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 525.5848 - mse: 525.5848 - mae: 17.1305 - val_loss: 448.0422 - val_mse: 448.0422 - val_mae: 16.0032\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 525.1074 - mse: 525.1074 - mae: 17.1200 - val_loss: 447.5896 - val_mse: 447.5896 - val_mae: 15.9936\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 524.6307 - mse: 524.6307 - mae: 17.1095 - val_loss: 447.1355 - val_mse: 447.1355 - val_mae: 15.9839\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 524.1639 - mse: 524.1639 - mae: 17.0992 - val_loss: 446.6804 - val_mse: 446.6804 - val_mae: 15.9743\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 523.6887 - mse: 523.6887 - mae: 17.0888 - val_loss: 446.2302 - val_mse: 446.2302 - val_mae: 15.9647\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 523.2208 - mse: 523.2208 - mae: 17.0785 - val_loss: 445.7713 - val_mse: 445.7713 - val_mae: 15.9550\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 522.7355 - mse: 522.7355 - mae: 17.0680 - val_loss: 445.3204 - val_mse: 445.3204 - val_mae: 15.9454\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 522.2704 - mse: 522.2704 - mae: 17.0575 - val_loss: 444.8668 - val_mse: 444.8668 - val_mae: 15.9357\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 521.7972 - mse: 521.7972 - mae: 17.0472 - val_loss: 444.4094 - val_mse: 444.4094 - val_mae: 15.9260\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 521.3206 - mse: 521.3206 - mae: 17.0369 - val_loss: 443.9652 - val_mse: 443.9652 - val_mae: 15.9165\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 520.8530 - mse: 520.8530 - mae: 17.0266 - val_loss: 443.5164 - val_mse: 443.5164 - val_mae: 15.9070\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 520.4059 - mse: 520.4059 - mae: 17.0165 - val_loss: 443.0543 - val_mse: 443.0543 - val_mae: 15.8971\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 519.9224 - mse: 519.9224 - mae: 17.0060 - val_loss: 442.6074 - val_mse: 442.6074 - val_mae: 15.8876\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 519.4556 - mse: 519.4556 - mae: 16.9958 - val_loss: 442.1663 - val_mse: 442.1663 - val_mae: 15.8781\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 519.0013 - mse: 519.0013 - mae: 16.9856 - val_loss: 441.7195 - val_mse: 441.7195 - val_mae: 15.8686\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 518.5345 - mse: 518.5345 - mae: 16.9756 - val_loss: 441.2753 - val_mse: 441.2753 - val_mae: 15.8591\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 518.0649 - mse: 518.0649 - mae: 16.9654 - val_loss: 440.8347 - val_mse: 440.8347 - val_mae: 15.8496\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 517.6038 - mse: 517.6038 - mae: 16.9553 - val_loss: 440.3903 - val_mse: 440.3903 - val_mae: 15.8401\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 517.1548 - mse: 517.1548 - mae: 16.9449 - val_loss: 439.9334 - val_mse: 439.9334 - val_mae: 15.8303\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 516.6743 - mse: 516.6743 - mae: 16.9349 - val_loss: 439.4908 - val_mse: 439.4908 - val_mae: 15.8208\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 516.2091 - mse: 516.2091 - mae: 16.9246 - val_loss: 439.0504 - val_mse: 439.0504 - val_mae: 15.8113\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 515.7575 - mse: 515.7575 - mae: 16.9147 - val_loss: 438.6030 - val_mse: 438.6030 - val_mae: 15.8017\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 515.2952 - mse: 515.2952 - mae: 16.9043 - val_loss: 438.1620 - val_mse: 438.1620 - val_mae: 15.7922\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 514.8331 - mse: 514.8331 - mae: 16.8943 - val_loss: 437.7185 - val_mse: 437.7185 - val_mae: 15.7827\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 514.3631 - mse: 514.3631 - mae: 16.8841 - val_loss: 437.2796 - val_mse: 437.2796 - val_mae: 15.7732\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 513.9220 - mse: 513.9220 - mae: 16.8741 - val_loss: 436.8291 - val_mse: 436.8291 - val_mae: 15.7635\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 513.4485 - mse: 513.4485 - mae: 16.8638 - val_loss: 436.3926 - val_mse: 436.3926 - val_mae: 15.7541\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 512.9954 - mse: 512.9954 - mae: 16.8539 - val_loss: 435.9539 - val_mse: 435.9539 - val_mae: 15.7446\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 512.5376 - mse: 512.5376 - mae: 16.8438 - val_loss: 435.5190 - val_mse: 435.5190 - val_mae: 15.7353\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 512.0834 - mse: 512.0834 - mae: 16.8337 - val_loss: 435.0890 - val_mse: 435.0890 - val_mae: 15.7260\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 511.6344 - mse: 511.6344 - mae: 16.8238 - val_loss: 434.6519 - val_mse: 434.6519 - val_mae: 15.7165\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 511.1694 - mse: 511.1694 - mae: 16.8136 - val_loss: 434.2196 - val_mse: 434.2196 - val_mae: 15.7071\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 510.7321 - mse: 510.7321 - mae: 16.8038 - val_loss: 433.7694 - val_mse: 433.7694 - val_mae: 15.6974\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 510.2592 - mse: 510.2592 - mae: 16.7937 - val_loss: 433.3376 - val_mse: 433.3376 - val_mae: 15.6880\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 509.8136 - mse: 509.8136 - mae: 16.7837 - val_loss: 432.8974 - val_mse: 432.8974 - val_mae: 15.6785\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 509.3552 - mse: 509.3552 - mae: 16.7737 - val_loss: 432.4618 - val_mse: 432.4618 - val_mae: 15.6690\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 508.9101 - mse: 508.9101 - mae: 16.7640 - val_loss: 432.0232 - val_mse: 432.0232 - val_mae: 15.6595\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 508.4408 - mse: 508.4408 - mae: 16.7538 - val_loss: 431.5920 - val_mse: 431.5920 - val_mae: 15.6502\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 507.9951 - mse: 507.9951 - mae: 16.7441 - val_loss: 431.1537 - val_mse: 431.1537 - val_mae: 15.6406\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 507.5427 - mse: 507.5427 - mae: 16.7341 - val_loss: 430.7182 - val_mse: 430.7182 - val_mae: 15.6312\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 507.0844 - mse: 507.0844 - mae: 16.7240 - val_loss: 430.2884 - val_mse: 430.2884 - val_mae: 15.6218\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 506.6495 - mse: 506.6495 - mae: 16.7142 - val_loss: 429.8441 - val_mse: 429.8441 - val_mae: 15.6121\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 506.1835 - mse: 506.1835 - mae: 16.7041 - val_loss: 429.4142 - val_mse: 429.4142 - val_mae: 15.6027\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 505.7304 - mse: 505.7304 - mae: 16.6943 - val_loss: 428.9867 - val_mse: 428.9867 - val_mae: 15.5934\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 505.2773 - mse: 505.2773 - mae: 16.6845 - val_loss: 428.5625 - val_mse: 428.5625 - val_mae: 15.5841\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 504.8420 - mse: 504.8420 - mae: 16.6746 - val_loss: 428.1272 - val_mse: 428.1272 - val_mae: 15.5747\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 504.3848 - mse: 504.3848 - mae: 16.6650 - val_loss: 427.6969 - val_mse: 427.6969 - val_mae: 15.5652\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 503.9354 - mse: 503.9354 - mae: 16.6549 - val_loss: 427.2543 - val_mse: 427.2543 - val_mae: 15.5556\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 503.4747 - mse: 503.4747 - mae: 16.6448 - val_loss: 426.8159 - val_mse: 426.8159 - val_mae: 15.5460\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 503.0148 - mse: 503.0148 - mae: 16.6348 - val_loss: 426.3766 - val_mse: 426.3766 - val_mae: 15.5364\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 502.5601 - mse: 502.5601 - mae: 16.6244 - val_loss: 425.9374 - val_mse: 425.9374 - val_mae: 15.5267\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 502.1033 - mse: 502.1033 - mae: 16.6145 - val_loss: 425.5013 - val_mse: 425.5013 - val_mae: 15.5172\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 501.6460 - mse: 501.6460 - mae: 16.6044 - val_loss: 425.0697 - val_mse: 425.0697 - val_mae: 15.5077\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 501.2041 - mse: 501.2041 - mae: 16.5946 - val_loss: 424.6319 - val_mse: 424.6319 - val_mae: 15.4981\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 500.7448 - mse: 500.7448 - mae: 16.5846 - val_loss: 424.2074 - val_mse: 424.2074 - val_mae: 15.4887\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 500.3070 - mse: 500.3070 - mae: 16.5746 - val_loss: 423.7753 - val_mse: 423.7753 - val_mae: 15.4792\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 499.8495 - mse: 499.8495 - mae: 16.5647 - val_loss: 423.3469 - val_mse: 423.3469 - val_mae: 15.4698\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 499.4066 - mse: 499.4066 - mae: 16.5549 - val_loss: 422.9193 - val_mse: 422.9193 - val_mae: 15.4604\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 498.9574 - mse: 498.9574 - mae: 16.5450 - val_loss: 422.4951 - val_mse: 422.4951 - val_mae: 15.4510\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 498.5233 - mse: 498.5233 - mae: 16.5352 - val_loss: 422.0676 - val_mse: 422.0676 - val_mae: 15.4416\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 498.0762 - mse: 498.0762 - mae: 16.5253 - val_loss: 421.6441 - val_mse: 421.6441 - val_mae: 15.4322\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 497.6265 - mse: 497.6265 - mae: 16.5153 - val_loss: 421.2228 - val_mse: 421.2228 - val_mae: 15.4229\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 497.1826 - mse: 497.1826 - mae: 16.5058 - val_loss: 420.7984 - val_mse: 420.7984 - val_mae: 15.4135\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 496.7493 - mse: 496.7493 - mae: 16.4959 - val_loss: 420.3647 - val_mse: 420.3647 - val_mae: 15.4040\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 496.2911 - mse: 496.2911 - mae: 16.4859 - val_loss: 419.9425 - val_mse: 419.9425 - val_mae: 15.3946\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 495.8637 - mse: 495.8637 - mae: 16.4759 - val_loss: 419.5090 - val_mse: 419.5090 - val_mae: 15.3850\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 495.4097 - mse: 495.4097 - mae: 16.4661 - val_loss: 419.0858 - val_mse: 419.0858 - val_mae: 15.3756\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 494.9627 - mse: 494.9627 - mae: 16.4563 - val_loss: 418.6641 - val_mse: 418.6641 - val_mae: 15.3663\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 494.5369 - mse: 494.5369 - mae: 16.4462 - val_loss: 418.2343 - val_mse: 418.2343 - val_mae: 15.3567\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 494.0864 - mse: 494.0864 - mae: 16.4365 - val_loss: 417.8138 - val_mse: 417.8138 - val_mae: 15.3474\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 493.6393 - mse: 493.6393 - mae: 16.4267 - val_loss: 417.4033 - val_mse: 417.4033 - val_mae: 15.3382\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 493.2017 - mse: 493.2017 - mae: 16.4169 - val_loss: 416.9861 - val_mse: 416.9861 - val_mae: 15.3289\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 492.7670 - mse: 492.7670 - mae: 16.4074 - val_loss: 416.5607 - val_mse: 416.5607 - val_mae: 15.3195\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 492.3210 - mse: 492.3210 - mae: 16.3976 - val_loss: 416.1386 - val_mse: 416.1386 - val_mae: 15.3101\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 491.8986 - mse: 491.8986 - mae: 16.3878 - val_loss: 415.7119 - val_mse: 415.7119 - val_mae: 15.3006\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 491.4470 - mse: 491.4470 - mae: 16.3780 - val_loss: 415.2918 - val_mse: 415.2918 - val_mae: 15.2912\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 491.0118 - mse: 491.0118 - mae: 16.3683 - val_loss: 414.8657 - val_mse: 414.8657 - val_mae: 15.2817\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 490.5681 - mse: 490.5681 - mae: 16.3584 - val_loss: 414.4434 - val_mse: 414.4434 - val_mae: 15.2722\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 490.1356 - mse: 490.1356 - mae: 16.3488 - val_loss: 414.0192 - val_mse: 414.0192 - val_mae: 15.2631\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 489.6848 - mse: 489.6848 - mae: 16.3389 - val_loss: 413.6055 - val_mse: 413.6055 - val_mae: 15.2545\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 489.2491 - mse: 489.2491 - mae: 16.3296 - val_loss: 413.1862 - val_mse: 413.1862 - val_mae: 15.2457\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 488.8047 - mse: 488.8047 - mae: 16.3197 - val_loss: 412.7739 - val_mse: 412.7739 - val_mae: 15.2371\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 488.3721 - mse: 488.3721 - mae: 16.3103 - val_loss: 412.3549 - val_mse: 412.3549 - val_mae: 15.2283\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 487.9454 - mse: 487.9454 - mae: 16.3007 - val_loss: 411.9329 - val_mse: 411.9329 - val_mae: 15.2195\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 487.5082 - mse: 487.5082 - mae: 16.2909 - val_loss: 411.5114 - val_mse: 411.5114 - val_mae: 15.2107\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 487.0646 - mse: 487.0646 - mae: 16.2810 - val_loss: 411.0969 - val_mse: 411.0969 - val_mae: 15.2020\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 486.6424 - mse: 486.6424 - mae: 16.2716 - val_loss: 410.6724 - val_mse: 410.6724 - val_mae: 15.1931\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 486.2002 - mse: 486.2002 - mae: 16.2617 - val_loss: 410.2567 - val_mse: 410.2567 - val_mae: 15.1844\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 485.7758 - mse: 485.7758 - mae: 16.2523 - val_loss: 409.8339 - val_mse: 409.8339 - val_mae: 15.1755\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 485.3163 - mse: 485.3163 - mae: 16.2424 - val_loss: 409.4305 - val_mse: 409.4305 - val_mae: 15.1670\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 484.8994 - mse: 484.8994 - mae: 16.2331 - val_loss: 409.0197 - val_mse: 409.0197 - val_mae: 15.1584\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 484.4581 - mse: 484.4581 - mae: 16.2233 - val_loss: 408.6144 - val_mse: 408.6144 - val_mae: 15.1498\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 484.0333 - mse: 484.0333 - mae: 16.2137 - val_loss: 408.2005 - val_mse: 408.2005 - val_mae: 15.1411\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 483.6099 - mse: 483.6099 - mae: 16.2042 - val_loss: 407.7836 - val_mse: 407.7836 - val_mae: 15.1323\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 483.1675 - mse: 483.1675 - mae: 16.1944 - val_loss: 407.3771 - val_mse: 407.3771 - val_mae: 15.1237\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 482.7470 - mse: 482.7470 - mae: 16.1851 - val_loss: 406.9586 - val_mse: 406.9586 - val_mae: 15.1149\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 482.3056 - mse: 482.3056 - mae: 16.1754 - val_loss: 406.5443 - val_mse: 406.5443 - val_mae: 15.1061\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 481.8762 - mse: 481.8762 - mae: 16.1659 - val_loss: 406.1335 - val_mse: 406.1335 - val_mae: 15.0975\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.5, model__n_hidden=2, model__n_neurons=5, model__optimizer=adam; total time=   5.3s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 398.4583 - mse: 398.4583 - mae: 16.2515 - val_loss: 345.0430 - val_mse: 345.0430 - val_mae: 15.5069\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 398.3833 - mse: 398.3833 - mae: 16.2500 - val_loss: 344.9883 - val_mse: 344.9883 - val_mae: 15.5057\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 398.3112 - mse: 398.3112 - mae: 16.2486 - val_loss: 344.9336 - val_mse: 344.9336 - val_mae: 15.5045\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 398.2415 - mse: 398.2415 - mae: 16.2473 - val_loss: 344.8793 - val_mse: 344.8793 - val_mae: 15.5033\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 398.1693 - mse: 398.1693 - mae: 16.2459 - val_loss: 344.8226 - val_mse: 344.8226 - val_mae: 15.5020\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 398.0946 - mse: 398.0946 - mae: 16.2444 - val_loss: 344.7693 - val_mse: 344.7693 - val_mae: 15.5008\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 398.0193 - mse: 398.0193 - mae: 16.2430 - val_loss: 344.7130 - val_mse: 344.7130 - val_mae: 15.4995\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 397.9504 - mse: 397.9504 - mae: 16.2416 - val_loss: 344.6539 - val_mse: 344.6539 - val_mae: 15.4982\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 397.8726 - mse: 397.8726 - mae: 16.2402 - val_loss: 344.5991 - val_mse: 344.5991 - val_mae: 15.4970\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 397.8035 - mse: 397.8035 - mae: 16.2388 - val_loss: 344.5434 - val_mse: 344.5434 - val_mae: 15.4958\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 397.7271 - mse: 397.7271 - mae: 16.2374 - val_loss: 344.4886 - val_mse: 344.4886 - val_mae: 15.4945\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.5, model__n_hidden=2, model__n_neurons=5, model__optimizer=adam; total time=   1.1s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 312.7803 - mse: 312.7803 - mae: 12.0969 - val_loss: 81.5997 - val_mse: 81.5997 - val_mae: 6.3139\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 112.7981 - mse: 112.7981 - mae: 7.6721 - val_loss: 61.8249 - val_mse: 61.8249 - val_mae: 5.4670\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 101.8433 - mse: 101.8433 - mae: 7.1873 - val_loss: 52.0758 - val_mse: 52.0758 - val_mae: 5.1770\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 96.8130 - mse: 96.8130 - mae: 6.9197 - val_loss: 50.7277 - val_mse: 50.7277 - val_mae: 5.3406\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 93.8275 - mse: 93.8275 - mae: 6.9001 - val_loss: 42.4655 - val_mse: 42.4655 - val_mae: 4.8368\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 91.7409 - mse: 91.7409 - mae: 6.7871 - val_loss: 38.5257 - val_mse: 38.5257 - val_mae: 4.7882\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 90.4964 - mse: 90.4964 - mae: 6.7377 - val_loss: 36.0789 - val_mse: 36.0789 - val_mae: 4.5854\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 89.5354 - mse: 89.5354 - mae: 6.7136 - val_loss: 37.0087 - val_mse: 37.0087 - val_mae: 4.7934\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 86.8255 - mse: 86.8255 - mae: 6.6810 - val_loss: 38.4898 - val_mse: 38.4898 - val_mae: 4.9542\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 86.8161 - mse: 86.8161 - mae: 6.7753 - val_loss: 30.2583 - val_mse: 30.2583 - val_mae: 4.3058\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 84.7814 - mse: 84.7814 - mae: 6.5730 - val_loss: 29.5292 - val_mse: 29.5292 - val_mae: 4.2800\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 85.4518 - mse: 85.4518 - mae: 6.6141 - val_loss: 28.3360 - val_mse: 28.3360 - val_mae: 4.1948\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 83.7491 - mse: 83.7491 - mae: 6.5767 - val_loss: 32.7892 - val_mse: 32.7892 - val_mae: 4.5721\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 85.7700 - mse: 85.7700 - mae: 6.8466 - val_loss: 27.2093 - val_mse: 27.2093 - val_mae: 4.1730\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 82.8228 - mse: 82.8228 - mae: 6.5544 - val_loss: 27.0169 - val_mse: 27.0169 - val_mae: 4.1618\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 83.3311 - mse: 83.3311 - mae: 6.6466 - val_loss: 26.6436 - val_mse: 26.6436 - val_mae: 4.2943\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 83.3412 - mse: 83.3412 - mae: 6.4605 - val_loss: 28.2157 - val_mse: 28.2157 - val_mae: 4.2572\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 83.0042 - mse: 83.0042 - mae: 6.5677 - val_loss: 26.4034 - val_mse: 26.4034 - val_mae: 4.1122\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 81.7456 - mse: 81.7456 - mae: 6.4217 - val_loss: 28.2655 - val_mse: 28.2655 - val_mae: 4.2719\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 82.2605 - mse: 82.2605 - mae: 6.5416 - val_loss: 29.9690 - val_mse: 29.9690 - val_mae: 4.4052\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 81.5576 - mse: 81.5576 - mae: 6.5681 - val_loss: 28.0913 - val_mse: 28.0913 - val_mae: 4.2565\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 82.7299 - mse: 82.7299 - mae: 6.5567 - val_loss: 25.7202 - val_mse: 25.7202 - val_mae: 4.1587\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 80.8498 - mse: 80.8498 - mae: 6.3804 - val_loss: 29.9586 - val_mse: 29.9586 - val_mae: 4.3957\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 81.6504 - mse: 81.6504 - mae: 6.4992 - val_loss: 29.0364 - val_mse: 29.0364 - val_mae: 4.3208\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 81.4012 - mse: 81.4012 - mae: 6.5749 - val_loss: 25.7243 - val_mse: 25.7243 - val_mae: 4.1327\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 81.2097 - mse: 81.2097 - mae: 6.4288 - val_loss: 29.1335 - val_mse: 29.1335 - val_mae: 4.3176\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 82.2833 - mse: 82.2833 - mae: 6.5709 - val_loss: 26.4853 - val_mse: 26.4853 - val_mae: 4.1195\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 83.3749 - mse: 83.3749 - mae: 6.5434 - val_loss: 25.7974 - val_mse: 25.7974 - val_mae: 4.1269\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 81.1579 - mse: 81.1579 - mae: 6.4182 - val_loss: 28.9901 - val_mse: 28.9901 - val_mae: 4.3004\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 80.8621 - mse: 80.8621 - mae: 6.5416 - val_loss: 25.3960 - val_mse: 25.3960 - val_mae: 4.2308\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 81.2401 - mse: 81.2401 - mae: 6.4378 - val_loss: 25.7776 - val_mse: 25.7776 - val_mae: 4.3275\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 81.4124 - mse: 81.4124 - mae: 6.3341 - val_loss: 30.8290 - val_mse: 30.8290 - val_mae: 4.4317\n",
      "Epoch 32: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.1, model__n_hidden=3, model__n_neurons=5, model__optimizer=sgd; total time=   2.1s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 50230.7031 - mse: 50230.7031 - mae: 116.8472 - val_loss: 447.1874 - val_mse: 447.1874 - val_mae: 20.2362\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 634.5305 - mse: 634.5305 - mae: 23.0745 - val_loss: 436.2789 - val_mse: 436.2789 - val_mae: 20.0147\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 579.4175 - mse: 579.4175 - mae: 22.0124 - val_loss: 223.5738 - val_mse: 223.5738 - val_mae: 14.4082\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 214.7971 - mse: 214.7971 - mae: 10.9461 - val_loss: 28.1449 - val_mse: 28.1449 - val_mae: 4.3669\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 107.3811 - mse: 107.3811 - mae: 7.7376 - val_loss: 34.9279 - val_mse: 34.9279 - val_mae: 4.7899\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 107.9754 - mse: 107.9754 - mae: 7.9339 - val_loss: 29.4332 - val_mse: 29.4332 - val_mae: 4.4713\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 107.9546 - mse: 107.9546 - mae: 7.8324 - val_loss: 32.2163 - val_mse: 32.2163 - val_mae: 4.6306\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 109.6736 - mse: 109.6736 - mae: 7.8638 - val_loss: 36.7880 - val_mse: 36.7880 - val_mae: 4.9102\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 106.0550 - mse: 106.0550 - mae: 7.8341 - val_loss: 37.9416 - val_mse: 37.9416 - val_mae: 4.9782\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 106.4134 - mse: 106.4134 - mae: 7.9337 - val_loss: 37.9275 - val_mse: 37.9275 - val_mae: 4.9802\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 106.5621 - mse: 106.5621 - mae: 7.9430 - val_loss: 32.8037 - val_mse: 32.8037 - val_mae: 4.6870\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 106.0672 - mse: 106.0672 - mae: 7.8079 - val_loss: 37.1599 - val_mse: 37.1599 - val_mae: 4.9390\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 105.6074 - mse: 105.6074 - mae: 7.8680 - val_loss: 37.2797 - val_mse: 37.2797 - val_mae: 4.9441\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 106.8185 - mse: 106.8185 - mae: 7.9470 - val_loss: 33.1818 - val_mse: 33.1818 - val_mae: 4.7016\n",
      "Epoch 14: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.1, model__n_hidden=3, model__n_neurons=5, model__optimizer=sgd; total time=   1.2s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 557.2040 - mse: 557.2040 - mae: 21.8940 - val_loss: 548.4952 - val_mse: 548.4952 - val_mae: 21.8335\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 557.0499 - mse: 557.0499 - mae: 21.8905 - val_loss: 548.3421 - val_mse: 548.3421 - val_mae: 21.8300\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 556.8965 - mse: 556.8965 - mae: 21.8870 - val_loss: 548.1889 - val_mse: 548.1889 - val_mae: 21.8265\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 556.7429 - mse: 556.7429 - mae: 21.8835 - val_loss: 548.0363 - val_mse: 548.0363 - val_mae: 21.8230\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 556.5898 - mse: 556.5898 - mae: 21.8800 - val_loss: 547.8840 - val_mse: 547.8840 - val_mae: 21.8195\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 556.4373 - mse: 556.4373 - mae: 21.8765 - val_loss: 547.7318 - val_mse: 547.7318 - val_mae: 21.8160\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 556.2845 - mse: 556.2845 - mae: 21.8730 - val_loss: 547.5784 - val_mse: 547.5784 - val_mae: 21.8125\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 556.1308 - mse: 556.1308 - mae: 21.8695 - val_loss: 547.4258 - val_mse: 547.4258 - val_mae: 21.8090\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 555.9779 - mse: 555.9779 - mae: 21.8660 - val_loss: 547.2738 - val_mse: 547.2738 - val_mae: 21.8055\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 555.8254 - mse: 555.8254 - mae: 21.8625 - val_loss: 547.1211 - val_mse: 547.1211 - val_mae: 21.8020\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 555.6723 - mse: 555.6723 - mae: 21.8590 - val_loss: 546.9687 - val_mse: 546.9687 - val_mae: 21.7985\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 555.5196 - mse: 555.5196 - mae: 21.8555 - val_loss: 546.8174 - val_mse: 546.8174 - val_mae: 21.7951\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 555.3679 - mse: 555.3679 - mae: 21.8521 - val_loss: 546.6652 - val_mse: 546.6652 - val_mae: 21.7916\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 555.2153 - mse: 555.2153 - mae: 21.8486 - val_loss: 546.5129 - val_mse: 546.5129 - val_mae: 21.7881\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 555.0626 - mse: 555.0626 - mae: 21.8450 - val_loss: 546.3611 - val_mse: 546.3611 - val_mae: 21.7846\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 554.9103 - mse: 554.9103 - mae: 21.8416 - val_loss: 546.2079 - val_mse: 546.2079 - val_mae: 21.7811\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 554.7569 - mse: 554.7569 - mae: 21.8380 - val_loss: 546.0562 - val_mse: 546.0562 - val_mae: 21.7776\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 554.6047 - mse: 554.6047 - mae: 21.8346 - val_loss: 545.9044 - val_mse: 545.9044 - val_mae: 21.7741\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 554.4527 - mse: 554.4527 - mae: 21.8311 - val_loss: 545.7534 - val_mse: 545.7534 - val_mae: 21.7706\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 554.3010 - mse: 554.3010 - mae: 21.8276 - val_loss: 545.6013 - val_mse: 545.6013 - val_mae: 21.7671\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 554.1487 - mse: 554.1487 - mae: 21.8241 - val_loss: 545.4500 - val_mse: 545.4500 - val_mae: 21.7637\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 553.9969 - mse: 553.9969 - mae: 21.8207 - val_loss: 545.2983 - val_mse: 545.2983 - val_mae: 21.7602\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 553.8448 - mse: 553.8448 - mae: 21.8172 - val_loss: 545.1462 - val_mse: 545.1462 - val_mae: 21.7567\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 553.6923 - mse: 553.6923 - mae: 21.8137 - val_loss: 544.9933 - val_mse: 544.9933 - val_mae: 21.7532\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 553.5391 - mse: 553.5391 - mae: 21.8102 - val_loss: 544.8409 - val_mse: 544.8409 - val_mae: 21.7497\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 553.3863 - mse: 553.3863 - mae: 21.8067 - val_loss: 544.6888 - val_mse: 544.6888 - val_mae: 21.7462\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 553.2339 - mse: 553.2339 - mae: 21.8032 - val_loss: 544.5372 - val_mse: 544.5372 - val_mae: 21.7427\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 553.0818 - mse: 553.0818 - mae: 21.7997 - val_loss: 544.3848 - val_mse: 544.3848 - val_mae: 21.7392\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 552.9290 - mse: 552.9290 - mae: 21.7962 - val_loss: 544.2334 - val_mse: 544.2334 - val_mae: 21.7357\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 552.7773 - mse: 552.7773 - mae: 21.7927 - val_loss: 544.0813 - val_mse: 544.0813 - val_mae: 21.7322\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 552.6248 - mse: 552.6248 - mae: 21.7892 - val_loss: 543.9294 - val_mse: 543.9294 - val_mae: 21.7287\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 552.4724 - mse: 552.4724 - mae: 21.7857 - val_loss: 543.7772 - val_mse: 543.7772 - val_mae: 21.7252\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 552.3200 - mse: 552.3200 - mae: 21.7822 - val_loss: 543.6268 - val_mse: 543.6268 - val_mae: 21.7217\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 552.1691 - mse: 552.1691 - mae: 21.7787 - val_loss: 543.4755 - val_mse: 543.4755 - val_mae: 21.7183\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 552.0173 - mse: 552.0173 - mae: 21.7753 - val_loss: 543.3237 - val_mse: 543.3237 - val_mae: 21.7148\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 551.8654 - mse: 551.8654 - mae: 21.7717 - val_loss: 543.1740 - val_mse: 543.1740 - val_mae: 21.7113\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 551.7152 - mse: 551.7152 - mae: 21.7682 - val_loss: 543.0238 - val_mse: 543.0238 - val_mae: 21.7079\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 551.5648 - mse: 551.5648 - mae: 21.7648 - val_loss: 542.8738 - val_mse: 542.8738 - val_mae: 21.7044\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 551.4142 - mse: 551.4142 - mae: 21.7614 - val_loss: 542.7228 - val_mse: 542.7228 - val_mae: 21.7009\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 551.2628 - mse: 551.2628 - mae: 21.7579 - val_loss: 542.5719 - val_mse: 542.5719 - val_mae: 21.6974\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 551.1116 - mse: 551.1116 - mae: 21.7544 - val_loss: 542.4214 - val_mse: 542.4214 - val_mae: 21.6940\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 550.9605 - mse: 550.9605 - mae: 21.7510 - val_loss: 542.2683 - val_mse: 542.2683 - val_mae: 21.6905\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 550.8073 - mse: 550.8073 - mae: 21.7475 - val_loss: 542.1175 - val_mse: 542.1175 - val_mae: 21.6870\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 550.6561 - mse: 550.6561 - mae: 21.7440 - val_loss: 541.9672 - val_mse: 541.9672 - val_mae: 21.6835\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 550.5051 - mse: 550.5051 - mae: 21.7405 - val_loss: 541.8156 - val_mse: 541.8156 - val_mae: 21.6800\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 550.3533 - mse: 550.3533 - mae: 21.7370 - val_loss: 541.6653 - val_mse: 541.6653 - val_mae: 21.6765\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 550.2027 - mse: 550.2027 - mae: 21.7335 - val_loss: 541.5155 - val_mse: 541.5155 - val_mae: 21.6731\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 550.0524 - mse: 550.0524 - mae: 21.7301 - val_loss: 541.3652 - val_mse: 541.3652 - val_mae: 21.6696\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 549.9016 - mse: 549.9016 - mae: 21.7266 - val_loss: 541.2145 - val_mse: 541.2145 - val_mae: 21.6661\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 549.7507 - mse: 549.7507 - mae: 21.7231 - val_loss: 541.0641 - val_mse: 541.0641 - val_mae: 21.6627\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 549.5999 - mse: 549.5999 - mae: 21.7196 - val_loss: 540.9142 - val_mse: 540.9142 - val_mae: 21.6592\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 549.4496 - mse: 549.4496 - mae: 21.7162 - val_loss: 540.7637 - val_mse: 540.7637 - val_mae: 21.6557\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 549.2986 - mse: 549.2986 - mae: 21.7128 - val_loss: 540.6128 - val_mse: 540.6128 - val_mae: 21.6523\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 549.1473 - mse: 549.1473 - mae: 21.7092 - val_loss: 540.4620 - val_mse: 540.4620 - val_mae: 21.6488\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 548.9962 - mse: 548.9962 - mae: 21.7058 - val_loss: 540.3118 - val_mse: 540.3118 - val_mae: 21.6453\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 548.8458 - mse: 548.8458 - mae: 21.7023 - val_loss: 540.1620 - val_mse: 540.1620 - val_mae: 21.6418\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 548.6955 - mse: 548.6955 - mae: 21.6989 - val_loss: 540.0113 - val_mse: 540.0113 - val_mae: 21.6384\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 548.5443 - mse: 548.5443 - mae: 21.6954 - val_loss: 539.8608 - val_mse: 539.8608 - val_mae: 21.6349\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 548.3934 - mse: 548.3934 - mae: 21.6919 - val_loss: 539.7095 - val_mse: 539.7095 - val_mae: 21.6314\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 548.2419 - mse: 548.2419 - mae: 21.6884 - val_loss: 539.5603 - val_mse: 539.5603 - val_mae: 21.6279\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 548.0921 - mse: 548.0921 - mae: 21.6850 - val_loss: 539.4092 - val_mse: 539.4092 - val_mae: 21.6244\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 547.9408 - mse: 547.9408 - mae: 21.6814 - val_loss: 539.2593 - val_mse: 539.2593 - val_mae: 21.6210\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 547.7905 - mse: 547.7905 - mae: 21.6780 - val_loss: 539.1101 - val_mse: 539.1101 - val_mae: 21.6175\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 547.6408 - mse: 547.6408 - mae: 21.6746 - val_loss: 538.9603 - val_mse: 538.9603 - val_mae: 21.6141\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 547.4907 - mse: 547.4907 - mae: 21.6710 - val_loss: 538.8112 - val_mse: 538.8112 - val_mae: 21.6106\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 547.3411 - mse: 547.3411 - mae: 21.6676 - val_loss: 538.6603 - val_mse: 538.6603 - val_mae: 21.6071\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 547.1899 - mse: 547.1899 - mae: 21.6641 - val_loss: 538.5103 - val_mse: 538.5103 - val_mae: 21.6036\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 547.0396 - mse: 547.0396 - mae: 21.6606 - val_loss: 538.3616 - val_mse: 538.3616 - val_mae: 21.6002\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 546.8903 - mse: 546.8903 - mae: 21.6572 - val_loss: 538.2104 - val_mse: 538.2104 - val_mae: 21.5967\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 546.7390 - mse: 546.7390 - mae: 21.6537 - val_loss: 538.0616 - val_mse: 538.0616 - val_mae: 21.5933\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 546.5896 - mse: 546.5896 - mae: 21.6503 - val_loss: 537.9119 - val_mse: 537.9119 - val_mae: 21.5898\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 546.4395 - mse: 546.4395 - mae: 21.6468 - val_loss: 537.7626 - val_mse: 537.7626 - val_mae: 21.5863\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 546.2900 - mse: 546.2900 - mae: 21.6434 - val_loss: 537.6136 - val_mse: 537.6136 - val_mae: 21.5829\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 546.1406 - mse: 546.1406 - mae: 21.6399 - val_loss: 537.4643 - val_mse: 537.4643 - val_mae: 21.5794\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 545.9908 - mse: 545.9908 - mae: 21.6365 - val_loss: 537.3150 - val_mse: 537.3150 - val_mae: 21.5760\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 545.8411 - mse: 545.8411 - mae: 21.6330 - val_loss: 537.1655 - val_mse: 537.1655 - val_mae: 21.5725\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 545.6912 - mse: 545.6912 - mae: 21.6295 - val_loss: 537.0161 - val_mse: 537.0161 - val_mae: 21.5690\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 545.5415 - mse: 545.5415 - mae: 21.6260 - val_loss: 536.8661 - val_mse: 536.8661 - val_mae: 21.5656\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 545.3912 - mse: 545.3912 - mae: 21.6225 - val_loss: 536.7185 - val_mse: 536.7185 - val_mae: 21.5621\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 545.2432 - mse: 545.2432 - mae: 21.6191 - val_loss: 536.5702 - val_mse: 536.5702 - val_mae: 21.5587\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 545.0942 - mse: 545.0942 - mae: 21.6157 - val_loss: 536.4194 - val_mse: 536.4194 - val_mae: 21.5552\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 544.9431 - mse: 544.9431 - mae: 21.6122 - val_loss: 536.2696 - val_mse: 536.2696 - val_mae: 21.5517\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 544.7929 - mse: 544.7929 - mae: 21.6087 - val_loss: 536.1191 - val_mse: 536.1191 - val_mae: 21.5482\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 544.6421 - mse: 544.6421 - mae: 21.6053 - val_loss: 535.9700 - val_mse: 535.9700 - val_mae: 21.5448\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 544.4927 - mse: 544.4927 - mae: 21.6017 - val_loss: 535.8216 - val_mse: 535.8216 - val_mae: 21.5413\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 544.3440 - mse: 544.3440 - mae: 21.5983 - val_loss: 535.6731 - val_mse: 535.6731 - val_mae: 21.5379\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 544.1951 - mse: 544.1951 - mae: 21.5949 - val_loss: 535.5250 - val_mse: 535.5250 - val_mae: 21.5344\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 544.0465 - mse: 544.0465 - mae: 21.5914 - val_loss: 535.3765 - val_mse: 535.3765 - val_mae: 21.5310\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 543.8976 - mse: 543.8976 - mae: 21.5880 - val_loss: 535.2280 - val_mse: 535.2280 - val_mae: 21.5275\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 543.7488 - mse: 543.7488 - mae: 21.5846 - val_loss: 535.0800 - val_mse: 535.0800 - val_mae: 21.5241\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 543.6002 - mse: 543.6002 - mae: 21.5811 - val_loss: 534.9305 - val_mse: 534.9305 - val_mae: 21.5206\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 543.4504 - mse: 543.4504 - mae: 21.5777 - val_loss: 534.7802 - val_mse: 534.7802 - val_mae: 21.5171\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 543.2997 - mse: 543.2997 - mae: 21.5741 - val_loss: 534.6316 - val_mse: 534.6316 - val_mae: 21.5137\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 543.1509 - mse: 543.1509 - mae: 21.5706 - val_loss: 534.4839 - val_mse: 534.4839 - val_mae: 21.5103\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 543.0027 - mse: 543.0027 - mae: 21.5673 - val_loss: 534.3346 - val_mse: 534.3346 - val_mae: 21.5068\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 542.8531 - mse: 542.8531 - mae: 21.5638 - val_loss: 534.1874 - val_mse: 534.1874 - val_mae: 21.5034\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 542.7054 - mse: 542.7054 - mae: 21.5604 - val_loss: 534.0388 - val_mse: 534.0388 - val_mae: 21.4999\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 542.5565 - mse: 542.5565 - mae: 21.5569 - val_loss: 533.8902 - val_mse: 533.8902 - val_mae: 21.4965\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 542.4075 - mse: 542.4075 - mae: 21.5534 - val_loss: 533.7417 - val_mse: 533.7417 - val_mae: 21.4930\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 542.2586 - mse: 542.2586 - mae: 21.5499 - val_loss: 533.5942 - val_mse: 533.5942 - val_mae: 21.4896\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.1, model__n_hidden=3, model__n_neurons=5, model__optimizer=sgd; total time=   6.1s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 41ms/step - loss: 281.1579 - mse: 281.1579 - mae: 12.6805 - val_loss: 97.4638 - val_mse: 97.4638 - val_mae: 7.3466\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 161.7506 - mse: 161.7506 - mae: 9.4484 - val_loss: 55.7853 - val_mse: 55.7853 - val_mae: 5.5375\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 126.1107 - mse: 126.1107 - mae: 8.1425 - val_loss: 41.0067 - val_mse: 41.0067 - val_mae: 4.9614\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 106.5891 - mse: 106.5891 - mae: 7.4852 - val_loss: 35.9145 - val_mse: 35.9145 - val_mae: 4.9473\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 96.8030 - mse: 96.8030 - mae: 7.2229 - val_loss: 29.5096 - val_mse: 29.5096 - val_mae: 4.6348\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 90.7762 - mse: 90.7762 - mae: 6.9595 - val_loss: 25.2712 - val_mse: 25.2712 - val_mae: 4.2754\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 86.4617 - mse: 86.4617 - mae: 6.6531 - val_loss: 28.7093 - val_mse: 28.7093 - val_mae: 4.7553\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 83.9859 - mse: 83.9859 - mae: 6.6312 - val_loss: 30.4377 - val_mse: 30.4377 - val_mae: 4.9149\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 82.0413 - mse: 82.0413 - mae: 6.5262 - val_loss: 33.0047 - val_mse: 33.0047 - val_mae: 5.1013\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 80.5622 - mse: 80.5622 - mae: 6.5717 - val_loss: 30.8630 - val_mse: 30.8630 - val_mae: 4.8829\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 79.3780 - mse: 79.3780 - mae: 6.4318 - val_loss: 29.1540 - val_mse: 29.1540 - val_mae: 4.7199\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 79.1303 - mse: 79.1303 - mae: 6.4122 - val_loss: 28.4487 - val_mse: 28.4487 - val_mae: 4.6453\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 77.8289 - mse: 77.8289 - mae: 6.2606 - val_loss: 35.1815 - val_mse: 35.1815 - val_mae: 5.1941\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 77.9445 - mse: 77.9445 - mae: 6.5438 - val_loss: 27.1404 - val_mse: 27.1404 - val_mae: 4.5158\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 76.4296 - mse: 76.4296 - mae: 6.2791 - val_loss: 26.9769 - val_mse: 26.9769 - val_mae: 4.4895\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 76.5564 - mse: 76.5564 - mae: 6.3212 - val_loss: 25.3511 - val_mse: 25.3511 - val_mae: 4.2844\n",
      "Epoch 16: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.1, model__n_hidden=2, model__n_neurons=25, model__optimizer=momentum; total time=   2.2s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 34ms/step - loss: 997.6270 - mse: 997.6270 - mae: 18.8210 - val_loss: 74.8550 - val_mse: 74.8550 - val_mae: 7.0381\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 119.4914 - mse: 119.4914 - mae: 8.1193 - val_loss: 45.9943 - val_mse: 45.9943 - val_mae: 5.5710\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 103.9954 - mse: 103.9954 - mae: 7.4694 - val_loss: 33.9488 - val_mse: 33.9488 - val_mae: 4.5443\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 94.4613 - mse: 94.4613 - mae: 7.1882 - val_loss: 27.5591 - val_mse: 27.5591 - val_mae: 4.1101\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 90.6206 - mse: 90.6206 - mae: 7.0319 - val_loss: 21.4349 - val_mse: 21.4349 - val_mae: 3.7509\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 90.8915 - mse: 90.8915 - mae: 6.9370 - val_loss: 19.5172 - val_mse: 19.5172 - val_mae: 3.6723\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 88.3493 - mse: 88.3493 - mae: 6.8052 - val_loss: 18.1593 - val_mse: 18.1593 - val_mae: 3.5560\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 90.5837 - mse: 90.5837 - mae: 6.7733 - val_loss: 18.3965 - val_mse: 18.3965 - val_mae: 3.4681\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 83.5851 - mse: 83.5851 - mae: 6.5602 - val_loss: 20.6945 - val_mse: 20.6945 - val_mae: 3.6377\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 82.8971 - mse: 82.8971 - mae: 6.6474 - val_loss: 17.9249 - val_mse: 17.9249 - val_mae: 3.4241\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 83.4527 - mse: 83.4527 - mae: 6.6399 - val_loss: 14.6696 - val_mse: 14.6696 - val_mae: 3.1438\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 82.9862 - mse: 82.9862 - mae: 6.4312 - val_loss: 18.0862 - val_mse: 18.0862 - val_mae: 3.4249\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 82.4274 - mse: 82.4274 - mae: 6.5304 - val_loss: 16.4083 - val_mse: 16.4083 - val_mae: 3.2707\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 82.2274 - mse: 82.2274 - mae: 6.6065 - val_loss: 13.7462 - val_mse: 13.7462 - val_mae: 2.9643\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 80.1378 - mse: 80.1378 - mae: 6.4310 - val_loss: 14.7519 - val_mse: 14.7519 - val_mae: 3.0835\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 80.5124 - mse: 80.5124 - mae: 6.4424 - val_loss: 13.4661 - val_mse: 13.4661 - val_mae: 2.9385\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 81.6424 - mse: 81.6424 - mae: 6.3778 - val_loss: 16.3798 - val_mse: 16.3798 - val_mae: 3.2260\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 82.1232 - mse: 82.1232 - mae: 6.5482 - val_loss: 13.0745 - val_mse: 13.0745 - val_mae: 2.8605\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 78.6218 - mse: 78.6218 - mae: 6.2475 - val_loss: 14.0244 - val_mse: 14.0244 - val_mae: 2.9492\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 79.9782 - mse: 79.9782 - mae: 6.3540 - val_loss: 17.7274 - val_mse: 17.7274 - val_mae: 3.3377\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 78.0482 - mse: 78.0482 - mae: 6.4091 - val_loss: 12.8253 - val_mse: 12.8253 - val_mae: 2.8313\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 79.5059 - mse: 79.5059 - mae: 6.2836 - val_loss: 12.9795 - val_mse: 12.9795 - val_mae: 2.8329\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 78.1501 - mse: 78.1501 - mae: 6.2900 - val_loss: 13.0301 - val_mse: 13.0301 - val_mae: 2.8279\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 80.7865 - mse: 80.7865 - mae: 6.3111 - val_loss: 15.7047 - val_mse: 15.7047 - val_mae: 3.1041\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 78.3197 - mse: 78.3197 - mae: 6.3957 - val_loss: 12.5677 - val_mse: 12.5677 - val_mae: 2.7982\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 79.8594 - mse: 79.8594 - mae: 6.3379 - val_loss: 15.1928 - val_mse: 15.1928 - val_mae: 3.0360\n",
      "Epoch 26: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.1, model__n_hidden=2, model__n_neurons=25, model__optimizer=momentum; total time=   2.9s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 33ms/step - loss: 300.1120 - mse: 300.1120 - mae: 14.0577 - val_loss: 317.4361 - val_mse: 317.4361 - val_mae: 15.1102\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 213.1076 - mse: 213.1076 - mae: 11.6280 - val_loss: 202.3300 - val_mse: 202.3300 - val_mae: 11.4836\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 170.4408 - mse: 170.4408 - mae: 10.2457 - val_loss: 161.2735 - val_mse: 161.2735 - val_mae: 9.8791\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 134.0354 - mse: 134.0354 - mae: 8.9463 - val_loss: 133.9784 - val_mse: 133.9784 - val_mae: 8.6542\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 120.1748 - mse: 120.1748 - mae: 8.4334 - val_loss: 119.7653 - val_mse: 119.7653 - val_mae: 8.0541\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 112.0113 - mse: 112.0113 - mae: 7.9638 - val_loss: 111.1124 - val_mse: 111.1124 - val_mae: 7.9685\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 112.2015 - mse: 112.2015 - mae: 8.0192 - val_loss: 113.4341 - val_mse: 113.4341 - val_mae: 8.6418\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 102.2300 - mse: 102.2300 - mae: 7.6361 - val_loss: 99.8044 - val_mse: 99.8044 - val_mae: 7.0797\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 98.7724 - mse: 98.7724 - mae: 7.4093 - val_loss: 93.2518 - val_mse: 93.2518 - val_mae: 7.5044\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 94.1063 - mse: 94.1063 - mae: 7.2249 - val_loss: 90.1173 - val_mse: 90.1173 - val_mae: 6.7887\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 92.5799 - mse: 92.5799 - mae: 7.1626 - val_loss: 82.9422 - val_mse: 82.9422 - val_mae: 6.8809\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 93.3951 - mse: 93.3951 - mae: 7.1000 - val_loss: 85.4146 - val_mse: 85.4146 - val_mae: 6.6663\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 90.1813 - mse: 90.1813 - mae: 6.8722 - val_loss: 78.1246 - val_mse: 78.1246 - val_mae: 6.6942\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 88.2272 - mse: 88.2272 - mae: 6.8443 - val_loss: 77.0995 - val_mse: 77.0995 - val_mae: 6.8946\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 85.9377 - mse: 85.9377 - mae: 6.8536 - val_loss: 76.3187 - val_mse: 76.3187 - val_mae: 6.3767\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 86.9960 - mse: 86.9960 - mae: 6.7842 - val_loss: 82.0598 - val_mse: 82.0598 - val_mae: 7.3811\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 82.8287 - mse: 82.8287 - mae: 6.7820 - val_loss: 80.1598 - val_mse: 80.1598 - val_mae: 6.6039\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 86.5886 - mse: 86.5886 - mae: 6.7605 - val_loss: 69.8945 - val_mse: 69.8945 - val_mae: 6.4107\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 84.6044 - mse: 84.6044 - mae: 6.8308 - val_loss: 68.5903 - val_mse: 68.5903 - val_mae: 6.2639\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 83.2308 - mse: 83.2308 - mae: 6.6961 - val_loss: 67.1798 - val_mse: 67.1798 - val_mae: 6.2362\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 83.4748 - mse: 83.4748 - mae: 6.6954 - val_loss: 68.2984 - val_mse: 68.2984 - val_mae: 6.0815\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 81.3831 - mse: 81.3831 - mae: 6.5722 - val_loss: 65.0961 - val_mse: 65.0961 - val_mae: 6.2462\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 77.8304 - mse: 77.8304 - mae: 6.5810 - val_loss: 63.8142 - val_mse: 63.8142 - val_mae: 6.1137\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 80.0163 - mse: 80.0163 - mae: 6.4499 - val_loss: 67.1253 - val_mse: 67.1253 - val_mae: 6.6708\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 78.3378 - mse: 78.3378 - mae: 6.5133 - val_loss: 61.5220 - val_mse: 61.5220 - val_mae: 6.0530\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 76.4233 - mse: 76.4233 - mae: 6.3494 - val_loss: 75.3052 - val_mse: 75.3052 - val_mae: 7.2195\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 79.2597 - mse: 79.2597 - mae: 6.7078 - val_loss: 59.7651 - val_mse: 59.7651 - val_mae: 5.8273\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 78.3624 - mse: 78.3624 - mae: 6.5451 - val_loss: 58.7819 - val_mse: 58.7819 - val_mae: 5.8515\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 75.2515 - mse: 75.2515 - mae: 6.3119 - val_loss: 58.9504 - val_mse: 58.9504 - val_mae: 5.7195\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 76.5474 - mse: 76.5474 - mae: 6.3264 - val_loss: 64.3705 - val_mse: 64.3705 - val_mae: 6.6079\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 74.6249 - mse: 74.6249 - mae: 6.2989 - val_loss: 55.9921 - val_mse: 55.9921 - val_mae: 5.7710\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 75.1247 - mse: 75.1247 - mae: 6.2881 - val_loss: 61.3454 - val_mse: 61.3454 - val_mae: 6.4311\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 74.4494 - mse: 74.4494 - mae: 6.4071 - val_loss: 58.4555 - val_mse: 58.4555 - val_mae: 5.7207\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 75.3941 - mse: 75.3941 - mae: 6.3126 - val_loss: 53.7721 - val_mse: 53.7721 - val_mae: 5.6711\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 71.0079 - mse: 71.0079 - mae: 6.1123 - val_loss: 54.1405 - val_mse: 54.1405 - val_mae: 5.8486\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 73.5897 - mse: 73.5897 - mae: 6.1896 - val_loss: 55.3352 - val_mse: 55.3352 - val_mae: 5.6232\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 73.9410 - mse: 73.9410 - mae: 6.1782 - val_loss: 56.0389 - val_mse: 56.0389 - val_mae: 5.6713\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 74.2039 - mse: 74.2039 - mae: 6.2119 - val_loss: 53.2012 - val_mse: 53.2012 - val_mae: 5.5525\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 70.6371 - mse: 70.6371 - mae: 6.1499 - val_loss: 50.2976 - val_mse: 50.2976 - val_mae: 5.5780\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 68.7129 - mse: 68.7129 - mae: 6.0121 - val_loss: 49.8013 - val_mse: 49.8013 - val_mae: 5.5273\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 70.1021 - mse: 70.1021 - mae: 6.1722 - val_loss: 50.3708 - val_mse: 50.3708 - val_mae: 5.4194\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 67.2175 - mse: 67.2175 - mae: 5.7586 - val_loss: 103.0828 - val_mse: 103.0828 - val_mae: 8.5577\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 78.1764 - mse: 78.1764 - mae: 6.5329 - val_loss: 47.5066 - val_mse: 47.5066 - val_mae: 5.3656\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 69.2597 - mse: 69.2597 - mae: 6.0009 - val_loss: 49.5705 - val_mse: 49.5705 - val_mae: 5.3902\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 69.2107 - mse: 69.2107 - mae: 5.8757 - val_loss: 50.5428 - val_mse: 50.5428 - val_mae: 5.7251\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 65.8306 - mse: 65.8306 - mae: 5.7797 - val_loss: 52.7553 - val_mse: 52.7553 - val_mae: 5.8944\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 66.1218 - mse: 66.1218 - mae: 5.8007 - val_loss: 53.0431 - val_mse: 53.0431 - val_mae: 5.5724\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 67.3426 - mse: 67.3426 - mae: 5.9216 - val_loss: 46.4784 - val_mse: 46.4784 - val_mae: 5.2407\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 65.5635 - mse: 65.5635 - mae: 5.8646 - val_loss: 47.0843 - val_mse: 47.0843 - val_mae: 5.4272\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 65.5761 - mse: 65.5761 - mae: 5.8072 - val_loss: 53.6253 - val_mse: 53.6253 - val_mae: 5.9517\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 63.9942 - mse: 63.9942 - mae: 5.8287 - val_loss: 45.9854 - val_mse: 45.9854 - val_mae: 5.2083\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 69.7120 - mse: 69.7120 - mae: 5.8945 - val_loss: 45.6662 - val_mse: 45.6662 - val_mae: 5.3004\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 64.6493 - mse: 64.6493 - mae: 5.7215 - val_loss: 48.5017 - val_mse: 48.5017 - val_mae: 5.5787\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 63.0229 - mse: 63.0229 - mae: 5.6569 - val_loss: 47.1200 - val_mse: 47.1200 - val_mae: 5.4787\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 66.4858 - mse: 66.4858 - mae: 5.8315 - val_loss: 44.3806 - val_mse: 44.3806 - val_mae: 5.2170\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 64.6877 - mse: 64.6877 - mae: 5.6801 - val_loss: 47.1939 - val_mse: 47.1939 - val_mae: 5.3420\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 64.4604 - mse: 64.4604 - mae: 5.6683 - val_loss: 43.2058 - val_mse: 43.2058 - val_mae: 5.1501\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 64.7720 - mse: 64.7720 - mae: 5.6535 - val_loss: 45.8395 - val_mse: 45.8395 - val_mae: 5.3772\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 62.7443 - mse: 62.7443 - mae: 5.6478 - val_loss: 44.9202 - val_mse: 44.9202 - val_mae: 5.3069\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 64.4860 - mse: 64.4860 - mae: 5.7458 - val_loss: 45.1448 - val_mse: 45.1448 - val_mae: 5.2385\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 62.7890 - mse: 62.7890 - mae: 5.5258 - val_loss: 44.5763 - val_mse: 44.5763 - val_mae: 5.2770\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 61.1065 - mse: 61.1065 - mae: 5.6316 - val_loss: 42.1198 - val_mse: 42.1198 - val_mae: 5.0561\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 62.0297 - mse: 62.0297 - mae: 5.6150 - val_loss: 42.8734 - val_mse: 42.8734 - val_mae: 5.0944\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 62.1754 - mse: 62.1754 - mae: 5.5521 - val_loss: 42.7103 - val_mse: 42.7103 - val_mae: 5.1336\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 61.2695 - mse: 61.2695 - mae: 5.5232 - val_loss: 41.4584 - val_mse: 41.4584 - val_mae: 5.0424\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 63.7020 - mse: 63.7020 - mae: 5.5755 - val_loss: 43.8116 - val_mse: 43.8116 - val_mae: 5.2088\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 61.8085 - mse: 61.8085 - mae: 5.6017 - val_loss: 46.1839 - val_mse: 46.1839 - val_mae: 5.3752\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 63.9273 - mse: 63.9273 - mae: 5.7375 - val_loss: 44.5419 - val_mse: 44.5419 - val_mae: 5.1980\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 59.7173 - mse: 59.7173 - mae: 5.5017 - val_loss: 42.6894 - val_mse: 42.6894 - val_mae: 5.1193\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 60.1643 - mse: 60.1643 - mae: 5.5507 - val_loss: 41.6437 - val_mse: 41.6437 - val_mae: 5.0284\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 59.6294 - mse: 59.6294 - mae: 5.4390 - val_loss: 44.8254 - val_mse: 44.8254 - val_mae: 5.2575\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 60.2382 - mse: 60.2382 - mae: 5.4555 - val_loss: 40.4435 - val_mse: 40.4435 - val_mae: 4.9463\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 58.9811 - mse: 58.9811 - mae: 5.2872 - val_loss: 45.3823 - val_mse: 45.3823 - val_mae: 5.2934\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 60.0919 - mse: 60.0919 - mae: 5.5150 - val_loss: 41.6019 - val_mse: 41.6019 - val_mae: 4.9970\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 59.3379 - mse: 59.3379 - mae: 5.3975 - val_loss: 40.0279 - val_mse: 40.0279 - val_mae: 4.9300\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 60.8120 - mse: 60.8120 - mae: 5.4444 - val_loss: 46.4133 - val_mse: 46.4133 - val_mae: 5.3617\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 60.2014 - mse: 60.2014 - mae: 5.4804 - val_loss: 39.7330 - val_mse: 39.7330 - val_mae: 4.9463\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 59.6129 - mse: 59.6129 - mae: 5.3899 - val_loss: 44.2594 - val_mse: 44.2594 - val_mae: 5.1893\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 56.7645 - mse: 56.7645 - mae: 5.4180 - val_loss: 43.2087 - val_mse: 43.2087 - val_mae: 5.1271\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 59.8507 - mse: 59.8507 - mae: 5.3853 - val_loss: 39.9427 - val_mse: 39.9427 - val_mae: 4.9272\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 61.4838 - mse: 61.4838 - mae: 5.4497 - val_loss: 45.3469 - val_mse: 45.3469 - val_mae: 5.2684\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 58.4088 - mse: 58.4088 - mae: 5.4980 - val_loss: 39.8893 - val_mse: 39.8893 - val_mae: 4.9125\n",
      "Epoch 82: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.1, model__n_hidden=2, model__n_neurons=25, model__optimizer=momentum; total time=   7.0s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 82ms/step - loss: 2956543199550595594346037248.0000 - mse: 2956543199550595594346037248.0000 - mae: 14845567041536.0000 - val_loss: 162120501816528160879355138932736.0000 - val_mse: 162120501816528160879355138932736.0000 - val_mae: 12462886991429632.0000\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: inf - mse: inf - mae: 3217299880522639870227644416.0000 - val_loss: inf - val_mse: inf - val_mae: 2655665179603295223857383735296.0000\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.9, model__n_hidden=0, model__n_neurons=125, model__optimizer=sgd; total time=   1.4s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 19ms/step - loss: 2836193099438852327369342976.0000 - mse: 2836193099438852327369342976.0000 - mae: 14592110493696.0000 - val_loss: 137828423768444257579076419584000.0000 - val_mse: 137828423768444257579076419584000.0000 - val_mae: 11525023974031360.0000\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: inf - mse: inf - mae: 2482090875939449671312736256.0000 - val_loss: inf - val_mse: inf - val_mae: 2047075903742181003473729880064.0000\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.9, model__n_hidden=0, model__n_neurons=125, model__optimizer=sgd; total time=   1.0s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 18ms/step - loss: 2418693548628782453348106240.0000 - mse: 2418693548628782453348106240.0000 - mae: 13974538027008.0000 - val_loss: 105631431242470773948262354255872.0000 - val_mse: 105631431242470773948262354255872.0000 - val_mae: 10049389175242752.0000\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: inf - mse: inf - mae: 1991954687794978118466273280.0000 - val_loss: inf - val_mse: inf - val_mae: 1570928833775945512209895915520.0000\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.9, model__n_hidden=0, model__n_neurons=125, model__optimizer=sgd; total time=   0.9s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 20ms/step - loss: 618.2767 - mse: 618.2767 - mae: 21.5469 - val_loss: 526.2899 - val_mse: 526.2899 - val_mae: 20.4211\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 528.0999 - mse: 528.0999 - mae: 19.5178 - val_loss: 438.6856 - val_mse: 438.6856 - val_mae: 18.3643\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 446.9710 - mse: 446.9710 - mae: 17.5571 - val_loss: 363.1704 - val_mse: 363.1704 - val_mae: 16.3874\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 377.0305 - mse: 377.0305 - mae: 15.7096 - val_loss: 298.3650 - val_mse: 298.3650 - val_mae: 14.4931\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 318.4933 - mse: 318.4933 - mae: 13.9838 - val_loss: 243.8753 - val_mse: 243.8753 - val_mae: 12.6946\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 268.6262 - mse: 268.6262 - mae: 12.3905 - val_loss: 199.2964 - val_mse: 199.2964 - val_mae: 11.0768\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 227.8566 - mse: 227.8566 - mae: 11.0522 - val_loss: 163.4568 - val_mse: 163.4568 - val_mae: 9.8237\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 195.3650 - mse: 195.3650 - mae: 9.9674 - val_loss: 134.4095 - val_mse: 134.4095 - val_mae: 8.7538\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 169.0654 - mse: 169.0654 - mae: 9.1429 - val_loss: 111.5739 - val_mse: 111.5739 - val_mae: 7.8096\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 148.7446 - mse: 148.7446 - mae: 8.5543 - val_loss: 94.0270 - val_mse: 94.0270 - val_mae: 7.1366\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 133.6093 - mse: 133.6093 - mae: 8.1574 - val_loss: 80.3389 - val_mse: 80.3389 - val_mae: 6.6876\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 121.7618 - mse: 121.7618 - mae: 7.8646 - val_loss: 70.5061 - val_mse: 70.5061 - val_mae: 6.3390\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 112.7878 - mse: 112.7878 - mae: 7.6868 - val_loss: 63.2918 - val_mse: 63.2918 - val_mae: 6.0329\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 107.2571 - mse: 107.2571 - mae: 7.5717 - val_loss: 57.4190 - val_mse: 57.4190 - val_mae: 5.8341\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 101.7550 - mse: 101.7550 - mae: 7.4606 - val_loss: 53.4973 - val_mse: 53.4973 - val_mae: 5.7145\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 98.2195 - mse: 98.2195 - mae: 7.4164 - val_loss: 50.5303 - val_mse: 50.5303 - val_mae: 5.6232\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 95.7539 - mse: 95.7539 - mae: 7.3808 - val_loss: 48.0010 - val_mse: 48.0010 - val_mae: 5.5515\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 93.3175 - mse: 93.3175 - mae: 7.3318 - val_loss: 46.0206 - val_mse: 46.0206 - val_mae: 5.4965\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 91.3214 - mse: 91.3214 - mae: 7.2874 - val_loss: 44.3792 - val_mse: 44.3792 - val_mae: 5.4350\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 89.9204 - mse: 89.9204 - mae: 7.2541 - val_loss: 42.8359 - val_mse: 42.8359 - val_mae: 5.3712\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 88.7189 - mse: 88.7189 - mae: 7.2281 - val_loss: 41.4954 - val_mse: 41.4954 - val_mae: 5.3082\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 87.3134 - mse: 87.3134 - mae: 7.1944 - val_loss: 40.4186 - val_mse: 40.4186 - val_mae: 5.2481\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 86.3382 - mse: 86.3382 - mae: 7.1584 - val_loss: 39.3016 - val_mse: 39.3016 - val_mae: 5.1784\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 85.3593 - mse: 85.3593 - mae: 7.1242 - val_loss: 38.2614 - val_mse: 38.2614 - val_mae: 5.1115\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 84.4599 - mse: 84.4599 - mae: 7.0948 - val_loss: 37.3679 - val_mse: 37.3679 - val_mae: 5.0499\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 83.5685 - mse: 83.5685 - mae: 7.0566 - val_loss: 36.4127 - val_mse: 36.4127 - val_mae: 4.9833\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 82.7700 - mse: 82.7700 - mae: 7.0161 - val_loss: 35.5422 - val_mse: 35.5422 - val_mae: 4.9343\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 82.0816 - mse: 82.0816 - mae: 6.9818 - val_loss: 34.6684 - val_mse: 34.6684 - val_mae: 4.8854\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 81.2702 - mse: 81.2702 - mae: 6.9338 - val_loss: 33.8894 - val_mse: 33.8894 - val_mae: 4.8305\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 80.6964 - mse: 80.6964 - mae: 6.9036 - val_loss: 33.1397 - val_mse: 33.1397 - val_mae: 4.7923\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 79.9760 - mse: 79.9760 - mae: 6.8671 - val_loss: 32.4215 - val_mse: 32.4215 - val_mae: 4.7431\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 79.4118 - mse: 79.4118 - mae: 6.8241 - val_loss: 31.6333 - val_mse: 31.6333 - val_mae: 4.6749\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 78.7805 - mse: 78.7805 - mae: 6.7756 - val_loss: 31.0658 - val_mse: 31.0658 - val_mae: 4.6354\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 78.2863 - mse: 78.2863 - mae: 6.7486 - val_loss: 30.4823 - val_mse: 30.4823 - val_mae: 4.5927\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 77.8311 - mse: 77.8311 - mae: 6.7162 - val_loss: 29.8909 - val_mse: 29.8909 - val_mae: 4.5443\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 77.2755 - mse: 77.2755 - mae: 6.6822 - val_loss: 29.4439 - val_mse: 29.4439 - val_mae: 4.5080\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 76.8648 - mse: 76.8648 - mae: 6.6519 - val_loss: 28.9659 - val_mse: 28.9659 - val_mae: 4.4689\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 76.5282 - mse: 76.5282 - mae: 6.6266 - val_loss: 28.4515 - val_mse: 28.4515 - val_mae: 4.4250\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 76.0218 - mse: 76.0218 - mae: 6.5881 - val_loss: 27.9347 - val_mse: 27.9347 - val_mae: 4.3702\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 75.6608 - mse: 75.6608 - mae: 6.5514 - val_loss: 27.4676 - val_mse: 27.4676 - val_mae: 4.3237\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 75.2634 - mse: 75.2634 - mae: 6.5304 - val_loss: 27.1886 - val_mse: 27.1886 - val_mae: 4.3064\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 74.8508 - mse: 74.8508 - mae: 6.5156 - val_loss: 26.8521 - val_mse: 26.8521 - val_mae: 4.2755\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 74.5380 - mse: 74.5380 - mae: 6.4937 - val_loss: 26.4805 - val_mse: 26.4805 - val_mae: 4.2381\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 74.1763 - mse: 74.1763 - mae: 6.4696 - val_loss: 26.1180 - val_mse: 26.1180 - val_mae: 4.2016\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 73.8396 - mse: 73.8396 - mae: 6.4425 - val_loss: 25.7569 - val_mse: 25.7569 - val_mae: 4.1659\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 73.4785 - mse: 73.4785 - mae: 6.4135 - val_loss: 25.3915 - val_mse: 25.3915 - val_mae: 4.1280\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 73.2288 - mse: 73.2288 - mae: 6.3824 - val_loss: 24.9287 - val_mse: 24.9287 - val_mae: 4.0797\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 72.9165 - mse: 72.9165 - mae: 6.3481 - val_loss: 24.5874 - val_mse: 24.5874 - val_mae: 4.0430\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 72.5728 - mse: 72.5728 - mae: 6.3213 - val_loss: 24.3018 - val_mse: 24.3018 - val_mae: 4.0110\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 72.2901 - mse: 72.2901 - mae: 6.3049 - val_loss: 24.1239 - val_mse: 24.1239 - val_mae: 3.9896\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 72.0372 - mse: 72.0372 - mae: 6.2918 - val_loss: 23.8704 - val_mse: 23.8704 - val_mae: 3.9595\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 71.7353 - mse: 71.7353 - mae: 6.2742 - val_loss: 23.7132 - val_mse: 23.7132 - val_mae: 3.9397\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 71.4884 - mse: 71.4884 - mae: 6.2607 - val_loss: 23.4775 - val_mse: 23.4775 - val_mae: 3.9113\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 71.3460 - mse: 71.3460 - mae: 6.2550 - val_loss: 23.4306 - val_mse: 23.4306 - val_mae: 3.8999\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 71.0086 - mse: 71.0086 - mae: 6.2293 - val_loss: 22.9913 - val_mse: 22.9913 - val_mae: 3.8529\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 70.7257 - mse: 70.7257 - mae: 6.1963 - val_loss: 22.7625 - val_mse: 22.7625 - val_mae: 3.8326\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 70.5342 - mse: 70.5342 - mae: 6.1737 - val_loss: 22.4894 - val_mse: 22.4894 - val_mae: 3.8067\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 70.3089 - mse: 70.3089 - mae: 6.1539 - val_loss: 22.3057 - val_mse: 22.3057 - val_mae: 3.7918\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 70.0694 - mse: 70.0694 - mae: 6.1396 - val_loss: 22.2262 - val_mse: 22.2262 - val_mae: 3.7895\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 69.8542 - mse: 69.8542 - mae: 6.1245 - val_loss: 21.9942 - val_mse: 21.9942 - val_mae: 3.7685\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 69.6360 - mse: 69.6360 - mae: 6.1035 - val_loss: 21.7308 - val_mse: 21.7308 - val_mae: 3.7420\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 69.4336 - mse: 69.4336 - mae: 6.0837 - val_loss: 21.6279 - val_mse: 21.6279 - val_mae: 3.7355\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 69.1808 - mse: 69.1808 - mae: 6.0624 - val_loss: 21.3443 - val_mse: 21.3443 - val_mae: 3.7056\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 68.9977 - mse: 68.9977 - mae: 6.0400 - val_loss: 21.1464 - val_mse: 21.1464 - val_mae: 3.6855\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 68.7878 - mse: 68.7878 - mae: 6.0158 - val_loss: 20.9665 - val_mse: 20.9665 - val_mae: 3.6681\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 68.5491 - mse: 68.5491 - mae: 6.0052 - val_loss: 20.8376 - val_mse: 20.8376 - val_mae: 3.6589\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 68.3355 - mse: 68.3355 - mae: 5.9896 - val_loss: 20.6764 - val_mse: 20.6764 - val_mae: 3.6438\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 68.1506 - mse: 68.1506 - mae: 5.9838 - val_loss: 20.6311 - val_mse: 20.6311 - val_mae: 3.6478\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 67.9744 - mse: 67.9744 - mae: 5.9845 - val_loss: 20.6558 - val_mse: 20.6558 - val_mae: 3.6616\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 67.7129 - mse: 67.7129 - mae: 5.9716 - val_loss: 20.4507 - val_mse: 20.4507 - val_mae: 3.6383\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 67.5157 - mse: 67.5157 - mae: 5.9580 - val_loss: 20.3549 - val_mse: 20.3549 - val_mae: 3.6311\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 67.3227 - mse: 67.3227 - mae: 5.9286 - val_loss: 19.9517 - val_mse: 19.9517 - val_mae: 3.5760\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 67.0976 - mse: 67.0976 - mae: 5.9004 - val_loss: 19.8549 - val_mse: 19.8549 - val_mae: 3.5689\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 66.9352 - mse: 66.9352 - mae: 5.8845 - val_loss: 19.6690 - val_mse: 19.6690 - val_mae: 3.5473\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 66.7404 - mse: 66.7404 - mae: 5.8738 - val_loss: 19.6807 - val_mse: 19.6807 - val_mae: 3.5539\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 66.6032 - mse: 66.6032 - mae: 5.8632 - val_loss: 19.5521 - val_mse: 19.5521 - val_mae: 3.5359\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 66.4391 - mse: 66.4391 - mae: 5.8614 - val_loss: 19.5792 - val_mse: 19.5792 - val_mae: 3.5466\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 66.2503 - mse: 66.2503 - mae: 5.8608 - val_loss: 19.6055 - val_mse: 19.6055 - val_mae: 3.5523\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 66.1063 - mse: 66.1063 - mae: 5.8495 - val_loss: 19.4617 - val_mse: 19.4617 - val_mae: 3.5342\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 65.9685 - mse: 65.9685 - mae: 5.8354 - val_loss: 19.3351 - val_mse: 19.3351 - val_mae: 3.5188\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 65.8943 - mse: 65.8943 - mae: 5.8322 - val_loss: 19.4341 - val_mse: 19.4341 - val_mae: 3.5387\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 65.7022 - mse: 65.7022 - mae: 5.8224 - val_loss: 19.1926 - val_mse: 19.1926 - val_mae: 3.5053\n",
      "Epoch 82: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.9, model__n_hidden=2, model__n_neurons=125, model__optimizer=adam; total time=   4.6s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 2318.0203 - mse: 2318.0203 - mae: 43.4294 - val_loss: 2468.2131 - val_mse: 2468.2131 - val_mae: 46.0481\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2071.7864 - mse: 2071.7864 - mae: 40.7777 - val_loss: 2204.7400 - val_mse: 2204.7400 - val_mae: 43.3517\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1845.7592 - mse: 1845.7592 - mae: 38.1805 - val_loss: 1958.1696 - val_mse: 1958.1696 - val_mae: 40.6745\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1634.2458 - mse: 1634.2458 - mae: 35.6256 - val_loss: 1731.9257 - val_mse: 1731.9257 - val_mae: 38.0628\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1441.5765 - mse: 1441.5765 - mae: 33.2121 - val_loss: 1527.3110 - val_mse: 1527.3110 - val_mae: 35.5433\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1266.7704 - mse: 1266.7704 - mae: 30.9011 - val_loss: 1342.8413 - val_mse: 1342.8413 - val_mae: 33.1160\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1112.8237 - mse: 1112.8237 - mae: 28.7082 - val_loss: 1175.8405 - val_mse: 1175.8405 - val_mae: 30.7623\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 976.9957 - mse: 976.9957 - mae: 26.6977 - val_loss: 1024.8981 - val_mse: 1024.8981 - val_mae: 28.4821\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 850.3394 - mse: 850.3394 - mae: 24.7694 - val_loss: 894.6059 - val_mse: 894.6059 - val_mae: 26.3594\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 744.9199 - mse: 744.9199 - mae: 23.0054 - val_loss: 779.2339 - val_mse: 779.2339 - val_mae: 24.3387\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 651.8837 - mse: 651.8837 - mae: 21.3886 - val_loss: 677.7322 - val_mse: 677.7322 - val_mae: 22.4238\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 570.9807 - mse: 570.9807 - mae: 19.8913 - val_loss: 589.7221 - val_mse: 589.7221 - val_mae: 20.6283\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 501.2418 - mse: 501.2418 - mae: 18.4443 - val_loss: 512.7806 - val_mse: 512.7806 - val_mae: 18.9363\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 442.6488 - mse: 442.6488 - mae: 17.1656 - val_loss: 445.0881 - val_mse: 445.0881 - val_mae: 17.3326\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 390.7855 - mse: 390.7855 - mae: 15.9983 - val_loss: 386.9532 - val_mse: 386.9532 - val_mae: 15.8407\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 345.5551 - mse: 345.5551 - mae: 14.9253 - val_loss: 338.4121 - val_mse: 338.4121 - val_mae: 14.5102\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 310.3134 - mse: 310.3134 - mae: 14.0511 - val_loss: 295.3426 - val_mse: 295.3426 - val_mae: 13.2942\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 279.0625 - mse: 279.0625 - mae: 13.2528 - val_loss: 258.5723 - val_mse: 258.5723 - val_mae: 12.1734\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 252.9244 - mse: 252.9244 - mae: 12.5730 - val_loss: 227.2957 - val_mse: 227.2957 - val_mae: 11.1649\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 230.2581 - mse: 230.2581 - mae: 11.9474 - val_loss: 201.0680 - val_mse: 201.0680 - val_mae: 10.3657\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 211.5546 - mse: 211.5546 - mae: 11.3973 - val_loss: 178.6026 - val_mse: 178.6026 - val_mae: 9.7033\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 196.5221 - mse: 196.5221 - mae: 10.9371 - val_loss: 158.6789 - val_mse: 158.6789 - val_mae: 9.0820\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 183.1128 - mse: 183.1128 - mae: 10.5181 - val_loss: 141.9447 - val_mse: 141.9447 - val_mae: 8.5477\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 172.2252 - mse: 172.2252 - mae: 10.1545 - val_loss: 127.9770 - val_mse: 127.9770 - val_mae: 8.1124\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 163.2067 - mse: 163.2067 - mae: 9.8383 - val_loss: 116.1315 - val_mse: 116.1315 - val_mae: 7.7422\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 155.6702 - mse: 155.6702 - mae: 9.5616 - val_loss: 105.8160 - val_mse: 105.8160 - val_mae: 7.4327\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 148.7674 - mse: 148.7674 - mae: 9.3170 - val_loss: 97.2309 - val_mse: 97.2309 - val_mae: 7.2065\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 143.6646 - mse: 143.6646 - mae: 9.1160 - val_loss: 89.0889 - val_mse: 89.0889 - val_mae: 6.9937\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 138.3030 - mse: 138.3030 - mae: 8.9049 - val_loss: 82.4519 - val_mse: 82.4519 - val_mae: 6.8056\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 134.0791 - mse: 134.0791 - mae: 8.7372 - val_loss: 76.7115 - val_mse: 76.7115 - val_mae: 6.6275\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 130.2405 - mse: 130.2405 - mae: 8.5774 - val_loss: 71.9773 - val_mse: 71.9773 - val_mae: 6.4637\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 126.9552 - mse: 126.9552 - mae: 8.4411 - val_loss: 67.5450 - val_mse: 67.5450 - val_mae: 6.3031\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 123.8696 - mse: 123.8696 - mae: 8.3055 - val_loss: 63.7652 - val_mse: 63.7652 - val_mae: 6.1529\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 121.0671 - mse: 121.0671 - mae: 8.1898 - val_loss: 60.3696 - val_mse: 60.3696 - val_mae: 6.0069\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 118.4702 - mse: 118.4702 - mae: 8.0760 - val_loss: 57.0658 - val_mse: 57.0658 - val_mae: 5.8579\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 115.6040 - mse: 115.6040 - mae: 7.9540 - val_loss: 54.3978 - val_mse: 54.3978 - val_mae: 5.7242\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 113.1827 - mse: 113.1827 - mae: 7.8528 - val_loss: 51.5416 - val_mse: 51.5416 - val_mae: 5.5778\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 110.6835 - mse: 110.6835 - mae: 7.7401 - val_loss: 48.8680 - val_mse: 48.8680 - val_mae: 5.4336\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 108.3904 - mse: 108.3904 - mae: 7.6430 - val_loss: 46.4105 - val_mse: 46.4105 - val_mae: 5.2885\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 106.2677 - mse: 106.2677 - mae: 7.5457 - val_loss: 43.9296 - val_mse: 43.9296 - val_mae: 5.1368\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 103.9576 - mse: 103.9576 - mae: 7.4479 - val_loss: 42.1370 - val_mse: 42.1370 - val_mae: 5.0056\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 102.1350 - mse: 102.1350 - mae: 7.3634 - val_loss: 40.1633 - val_mse: 40.1633 - val_mae: 4.8731\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 100.2856 - mse: 100.2856 - mae: 7.2779 - val_loss: 38.2564 - val_mse: 38.2564 - val_mae: 4.7686\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 98.5467 - mse: 98.5467 - mae: 7.2030 - val_loss: 36.4930 - val_mse: 36.4930 - val_mae: 4.6695\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 96.9804 - mse: 96.9804 - mae: 7.1438 - val_loss: 35.2647 - val_mse: 35.2647 - val_mae: 4.5909\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 95.6093 - mse: 95.6093 - mae: 7.0846 - val_loss: 33.6914 - val_mse: 33.6914 - val_mae: 4.5063\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 94.1440 - mse: 94.1440 - mae: 7.0226 - val_loss: 32.3181 - val_mse: 32.3181 - val_mae: 4.4349\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 92.8335 - mse: 92.8335 - mae: 6.9688 - val_loss: 31.2243 - val_mse: 31.2243 - val_mae: 4.3734\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 91.7406 - mse: 91.7406 - mae: 6.9265 - val_loss: 30.0611 - val_mse: 30.0611 - val_mae: 4.3047\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 90.4513 - mse: 90.4513 - mae: 6.8793 - val_loss: 29.3563 - val_mse: 29.3563 - val_mae: 4.2574\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 89.3468 - mse: 89.3468 - mae: 6.8484 - val_loss: 28.5720 - val_mse: 28.5720 - val_mae: 4.2074\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 88.3634 - mse: 88.3634 - mae: 6.8177 - val_loss: 27.7131 - val_mse: 27.7131 - val_mae: 4.1514\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 87.2782 - mse: 87.2782 - mae: 6.7848 - val_loss: 26.9202 - val_mse: 26.9202 - val_mae: 4.1061\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 86.3393 - mse: 86.3393 - mae: 6.7539 - val_loss: 26.3159 - val_mse: 26.3159 - val_mae: 4.0707\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 85.5259 - mse: 85.5259 - mae: 6.7279 - val_loss: 25.5088 - val_mse: 25.5088 - val_mae: 4.0208\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 84.7387 - mse: 84.7387 - mae: 6.6989 - val_loss: 24.8664 - val_mse: 24.8664 - val_mae: 3.9889\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 84.1287 - mse: 84.1287 - mae: 6.6789 - val_loss: 24.2578 - val_mse: 24.2578 - val_mae: 3.9574\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 83.4174 - mse: 83.4174 - mae: 6.6580 - val_loss: 23.7481 - val_mse: 23.7481 - val_mae: 3.9288\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 82.8268 - mse: 82.8268 - mae: 6.6314 - val_loss: 23.2788 - val_mse: 23.2788 - val_mae: 3.9006\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 82.3351 - mse: 82.3351 - mae: 6.6131 - val_loss: 22.8685 - val_mse: 22.8685 - val_mae: 3.8763\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 81.8113 - mse: 81.8113 - mae: 6.5977 - val_loss: 22.6012 - val_mse: 22.6012 - val_mae: 3.8617\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 81.3134 - mse: 81.3134 - mae: 6.5813 - val_loss: 22.2458 - val_mse: 22.2458 - val_mae: 3.8385\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 80.8892 - mse: 80.8892 - mae: 6.5619 - val_loss: 21.8628 - val_mse: 21.8628 - val_mae: 3.8148\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 80.4238 - mse: 80.4238 - mae: 6.5353 - val_loss: 21.3924 - val_mse: 21.3924 - val_mae: 3.7825\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 80.1448 - mse: 80.1448 - mae: 6.5031 - val_loss: 20.8079 - val_mse: 20.8079 - val_mae: 3.7382\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 79.7326 - mse: 79.7326 - mae: 6.4794 - val_loss: 20.4522 - val_mse: 20.4522 - val_mae: 3.7147\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 79.3251 - mse: 79.3251 - mae: 6.4592 - val_loss: 20.3088 - val_mse: 20.3088 - val_mae: 3.7102\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 78.9917 - mse: 78.9917 - mae: 6.4464 - val_loss: 20.0548 - val_mse: 20.0548 - val_mae: 3.6928\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 78.7512 - mse: 78.7512 - mae: 6.4325 - val_loss: 19.9104 - val_mse: 19.9104 - val_mae: 3.6875\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 78.4185 - mse: 78.4185 - mae: 6.4218 - val_loss: 19.7091 - val_mse: 19.7091 - val_mae: 3.6741\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 78.1344 - mse: 78.1344 - mae: 6.4109 - val_loss: 19.6437 - val_mse: 19.6437 - val_mae: 3.6737\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 77.8647 - mse: 77.8647 - mae: 6.3935 - val_loss: 19.2872 - val_mse: 19.2872 - val_mae: 3.6422\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 77.6166 - mse: 77.6166 - mae: 6.3738 - val_loss: 19.0698 - val_mse: 19.0698 - val_mae: 3.6249\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 77.3631 - mse: 77.3631 - mae: 6.3593 - val_loss: 18.8860 - val_mse: 18.8860 - val_mae: 3.6103\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 77.1270 - mse: 77.1270 - mae: 6.3475 - val_loss: 18.7967 - val_mse: 18.7967 - val_mae: 3.6081\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 76.9654 - mse: 76.9654 - mae: 6.3354 - val_loss: 18.5777 - val_mse: 18.5777 - val_mae: 3.5923\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 76.7090 - mse: 76.7090 - mae: 6.3310 - val_loss: 18.5959 - val_mse: 18.5959 - val_mae: 3.6015\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 76.5332 - mse: 76.5332 - mae: 6.3370 - val_loss: 18.8383 - val_mse: 18.8383 - val_mae: 3.6360\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 76.3124 - mse: 76.3124 - mae: 6.3314 - val_loss: 18.5810 - val_mse: 18.5810 - val_mae: 3.6122\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 76.0857 - mse: 76.0857 - mae: 6.3125 - val_loss: 18.4412 - val_mse: 18.4412 - val_mae: 3.6010\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 75.9057 - mse: 75.9057 - mae: 6.2966 - val_loss: 18.3369 - val_mse: 18.3369 - val_mae: 3.5933\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 75.7531 - mse: 75.7531 - mae: 6.2795 - val_loss: 18.0433 - val_mse: 18.0433 - val_mae: 3.5666\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 75.5708 - mse: 75.5708 - mae: 6.2595 - val_loss: 17.8814 - val_mse: 17.8814 - val_mae: 3.5510\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 75.4161 - mse: 75.4161 - mae: 6.2504 - val_loss: 17.9570 - val_mse: 17.9570 - val_mae: 3.5643\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 75.2310 - mse: 75.2310 - mae: 6.2497 - val_loss: 17.9237 - val_mse: 17.9237 - val_mae: 3.5625\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 75.0819 - mse: 75.0819 - mae: 6.2401 - val_loss: 17.8873 - val_mse: 17.8873 - val_mae: 3.5608\n",
      "Epoch 86: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.9, model__n_hidden=2, model__n_neurons=125, model__optimizer=adam; total time=   4.6s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 7077.5039 - mse: 7077.5039 - mae: 83.5264 - val_loss: 6499.2822 - val_mse: 6499.2822 - val_mae: 80.0550\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6571.7056 - mse: 6571.7056 - mae: 80.4686 - val_loss: 6020.9917 - val_mse: 6020.9917 - val_mae: 77.0129\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6095.7988 - mse: 6095.7988 - mae: 77.4622 - val_loss: 5565.4409 - val_mse: 5565.4409 - val_mae: 73.9939\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 5640.9683 - mse: 5640.9683 - mae: 74.4828 - val_loss: 5137.3252 - val_mse: 5137.3252 - val_mae: 71.0344\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 5213.5889 - mse: 5213.5889 - mae: 71.5578 - val_loss: 4734.9976 - val_mse: 4734.9976 - val_mae: 68.1296\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 4810.1377 - mse: 4810.1377 - mae: 68.6793 - val_loss: 4359.3398 - val_mse: 4359.3398 - val_mae: 65.2953\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4432.0054 - mse: 4432.0054 - mae: 65.8791 - val_loss: 4009.3628 - val_mse: 4009.3628 - val_mae: 62.5330\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4083.4319 - mse: 4083.4319 - mae: 63.1491 - val_loss: 3680.9612 - val_mse: 3680.9612 - val_mae: 59.8187\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3751.9761 - mse: 3751.9761 - mae: 60.4692 - val_loss: 3376.4861 - val_mse: 3376.4861 - val_mae: 57.1820\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3447.1990 - mse: 3447.1990 - mae: 57.8705 - val_loss: 3092.3928 - val_mse: 3092.3928 - val_mae: 54.6000\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3159.9509 - mse: 3159.9509 - mae: 55.3238 - val_loss: 2829.4846 - val_mse: 2829.4846 - val_mae: 52.0905\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2896.1868 - mse: 2896.1868 - mae: 52.8534 - val_loss: 2585.4707 - val_mse: 2585.4707 - val_mae: 49.6415\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 2651.5823 - mse: 2651.5823 - mae: 50.4677 - val_loss: 2360.1921 - val_mse: 2360.1921 - val_mae: 47.2602\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2424.0762 - mse: 2424.0762 - mae: 48.1259 - val_loss: 2152.9749 - val_mse: 2152.9749 - val_mae: 44.9522\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2215.8799 - mse: 2215.8799 - mae: 45.8573 - val_loss: 1961.6030 - val_mse: 1961.6030 - val_mae: 42.7040\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2021.5328 - mse: 2021.5328 - mae: 43.6518 - val_loss: 1786.9821 - val_mse: 1786.9821 - val_mae: 40.5363\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1845.6722 - mse: 1845.6722 - mae: 41.5204 - val_loss: 1625.8585 - val_mse: 1625.8585 - val_mae: 38.4214\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1681.9354 - mse: 1681.9354 - mae: 39.4698 - val_loss: 1479.5553 - val_mse: 1479.5553 - val_mae: 36.3868\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1532.6985 - mse: 1532.6985 - mae: 37.4925 - val_loss: 1346.5538 - val_mse: 1346.5538 - val_mae: 34.4253\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1397.2069 - mse: 1397.2069 - mae: 35.5714 - val_loss: 1225.7794 - val_mse: 1225.7794 - val_mae: 32.5317\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1272.9408 - mse: 1272.9408 - mae: 33.7251 - val_loss: 1116.2889 - val_mse: 1116.2889 - val_mae: 30.7122\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1160.0803 - mse: 1160.0803 - mae: 31.9518 - val_loss: 1017.5866 - val_mse: 1017.5866 - val_mae: 28.9655\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1057.0402 - mse: 1057.0402 - mae: 30.2480 - val_loss: 928.5819 - val_mse: 928.5819 - val_mae: 27.2917\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 964.7153 - mse: 964.7153 - mae: 28.6367 - val_loss: 847.6807 - val_mse: 847.6807 - val_mae: 25.7356\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 881.2554 - mse: 881.2554 - mae: 27.0856 - val_loss: 774.5886 - val_mse: 774.5886 - val_mae: 24.3218\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 804.2897 - mse: 804.2897 - mae: 25.6102 - val_loss: 709.5817 - val_mse: 709.5817 - val_mae: 22.9804\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 736.3917 - mse: 736.3917 - mae: 24.2334 - val_loss: 651.2163 - val_mse: 651.2163 - val_mae: 21.7002\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 674.4576 - mse: 674.4576 - mae: 22.9484 - val_loss: 599.5152 - val_mse: 599.5152 - val_mae: 20.6011\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 619.1416 - mse: 619.1416 - mae: 21.7519 - val_loss: 553.4753 - val_mse: 553.4753 - val_mae: 19.6275\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 570.2767 - mse: 570.2767 - mae: 20.6985 - val_loss: 512.5538 - val_mse: 512.5538 - val_mae: 18.7383\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 525.6553 - mse: 525.6553 - mae: 19.7624 - val_loss: 476.3813 - val_mse: 476.3813 - val_mae: 17.8935\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 485.4008 - mse: 485.4008 - mae: 18.9276 - val_loss: 444.6962 - val_mse: 444.6962 - val_mae: 17.1113\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 451.2888 - mse: 451.2888 - mae: 18.1813 - val_loss: 416.0232 - val_mse: 416.0232 - val_mae: 16.5848\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 419.5449 - mse: 419.5449 - mae: 17.4632 - val_loss: 391.0686 - val_mse: 391.0686 - val_mae: 16.0963\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 391.1497 - mse: 391.1497 - mae: 16.8061 - val_loss: 369.2799 - val_mse: 369.2799 - val_mae: 15.6329\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 366.6826 - mse: 366.6826 - mae: 16.2047 - val_loss: 349.7298 - val_mse: 349.7298 - val_mae: 15.1824\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 345.3773 - mse: 345.3773 - mae: 15.6826 - val_loss: 332.3994 - val_mse: 332.3994 - val_mae: 14.7503\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 325.1293 - mse: 325.1293 - mae: 15.1774 - val_loss: 317.5466 - val_mse: 317.5466 - val_mae: 14.4061\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 307.4287 - mse: 307.4287 - mae: 14.7173 - val_loss: 304.5268 - val_mse: 304.5268 - val_mae: 14.0832\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 291.7972 - mse: 291.7972 - mae: 14.3082 - val_loss: 292.7796 - val_mse: 292.7796 - val_mae: 13.7688\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 278.2769 - mse: 278.2769 - mae: 13.9518 - val_loss: 282.0363 - val_mse: 282.0363 - val_mae: 13.4580\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 265.2554 - mse: 265.2554 - mae: 13.5844 - val_loss: 272.6377 - val_mse: 272.6377 - val_mae: 13.1646\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 253.8857 - mse: 253.8857 - mae: 13.2479 - val_loss: 264.0880 - val_mse: 264.0880 - val_mae: 12.8889\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 243.7636 - mse: 243.7636 - mae: 12.9273 - val_loss: 256.2706 - val_mse: 256.2706 - val_mae: 12.7110\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 234.2928 - mse: 234.2928 - mae: 12.6375 - val_loss: 249.2552 - val_mse: 249.2552 - val_mae: 12.5789\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 226.2489 - mse: 226.2489 - mae: 12.3681 - val_loss: 242.6667 - val_mse: 242.6667 - val_mae: 12.4672\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 218.7367 - mse: 218.7367 - mae: 12.1159 - val_loss: 236.8128 - val_mse: 236.8128 - val_mae: 12.3594\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 211.5261 - mse: 211.5261 - mae: 11.8760 - val_loss: 231.4347 - val_mse: 231.4347 - val_mae: 12.2436\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 205.2845 - mse: 205.2845 - mae: 11.6513 - val_loss: 226.3188 - val_mse: 226.3188 - val_mae: 12.1238\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 199.8398 - mse: 199.8398 - mae: 11.4643 - val_loss: 221.4549 - val_mse: 221.4549 - val_mae: 12.0056\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 194.3759 - mse: 194.3759 - mae: 11.2732 - val_loss: 217.0337 - val_mse: 217.0337 - val_mae: 11.9025\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 189.4557 - mse: 189.4557 - mae: 11.0941 - val_loss: 212.6416 - val_mse: 212.6416 - val_mae: 11.7983\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 184.5216 - mse: 184.5216 - mae: 10.9173 - val_loss: 208.4625 - val_mse: 208.4625 - val_mae: 11.6894\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 180.3534 - mse: 180.3534 - mae: 10.7608 - val_loss: 204.3459 - val_mse: 204.3459 - val_mae: 11.5799\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 176.0959 - mse: 176.0959 - mae: 10.6020 - val_loss: 200.5453 - val_mse: 200.5453 - val_mae: 11.4755\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 172.1391 - mse: 172.1391 - mae: 10.4600 - val_loss: 196.9480 - val_mse: 196.9480 - val_mae: 11.3712\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 168.4303 - mse: 168.4303 - mae: 10.3198 - val_loss: 193.2722 - val_mse: 193.2722 - val_mae: 11.2572\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 165.0329 - mse: 165.0329 - mae: 10.1901 - val_loss: 189.6305 - val_mse: 189.6305 - val_mae: 11.1414\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 161.5966 - mse: 161.5966 - mae: 10.0592 - val_loss: 186.2051 - val_mse: 186.2051 - val_mae: 11.0296\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 158.4096 - mse: 158.4096 - mae: 9.9372 - val_loss: 182.8361 - val_mse: 182.8361 - val_mae: 10.9165\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 155.3463 - mse: 155.3463 - mae: 9.8185 - val_loss: 179.5134 - val_mse: 179.5134 - val_mae: 10.8013\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 152.4540 - mse: 152.4540 - mae: 9.7113 - val_loss: 176.4249 - val_mse: 176.4249 - val_mae: 10.6928\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 149.6810 - mse: 149.6810 - mae: 9.5991 - val_loss: 173.3692 - val_mse: 173.3692 - val_mae: 10.5825\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 146.8740 - mse: 146.8740 - mae: 9.4891 - val_loss: 170.1556 - val_mse: 170.1556 - val_mae: 10.4634\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 144.4585 - mse: 144.4585 - mae: 9.3885 - val_loss: 167.1677 - val_mse: 167.1677 - val_mae: 10.3513\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 141.8395 - mse: 141.8395 - mae: 9.2814 - val_loss: 164.1512 - val_mse: 164.1512 - val_mae: 10.2360\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 139.3826 - mse: 139.3826 - mae: 9.1863 - val_loss: 161.1588 - val_mse: 161.1588 - val_mae: 10.1197\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 137.0116 - mse: 137.0116 - mae: 9.0870 - val_loss: 158.3969 - val_mse: 158.3969 - val_mae: 10.0104\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 134.7203 - mse: 134.7203 - mae: 8.9987 - val_loss: 155.4512 - val_mse: 155.4512 - val_mae: 9.8933\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 132.5555 - mse: 132.5555 - mae: 8.9030 - val_loss: 152.6383 - val_mse: 152.6383 - val_mae: 9.7794\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 130.4302 - mse: 130.4302 - mae: 8.8165 - val_loss: 149.7954 - val_mse: 149.7954 - val_mae: 9.6630\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 128.2729 - mse: 128.2729 - mae: 8.7273 - val_loss: 147.3335 - val_mse: 147.3335 - val_mae: 9.5597\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 126.3165 - mse: 126.3165 - mae: 8.6432 - val_loss: 144.6038 - val_mse: 144.6038 - val_mae: 9.4456\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 124.3923 - mse: 124.3923 - mae: 8.5608 - val_loss: 142.0995 - val_mse: 142.0995 - val_mae: 9.3381\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 122.4097 - mse: 122.4097 - mae: 8.4739 - val_loss: 139.6401 - val_mse: 139.6401 - val_mae: 9.2323\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 120.5451 - mse: 120.5451 - mae: 8.3997 - val_loss: 137.1578 - val_mse: 137.1578 - val_mae: 9.1243\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 118.7353 - mse: 118.7353 - mae: 8.3196 - val_loss: 134.7945 - val_mse: 134.7945 - val_mae: 9.0189\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 116.9445 - mse: 116.9445 - mae: 8.2437 - val_loss: 132.3888 - val_mse: 132.3888 - val_mae: 8.9101\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 115.2538 - mse: 115.2538 - mae: 8.1683 - val_loss: 130.0749 - val_mse: 130.0749 - val_mae: 8.8028\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 113.4601 - mse: 113.4601 - mae: 8.0930 - val_loss: 127.5611 - val_mse: 127.5611 - val_mae: 8.6893\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 111.8189 - mse: 111.8189 - mae: 8.0210 - val_loss: 125.0689 - val_mse: 125.0689 - val_mae: 8.5792\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 110.1495 - mse: 110.1495 - mae: 7.9498 - val_loss: 122.8597 - val_mse: 122.8597 - val_mae: 8.4834\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 108.5559 - mse: 108.5559 - mae: 7.8716 - val_loss: 120.6698 - val_mse: 120.6698 - val_mae: 8.3876\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 107.0071 - mse: 107.0071 - mae: 7.7981 - val_loss: 118.5632 - val_mse: 118.5632 - val_mae: 8.2943\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 105.5099 - mse: 105.5099 - mae: 7.7254 - val_loss: 116.6500 - val_mse: 116.6500 - val_mae: 8.2080\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 103.9537 - mse: 103.9537 - mae: 7.6537 - val_loss: 114.4895 - val_mse: 114.4895 - val_mae: 8.1095\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 102.5725 - mse: 102.5725 - mae: 7.5898 - val_loss: 112.0384 - val_mse: 112.0384 - val_mae: 7.9963\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 101.0726 - mse: 101.0726 - mae: 7.5177 - val_loss: 110.0240 - val_mse: 110.0240 - val_mae: 7.9022\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 99.7186 - mse: 99.7186 - mae: 7.4514 - val_loss: 107.9489 - val_mse: 107.9489 - val_mae: 7.8039\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 98.4086 - mse: 98.4086 - mae: 7.3869 - val_loss: 105.9037 - val_mse: 105.9037 - val_mae: 7.7058\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 97.0496 - mse: 97.0496 - mae: 7.3194 - val_loss: 104.1585 - val_mse: 104.1585 - val_mae: 7.6208\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 95.8225 - mse: 95.8225 - mae: 7.2572 - val_loss: 102.4136 - val_mse: 102.4136 - val_mae: 7.5364\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 94.6510 - mse: 94.6510 - mae: 7.1990 - val_loss: 100.7416 - val_mse: 100.7416 - val_mae: 7.4588\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 93.4553 - mse: 93.4553 - mae: 7.1377 - val_loss: 99.1309 - val_mse: 99.1309 - val_mae: 7.3828\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 92.2115 - mse: 92.2115 - mae: 7.0819 - val_loss: 97.2860 - val_mse: 97.2860 - val_mae: 7.2914\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 91.1356 - mse: 91.1356 - mae: 7.0300 - val_loss: 95.4730 - val_mse: 95.4730 - val_mae: 7.2010\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 89.8750 - mse: 89.8750 - mae: 6.9744 - val_loss: 94.0316 - val_mse: 94.0316 - val_mae: 7.1293\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 88.8610 - mse: 88.8610 - mae: 6.9229 - val_loss: 92.4585 - val_mse: 92.4585 - val_mae: 7.0510\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 87.8008 - mse: 87.8008 - mae: 6.8699 - val_loss: 90.7642 - val_mse: 90.7642 - val_mae: 6.9644\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 86.7180 - mse: 86.7180 - mae: 6.8187 - val_loss: 89.3560 - val_mse: 89.3560 - val_mae: 6.8928\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.9, model__n_hidden=2, model__n_neurons=125, model__optimizer=adam; total time=   5.6s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 12414739456.0000 - mse: 12414739456.0000 - mae: 11782.8936 - val_loss: 8437.1328 - val_mse: 8437.1328 - val_mae: 91.6462\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 15284.4795 - mse: 15284.4785 - mae: 122.0820 - val_loss: 22372.9902 - val_mse: 22372.9902 - val_mae: 149.4486\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 26820.3887 - mse: 26820.3887 - mae: 163.3882 - val_loss: 29948.9023 - val_mse: 29948.9023 - val_mae: 172.9474\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 32373.9258 - mse: 32373.9258 - mae: 179.6823 - val_loss: 32968.8281 - val_mse: 32968.8281 - val_mae: 181.4682\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 34331.3359 - mse: 34331.3359 - mae: 185.0614 - val_loss: 33695.8320 - val_mse: 33695.8320 - val_mae: 183.4604\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 34564.5625 - mse: 34564.5625 - mae: 185.6902 - val_loss: 33389.8242 - val_mse: 33389.8242 - val_mae: 182.6245\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 34039.9375 - mse: 34039.9375 - mae: 184.2714 - val_loss: 32648.2773 - val_mse: 32648.2773 - val_mae: 180.5829\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 33202.0469 - mse: 33202.0469 - mae: 181.9862 - val_loss: 31736.5801 - val_mse: 31736.5801 - val_mae: 178.0406\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 32248.8652 - mse: 32248.8652 - mae: 179.3458 - val_loss: 30768.9277 - val_mse: 30768.9277 - val_mae: 175.3021\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 31263.2734 - mse: 31263.2734 - mae: 176.5754 - val_loss: 29795.5293 - val_mse: 29795.5293 - val_mae: 172.5034\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 30281.4121 - mse: 30281.4121 - mae: 173.7757 - val_loss: 28839.0469 - val_mse: 28839.0469 - val_mae: 169.7084\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.9, model__n_hidden=2, model__n_neurons=25, model__optimizer=momentum; total time=   1.1s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 419605443831689904128.0000 - mse: 419605443831689904128.0000 - mae: 7463092736.0000 - val_loss: 2123050384359424.0000 - val_mse: 2123050384359424.0000 - val_mae: 46076576.0000\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 4240946271092736.0000 - mse: 4240946271092736.0000 - mae: 64199812.0000 - val_loss: 6974026161324032.0000 - val_mse: 6974026161324032.0000 - val_mae: 83510632.0000\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 8304240883662848.0000 - mse: 8304240883662848.0000 - mae: 91016432.0000 - val_loss: 9774210620588032.0000 - val_mse: 9774210620588032.0000 - val_mae: 98864600.0000\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 10352192959545344.0000 - mse: 10352192959545344.0000 - mae: 101731616.0000 - val_loss: 10935732986183680.0000 - val_mse: 10935732986183680.0000 - val_mae: 104574056.0000\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 11113204256079872.0000 - mse: 11113204256079872.0000 - mae: 105418136.0000 - val_loss: 11252942090797056.0000 - val_mse: 11252942090797056.0000 - val_mae: 106079896.0000\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 11245647088844800.0000 - mse: 11245647088844800.0000 - mae: 106045472.0000 - val_loss: 11185705451520000.0000 - val_mse: 11185705451520000.0000 - val_mae: 105762496.0000\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 11098757059837952.0000 - mse: 11098757059837952.0000 - mae: 105350232.0000 - val_loss: 10954378512957440.0000 - val_mse: 10954378512957440.0000 - val_mae: 104663176.0000\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 10835295175966720.0000 - mse: 10835295175966720.0000 - mae: 104092000.0000 - val_loss: 10657926985285632.0000 - val_mse: 10657926985285632.0000 - val_mae: 103237232.0000\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 10527528556953600.0000 - mse: 10527528556953600.0000 - mae: 102602880.0000 - val_loss: 10339575419371520.0000 - val_mse: 10339575419371520.0000 - val_mae: 101683704.0000\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 10206827643928576.0000 - mse: 10206827643928576.0000 - mae: 101027896.0000 - val_loss: 10017881295159296.0000 - val_mse: 10017881295159296.0000 - val_mae: 100089360.0000\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 9886578776211456.0000 - mse: 9886578776211456.0000 - mae: 99430312.0000 - val_loss: 9700670043062272.0000 - val_mse: 9700670043062272.0000 - val_mae: 98491992.0000\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.9, model__n_hidden=2, model__n_neurons=25, model__optimizer=momentum; total time=   1.1s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 416665120.0000 - mse: 416665120.0000 - mae: 3209.1589 - val_loss: 1493454.1250 - val_mse: 1493454.1250 - val_mae: 1222.0402\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 275151.8438 - mse: 275151.8438 - mae: 402.5088 - val_loss: 15234.6641 - val_mse: 15234.6641 - val_mae: 123.1376\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 36443.1797 - mse: 36443.1797 - mae: 170.6756 - val_loss: 50209.0508 - val_mse: 50209.0508 - val_mae: 223.9135\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 19531.1992 - mse: 19531.1992 - mae: 126.8415 - val_loss: 3614.2432 - val_mse: 3614.2432 - val_mae: 59.5185\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 5077.9067 - mse: 5077.9067 - mae: 65.1082 - val_loss: 295.1628 - val_mse: 295.1628 - val_mae: 15.0857\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1934.6641 - mse: 1934.6641 - mae: 39.5130 - val_loss: 469.8975 - val_mse: 469.8975 - val_mae: 20.2200\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 721.3911 - mse: 721.3911 - mae: 23.2912 - val_loss: 713.3647 - val_mse: 713.3647 - val_mae: 25.3293\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 330.3043 - mse: 330.3043 - mae: 15.2714 - val_loss: 451.7889 - val_mse: 451.7889 - val_mae: 19.7949\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 236.2577 - mse: 236.2577 - mae: 12.9584 - val_loss: 135.5529 - val_mse: 135.5529 - val_mae: 8.9966\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 162.6162 - mse: 162.6162 - mae: 9.6844 - val_loss: 73.6921 - val_mse: 73.6921 - val_mae: 6.1251\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 96.3175 - mse: 96.3175 - mae: 7.9929 - val_loss: 96.4009 - val_mse: 96.4009 - val_mae: 8.5617\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 88.1797 - mse: 88.1797 - mae: 7.0686 - val_loss: 82.0067 - val_mse: 82.0067 - val_mae: 6.2368\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 84.9110 - mse: 84.9110 - mae: 6.5027 - val_loss: 72.3213 - val_mse: 72.3213 - val_mae: 6.4843\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 79.7057 - mse: 79.7057 - mae: 6.6934 - val_loss: 73.3039 - val_mse: 73.3039 - val_mae: 6.6517\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 77.8829 - mse: 77.8829 - mae: 6.3826 - val_loss: 72.2579 - val_mse: 72.2579 - val_mae: 6.2024\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 78.6185 - mse: 78.6185 - mae: 6.3253 - val_loss: 71.8143 - val_mse: 71.8143 - val_mae: 6.2619\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 79.8059 - mse: 79.8059 - mae: 6.5528 - val_loss: 72.3005 - val_mse: 72.3005 - val_mae: 6.4795\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 77.6056 - mse: 77.6056 - mae: 6.3593 - val_loss: 72.6910 - val_mse: 72.6910 - val_mae: 6.1729\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 79.2072 - mse: 79.2072 - mae: 6.3671 - val_loss: 71.9776 - val_mse: 71.9776 - val_mae: 6.3877\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 78.6028 - mse: 78.6028 - mae: 6.4522 - val_loss: 71.7948 - val_mse: 71.7948 - val_mae: 6.2799\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 78.1473 - mse: 78.1473 - mae: 6.3623 - val_loss: 71.8018 - val_mse: 71.8018 - val_mae: 6.3060\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 78.2537 - mse: 78.2537 - mae: 6.3560 - val_loss: 71.8184 - val_mse: 71.8184 - val_mae: 6.2604\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 78.4617 - mse: 78.4617 - mae: 6.3436 - val_loss: 71.9453 - val_mse: 71.9453 - val_mae: 6.3775\n",
      "Epoch 23: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.9, model__n_hidden=2, model__n_neurons=25, model__optimizer=momentum; total time=   1.7s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 68184711364608.0000 - mse: 68184711364608.0000 - mae: 2927758.2500 - val_loss: 25378369046577152.0000 - val_mse: 25378369046577152.0000 - val_mae: 155923216.0000\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 17662637546740416560234496.0000 - mse: 17662637546740416560234496.0000 - mae: 1560093392896.0000 - val_loss: 6328367765829885636550590464.0000 - val_mse: 6328367765829885636550590464.0000 - val_mae: 78017233682432.0000\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: inf - mse: inf - mae: 760616649169567744.0000 - val_loss: inf - val_mse: inf - val_mae: 40779149045839953920.0000\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: inf - mse: inf - mae: 411238886127541992554496.0000 - val_loss: inf - val_mse: inf - val_mae: 18791522943819219990478848.0000\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: inf - mse: inf - mae: 193676263162816570591096602624.0000 - val_loss: inf - val_mse: inf - val_mae: 9471166733248074101611537891328.0000\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: inf - mse: inf - mae: 90417011212536604424559206827294720.0000 - val_loss: inf - val_mse: inf - val_mae: inf\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.1, model__n_hidden=0, model__n_neurons=125, model__optimizer=momentum; total time=   1.0s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 21ms/step - loss: 21658379223040.0000 - mse: 21658379223040.0000 - mae: 1728008.8750 - val_loss: 6662244284760064.0000 - val_mse: 6662244284760064.0000 - val_mae: 80154928.0000\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3095443783613032937029632.0000 - mse: 3095444071843409088741376.0000 - mae: 644602396672.0000 - val_loss: 1086176019459825179578859520.0000 - val_mse: 1086176019459825179578859520.0000 - val_mae: 32365009174528.0000\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 502670643361270202106479776354009088.0000 - mse: 502670643361270202106479776354009088.0000 - mae: 260966112735264768.0000 - val_loss: inf - val_mse: inf - val_mae: 12061781095167819776.0000\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: inf - mse: inf - mae: 100398854579361524219904.0000 - val_loss: inf - val_mse: inf - val_mae: 4399000823865039808102400.0000\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: inf - mse: inf - mae: 36202785380901623183754395648.0000 - val_loss: inf - val_mse: inf - val_mae: 1675425812656067376984501518336.0000\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: inf - mse: inf - mae: 12596155028152397837464154597752832.0000 - val_loss: inf - val_mse: inf - val_mae: 619302104485868676420585451467833344.0000\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.1, model__n_hidden=0, model__n_neurons=125, model__optimizer=momentum; total time=   1.1s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 24ms/step - loss: 10348024299520.0000 - mse: 10348024299520.0000 - mae: 1224138.6250 - val_loss: 2765244631875584.0000 - val_mse: 2765244631875584.0000 - val_mae: 51435904.0000\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 984950569160824491868160.0000 - mse: 984950497103230453940224.0000 - mae: 358722895872.0000 - val_loss: 326544448060247219801948160.0000 - val_mse: 326544448060247219801948160.0000 - val_mae: 17656725897216.0000\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 124037838502195990271898753100152832.0000 - mse: 124037838502195990271898753100152832.0000 - mae: 135138680657412096.0000 - val_loss: inf - val_mse: inf - val_mae: 5936325950525407232.0000\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: inf - mse: inf - mae: 45210200010891278155776.0000 - val_loss: inf - val_mse: inf - val_mae: 2062082400761737130278912.0000\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: inf - mse: inf - mae: 16604508768626998626152873984.0000 - val_loss: inf - val_mse: inf - val_mae: 709806136761166692549987926016.0000\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: inf - mse: inf - mae: 5490828574888575652582969934086144.0000 - val_loss: inf - val_mse: inf - val_mae: 234584804526666191968790834267029504.0000\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.1, model__n_hidden=0, model__n_neurons=125, model__optimizer=momentum; total time=   1.1s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 69ms/step - loss: 15150.5215 - mse: 15150.5215 - mae: 80.8708 - val_loss: 18486.3535 - val_mse: 18486.3535 - val_mae: 91.4119\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 15139.3867 - mse: 15139.3867 - mae: 80.8438 - val_loss: 18472.7285 - val_mse: 18472.7285 - val_mae: 91.3845\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 15127.8047 - mse: 15127.8047 - mae: 80.8167 - val_loss: 18459.3262 - val_mse: 18459.3262 - val_mae: 91.3575\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 15116.5156 - mse: 15116.5156 - mae: 80.7890 - val_loss: 18445.8418 - val_mse: 18445.8418 - val_mae: 91.3304\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 15105.5576 - mse: 15105.5576 - mae: 80.7625 - val_loss: 18432.4082 - val_mse: 18432.4082 - val_mae: 91.3033\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 15093.9619 - mse: 15093.9619 - mae: 80.7332 - val_loss: 18419.1777 - val_mse: 18419.1777 - val_mae: 91.2767\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 15082.9072 - mse: 15082.9072 - mae: 80.7067 - val_loss: 18405.7480 - val_mse: 18405.7480 - val_mae: 91.2496\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 15071.6592 - mse: 15071.6592 - mae: 80.6785 - val_loss: 18392.1641 - val_mse: 18392.1641 - val_mae: 91.2221\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 15060.0400 - mse: 15060.0400 - mae: 80.6500 - val_loss: 18378.7070 - val_mse: 18378.7070 - val_mae: 91.1949\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 15048.7021 - mse: 15048.7021 - mae: 80.6244 - val_loss: 18365.4297 - val_mse: 18365.4297 - val_mae: 91.1681\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 15037.9111 - mse: 15037.9111 - mae: 80.5970 - val_loss: 18351.7754 - val_mse: 18351.7754 - val_mae: 91.1404\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 15026.5547 - mse: 15026.5547 - mae: 80.5712 - val_loss: 18338.6660 - val_mse: 18338.6660 - val_mae: 91.1138\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 15015.2510 - mse: 15015.2510 - mae: 80.5423 - val_loss: 18325.6250 - val_mse: 18325.6250 - val_mae: 91.0874\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 15004.9463 - mse: 15004.9463 - mae: 80.5175 - val_loss: 18311.9844 - val_mse: 18311.9844 - val_mae: 91.0597\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 14993.0967 - mse: 14993.0967 - mae: 80.4907 - val_loss: 18299.0762 - val_mse: 18299.0762 - val_mae: 91.0335\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14982.0381 - mse: 14982.0381 - mae: 80.4656 - val_loss: 18286.0020 - val_mse: 18286.0020 - val_mae: 91.0069\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14971.5723 - mse: 14971.5723 - mae: 80.4391 - val_loss: 18272.3535 - val_mse: 18272.3535 - val_mae: 90.9792\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14960.2979 - mse: 14960.2979 - mae: 80.4131 - val_loss: 18258.9375 - val_mse: 18258.9375 - val_mae: 90.9519\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 14948.8555 - mse: 14948.8555 - mae: 80.3859 - val_loss: 18246.0801 - val_mse: 18246.0801 - val_mae: 90.9257\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14938.0352 - mse: 14938.0352 - mae: 80.3601 - val_loss: 18232.9785 - val_mse: 18232.9785 - val_mae: 90.8989\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 14927.1016 - mse: 14927.1016 - mae: 80.3334 - val_loss: 18219.8145 - val_mse: 18219.8145 - val_mae: 90.8720\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 14916.0537 - mse: 14916.0537 - mae: 80.3075 - val_loss: 18206.7969 - val_mse: 18206.7969 - val_mae: 90.8455\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 14905.4062 - mse: 14905.4062 - mae: 80.2832 - val_loss: 18193.5488 - val_mse: 18193.5488 - val_mae: 90.8184\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14894.3281 - mse: 14894.3281 - mae: 80.2563 - val_loss: 18180.4453 - val_mse: 18180.4453 - val_mae: 90.7915\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14883.4854 - mse: 14883.4854 - mae: 80.2311 - val_loss: 18167.3906 - val_mse: 18167.3906 - val_mae: 90.7648\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14872.3447 - mse: 14872.3447 - mae: 80.2045 - val_loss: 18154.4980 - val_mse: 18154.4980 - val_mae: 90.7384\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14861.6670 - mse: 14861.6670 - mae: 80.1785 - val_loss: 18141.3184 - val_mse: 18141.3184 - val_mae: 90.7114\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14850.8428 - mse: 14850.8428 - mae: 80.1523 - val_loss: 18128.3984 - val_mse: 18128.3984 - val_mae: 90.6849\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 14840.1230 - mse: 14840.1230 - mae: 80.1293 - val_loss: 18115.4336 - val_mse: 18115.4336 - val_mae: 90.6583\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 14829.0156 - mse: 14829.0156 - mae: 80.1014 - val_loss: 18102.5332 - val_mse: 18102.5332 - val_mae: 90.6318\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14818.4160 - mse: 14818.4160 - mae: 80.0755 - val_loss: 18089.4844 - val_mse: 18089.4844 - val_mae: 90.6049\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14807.4492 - mse: 14807.4492 - mae: 80.0484 - val_loss: 18076.5781 - val_mse: 18076.5781 - val_mae: 90.5784\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14796.9648 - mse: 14796.9648 - mae: 80.0257 - val_loss: 18063.5332 - val_mse: 18063.5332 - val_mae: 90.5515\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14785.9561 - mse: 14785.9561 - mae: 79.9991 - val_loss: 18050.9375 - val_mse: 18050.9375 - val_mae: 90.5256\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14775.5273 - mse: 14775.5273 - mae: 79.9742 - val_loss: 18038.2656 - val_mse: 18038.2656 - val_mae: 90.4994\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14764.2090 - mse: 14764.2090 - mae: 79.9481 - val_loss: 18026.0781 - val_mse: 18026.0781 - val_mae: 90.4743\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14754.2080 - mse: 14754.2080 - mae: 79.9264 - val_loss: 18012.7910 - val_mse: 18012.7910 - val_mae: 90.4468\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14743.3926 - mse: 14743.3926 - mae: 79.9009 - val_loss: 17999.5977 - val_mse: 17999.5977 - val_mae: 90.4195\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 14732.5488 - mse: 14732.5488 - mae: 79.8760 - val_loss: 17986.5781 - val_mse: 17986.5781 - val_mae: 90.3927\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14721.7354 - mse: 14721.7354 - mae: 79.8526 - val_loss: 17973.7402 - val_mse: 17973.7402 - val_mae: 90.3660\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14710.9404 - mse: 14710.9404 - mae: 79.8270 - val_loss: 17960.9629 - val_mse: 17960.9629 - val_mae: 90.3395\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 14700.2002 - mse: 14700.2002 - mae: 79.8026 - val_loss: 17948.3086 - val_mse: 17948.3086 - val_mae: 90.3133\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 14689.3994 - mse: 14689.3994 - mae: 79.7768 - val_loss: 17935.6426 - val_mse: 17935.6426 - val_mae: 90.2870\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 14679.3711 - mse: 14679.3711 - mae: 79.7563 - val_loss: 17922.2812 - val_mse: 17922.2812 - val_mae: 90.2592\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 14667.9492 - mse: 14667.9492 - mae: 79.7278 - val_loss: 17909.7090 - val_mse: 17909.7090 - val_mae: 90.2331\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 14657.5439 - mse: 14657.5439 - mae: 79.7046 - val_loss: 17897.0410 - val_mse: 17897.0410 - val_mae: 90.2067\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 14646.8721 - mse: 14646.8721 - mae: 79.6799 - val_loss: 17884.4668 - val_mse: 17884.4668 - val_mae: 90.1806\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14636.3936 - mse: 14636.3936 - mae: 79.6564 - val_loss: 17871.6953 - val_mse: 17871.6953 - val_mae: 90.1539\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14625.7021 - mse: 14625.7021 - mae: 79.6323 - val_loss: 17859.0781 - val_mse: 17859.0781 - val_mae: 90.1276\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14615.2607 - mse: 14615.2607 - mae: 79.6078 - val_loss: 17846.0801 - val_mse: 17846.0801 - val_mae: 90.1005\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14604.4756 - mse: 14604.4756 - mae: 79.5843 - val_loss: 17833.3984 - val_mse: 17833.3984 - val_mae: 90.0740\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 14593.7783 - mse: 14593.7783 - mae: 79.5580 - val_loss: 17820.6230 - val_mse: 17820.6230 - val_mae: 90.0473\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 14583.0947 - mse: 14583.0947 - mae: 79.5328 - val_loss: 17807.9824 - val_mse: 17807.9824 - val_mae: 90.0209\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14572.6650 - mse: 14572.6650 - mae: 79.5095 - val_loss: 17795.1582 - val_mse: 17795.1582 - val_mae: 89.9940\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14562.0967 - mse: 14562.0967 - mae: 79.4847 - val_loss: 17782.2188 - val_mse: 17782.2188 - val_mae: 89.9669\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14551.0742 - mse: 14551.0742 - mae: 79.4600 - val_loss: 17769.6523 - val_mse: 17769.6523 - val_mae: 89.9406\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14540.6240 - mse: 14540.6240 - mae: 79.4372 - val_loss: 17757.0234 - val_mse: 17757.0234 - val_mae: 89.9141\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 14530.4209 - mse: 14530.4209 - mae: 79.4104 - val_loss: 17743.8711 - val_mse: 17743.8711 - val_mae: 89.8864\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14519.1982 - mse: 14519.1982 - mae: 79.3870 - val_loss: 17731.4551 - val_mse: 17731.4551 - val_mae: 89.8603\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14509.0664 - mse: 14509.0664 - mae: 79.3628 - val_loss: 17718.8340 - val_mse: 17718.8340 - val_mae: 89.8338\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14498.4102 - mse: 14498.4102 - mae: 79.3382 - val_loss: 17706.2852 - val_mse: 17706.2852 - val_mae: 89.8074\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14487.9121 - mse: 14487.9121 - mae: 79.3149 - val_loss: 17693.8574 - val_mse: 17693.8574 - val_mae: 89.7812\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14477.5947 - mse: 14477.5947 - mae: 79.2900 - val_loss: 17681.3398 - val_mse: 17681.3398 - val_mae: 89.7548\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 14467.1406 - mse: 14467.1406 - mae: 79.2649 - val_loss: 17668.6289 - val_mse: 17668.6289 - val_mae: 89.7280\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14456.3994 - mse: 14456.3994 - mae: 79.2403 - val_loss: 17656.1660 - val_mse: 17656.1660 - val_mae: 89.7017\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14446.3564 - mse: 14446.3564 - mae: 79.2175 - val_loss: 17643.0781 - val_mse: 17643.0781 - val_mae: 89.6741\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 14435.4639 - mse: 14435.4639 - mae: 79.1907 - val_loss: 17630.3301 - val_mse: 17630.3301 - val_mae: 89.6471\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14425.1807 - mse: 14425.1807 - mae: 79.1692 - val_loss: 17617.4727 - val_mse: 17617.4727 - val_mae: 89.6199\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14414.3340 - mse: 14414.3340 - mae: 79.1433 - val_loss: 17605.2754 - val_mse: 17605.2754 - val_mae: 89.5941\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14404.1943 - mse: 14404.1943 - mae: 79.1196 - val_loss: 17592.8418 - val_mse: 17592.8418 - val_mae: 89.5677\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14393.7295 - mse: 14393.7295 - mae: 79.0952 - val_loss: 17580.6426 - val_mse: 17580.6426 - val_mae: 89.5418\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14383.8193 - mse: 14383.8193 - mae: 79.0727 - val_loss: 17567.8711 - val_mse: 17567.8711 - val_mae: 89.5147\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14373.0752 - mse: 14373.0752 - mae: 79.0488 - val_loss: 17555.8711 - val_mse: 17555.8711 - val_mae: 89.4893\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 14363.1992 - mse: 14363.1992 - mae: 79.0262 - val_loss: 17543.3594 - val_mse: 17543.3594 - val_mae: 89.4626\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 14352.8984 - mse: 14352.8984 - mae: 79.0044 - val_loss: 17530.7812 - val_mse: 17530.7812 - val_mae: 89.4359\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 14342.4580 - mse: 14342.4580 - mae: 78.9790 - val_loss: 17518.4141 - val_mse: 17518.4141 - val_mae: 89.4096\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14332.1553 - mse: 14332.1553 - mae: 78.9555 - val_loss: 17505.9277 - val_mse: 17505.9277 - val_mae: 89.3829\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14321.6504 - mse: 14321.6504 - mae: 78.9339 - val_loss: 17493.7051 - val_mse: 17493.7051 - val_mae: 89.3569\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14311.6240 - mse: 14311.6240 - mae: 78.9103 - val_loss: 17481.2305 - val_mse: 17481.2305 - val_mae: 89.3303\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14301.0557 - mse: 14301.0557 - mae: 78.8848 - val_loss: 17469.0840 - val_mse: 17469.0840 - val_mae: 89.3044\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 14291.1846 - mse: 14291.1846 - mae: 78.8642 - val_loss: 17456.5352 - val_mse: 17456.5352 - val_mae: 89.2776\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14280.5674 - mse: 14280.5674 - mae: 78.8412 - val_loss: 17443.8711 - val_mse: 17443.8711 - val_mae: 89.2504\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 14270.2676 - mse: 14270.2676 - mae: 78.8172 - val_loss: 17431.3398 - val_mse: 17431.3398 - val_mae: 89.2236\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 14259.9541 - mse: 14259.9541 - mae: 78.7948 - val_loss: 17419.1270 - val_mse: 17419.1270 - val_mae: 89.1975\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 14249.7861 - mse: 14249.7861 - mae: 78.7711 - val_loss: 17407.0391 - val_mse: 17407.0391 - val_mae: 89.1715\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 14239.6631 - mse: 14239.6631 - mae: 78.7492 - val_loss: 17394.9883 - val_mse: 17394.9883 - val_mae: 89.1456\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14229.5732 - mse: 14229.5732 - mae: 78.7265 - val_loss: 17382.9492 - val_mse: 17382.9492 - val_mae: 89.1198\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 14220.0537 - mse: 14220.0537 - mae: 78.7059 - val_loss: 17370.3984 - val_mse: 17370.3984 - val_mae: 89.0929\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 14209.7695 - mse: 14209.7695 - mae: 78.6799 - val_loss: 17357.9648 - val_mse: 17357.9648 - val_mae: 89.0661\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 14199.3721 - mse: 14199.3721 - mae: 78.6590 - val_loss: 17345.9395 - val_mse: 17345.9395 - val_mae: 89.0402\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 14189.6299 - mse: 14189.6299 - mae: 78.6374 - val_loss: 17333.6738 - val_mse: 17333.6738 - val_mae: 89.0138\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 14179.2422 - mse: 14179.2422 - mae: 78.6139 - val_loss: 17321.6211 - val_mse: 17321.6211 - val_mae: 88.9877\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 14169.1387 - mse: 14169.1387 - mae: 78.5904 - val_loss: 17309.4316 - val_mse: 17309.4316 - val_mae: 88.9614\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 14159.1914 - mse: 14159.1914 - mae: 78.5702 - val_loss: 17297.2266 - val_mse: 17297.2266 - val_mae: 88.9351\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14148.8096 - mse: 14148.8096 - mae: 78.5457 - val_loss: 17285.2695 - val_mse: 17285.2695 - val_mae: 88.9093\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14138.9570 - mse: 14138.9570 - mae: 78.5240 - val_loss: 17273.0840 - val_mse: 17273.0840 - val_mae: 88.8829\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 14129.1475 - mse: 14129.1475 - mae: 78.5034 - val_loss: 17260.6719 - val_mse: 17260.6719 - val_mae: 88.8561\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14118.4609 - mse: 14118.4609 - mae: 78.4827 - val_loss: 17248.8867 - val_mse: 17248.8867 - val_mae: 88.8306\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14108.8701 - mse: 14108.8701 - mae: 78.4590 - val_loss: 17236.4414 - val_mse: 17236.4414 - val_mae: 88.8036\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14098.7725 - mse: 14098.7725 - mae: 78.4369 - val_loss: 17224.3965 - val_mse: 17224.3965 - val_mae: 88.7775\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.5, model__n_hidden=0, model__n_neurons=5, model__optimizer=adam; total time=   5.4s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 74712.0781 - mse: 74712.0781 - mae: 265.7816 - val_loss: 78951.4844 - val_mse: 78951.4844 - val_mae: 275.0066\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 74671.9922 - mse: 74671.9922 - mae: 265.7094 - val_loss: 78909.8125 - val_mse: 78909.8125 - val_mae: 274.9337\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 74632.2578 - mse: 74632.2578 - mae: 265.6374 - val_loss: 78868.0312 - val_mse: 78868.0312 - val_mae: 274.8605\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 74592.3906 - mse: 74592.3906 - mae: 265.5652 - val_loss: 78826.3984 - val_mse: 78826.3984 - val_mae: 274.7877\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 74552.2969 - mse: 74552.2969 - mae: 265.4931 - val_loss: 78784.9844 - val_mse: 78784.9844 - val_mae: 274.7151\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 74512.4297 - mse: 74512.4297 - mae: 265.4212 - val_loss: 78743.5625 - val_mse: 78743.5625 - val_mae: 274.6425\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 74472.5859 - mse: 74472.5859 - mae: 265.3494 - val_loss: 78702.0312 - val_mse: 78702.0312 - val_mae: 274.5698\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 74432.8906 - mse: 74432.8906 - mae: 265.2775 - val_loss: 78660.2500 - val_mse: 78660.2500 - val_mae: 274.4966\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 74392.6016 - mse: 74392.6016 - mae: 265.2051 - val_loss: 78618.7891 - val_mse: 78618.7891 - val_mae: 274.4239\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 74353.0781 - mse: 74353.0781 - mae: 265.1333 - val_loss: 78577.2188 - val_mse: 78577.2188 - val_mae: 274.3510\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 74313.2656 - mse: 74313.2656 - mae: 265.0613 - val_loss: 78535.6484 - val_mse: 78535.6484 - val_mae: 274.2781\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 74273.3359 - mse: 74273.3359 - mae: 264.9893 - val_loss: 78494.3828 - val_mse: 78494.3828 - val_mae: 274.2056\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 74233.6094 - mse: 74233.6094 - mae: 264.9174 - val_loss: 78452.9688 - val_mse: 78452.9688 - val_mae: 274.1330\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 74194.3516 - mse: 74194.3516 - mae: 264.8458 - val_loss: 78411.2188 - val_mse: 78411.2188 - val_mae: 274.0597\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 74154.3516 - mse: 74154.3516 - mae: 264.7735 - val_loss: 78369.7812 - val_mse: 78369.7812 - val_mae: 273.9869\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 74114.2422 - mse: 74114.2422 - mae: 264.7014 - val_loss: 78328.6641 - val_mse: 78328.6641 - val_mae: 273.9147\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 74075.0625 - mse: 74075.0625 - mae: 264.6300 - val_loss: 78287.2109 - val_mse: 78287.2109 - val_mae: 273.8418\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 74035.5859 - mse: 74035.5859 - mae: 264.5580 - val_loss: 78245.6641 - val_mse: 78245.6641 - val_mae: 273.7688\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 73995.6250 - mse: 73995.6250 - mae: 264.4859 - val_loss: 78204.5312 - val_mse: 78204.5312 - val_mae: 273.6965\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 73955.8359 - mse: 73955.8359 - mae: 264.4142 - val_loss: 78163.4297 - val_mse: 78163.4297 - val_mae: 273.6242\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 73916.3984 - mse: 73916.3984 - mae: 264.3426 - val_loss: 78121.9141 - val_mse: 78121.9141 - val_mae: 273.5512\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 73876.8281 - mse: 73876.8281 - mae: 264.2704 - val_loss: 78080.2422 - val_mse: 78080.2422 - val_mae: 273.4779\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 73836.7656 - mse: 73836.7656 - mae: 264.1980 - val_loss: 78038.8125 - val_mse: 78038.8125 - val_mae: 273.4050\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 73797.0781 - mse: 73797.0781 - mae: 264.1260 - val_loss: 77997.5156 - val_mse: 77997.5156 - val_mae: 273.3323\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 73757.8438 - mse: 73757.8438 - mae: 264.0542 - val_loss: 77956.1797 - val_mse: 77956.1797 - val_mae: 273.2595\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 73718.1797 - mse: 73718.1797 - mae: 263.9822 - val_loss: 77914.8906 - val_mse: 77914.8906 - val_mae: 273.1868\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 73678.3203 - mse: 73678.3203 - mae: 263.9104 - val_loss: 77873.8594 - val_mse: 77873.8594 - val_mae: 273.1145\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 73639.3203 - mse: 73639.3203 - mae: 263.8389 - val_loss: 77832.3828 - val_mse: 77832.3828 - val_mae: 273.0415\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 73599.6797 - mse: 73599.6797 - mae: 263.7668 - val_loss: 77791.1328 - val_mse: 77791.1328 - val_mae: 272.9687\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 73560.3672 - mse: 73560.3672 - mae: 263.6950 - val_loss: 77749.8438 - val_mse: 77749.8438 - val_mae: 272.8960\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 73520.4688 - mse: 73520.4688 - mae: 263.6232 - val_loss: 77709.0859 - val_mse: 77709.0859 - val_mae: 272.8241\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 73481.4141 - mse: 73481.4141 - mae: 263.5520 - val_loss: 77668.0312 - val_mse: 77668.0312 - val_mae: 272.7516\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 73442.1953 - mse: 73442.1953 - mae: 263.4804 - val_loss: 77627.0703 - val_mse: 77627.0703 - val_mae: 272.6794\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 73402.7031 - mse: 73402.7031 - mae: 263.4088 - val_loss: 77586.3047 - val_mse: 77586.3047 - val_mae: 272.6074\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 73363.8828 - mse: 73363.8828 - mae: 263.3377 - val_loss: 77545.1484 - val_mse: 77545.1484 - val_mae: 272.5348\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 73324.1016 - mse: 73324.1016 - mae: 263.2660 - val_loss: 77504.4844 - val_mse: 77504.4844 - val_mae: 272.4630\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 73285.2656 - mse: 73285.2656 - mae: 263.1948 - val_loss: 77463.2188 - val_mse: 77463.2188 - val_mae: 272.3901\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 73245.8047 - mse: 73245.8047 - mae: 263.1228 - val_loss: 77422.1641 - val_mse: 77422.1641 - val_mae: 272.3175\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 73206.7578 - mse: 73206.7578 - mae: 263.0512 - val_loss: 77381.0156 - val_mse: 77381.0156 - val_mae: 272.2448\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 73167.1797 - mse: 73167.1797 - mae: 262.9794 - val_loss: 77340.1016 - val_mse: 77340.1016 - val_mae: 272.1725\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 73127.4922 - mse: 73127.4922 - mae: 262.9078 - val_loss: 77299.4688 - val_mse: 77299.4688 - val_mae: 272.1006\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 73088.5859 - mse: 73088.5859 - mae: 262.8365 - val_loss: 77258.4922 - val_mse: 77258.4922 - val_mae: 272.0282\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 73049.1953 - mse: 73049.1953 - mae: 262.7647 - val_loss: 77217.5391 - val_mse: 77217.5391 - val_mae: 271.9558\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 73010.2578 - mse: 73010.2578 - mae: 262.6932 - val_loss: 77176.2812 - val_mse: 77176.2812 - val_mae: 271.8827\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 72970.3203 - mse: 72970.3203 - mae: 262.6211 - val_loss: 77135.5859 - val_mse: 77135.5859 - val_mae: 271.8107\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 72931.6562 - mse: 72931.6562 - mae: 262.5498 - val_loss: 77094.5000 - val_mse: 77094.5000 - val_mae: 271.7379\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 72892.2891 - mse: 72892.2891 - mae: 262.4779 - val_loss: 77053.4688 - val_mse: 77053.4688 - val_mae: 271.6653\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 72852.9922 - mse: 72852.9922 - mae: 262.4061 - val_loss: 77012.6172 - val_mse: 77012.6172 - val_mae: 271.5929\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 72814.0938 - mse: 72814.0938 - mae: 262.3348 - val_loss: 76971.6719 - val_mse: 76971.6719 - val_mae: 271.5204\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 72774.5703 - mse: 72774.5703 - mae: 262.2629 - val_loss: 76930.8906 - val_mse: 76930.8906 - val_mae: 271.4481\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 72735.2500 - mse: 72735.2500 - mae: 262.1912 - val_loss: 76889.9844 - val_mse: 76889.9844 - val_mae: 271.3755\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 72696.0469 - mse: 72696.0469 - mae: 262.1195 - val_loss: 76848.8594 - val_mse: 76848.8594 - val_mae: 271.3026\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 72656.6094 - mse: 72656.6094 - mae: 262.0473 - val_loss: 76807.9688 - val_mse: 76807.9688 - val_mae: 271.2301\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 72617.1094 - mse: 72617.1094 - mae: 261.9756 - val_loss: 76767.1406 - val_mse: 76767.1406 - val_mae: 271.1577\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 72578.1953 - mse: 72578.1953 - mae: 261.9040 - val_loss: 76726.0703 - val_mse: 76726.0703 - val_mae: 271.0848\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 72539.2734 - mse: 72539.2734 - mae: 261.8321 - val_loss: 76685.0312 - val_mse: 76685.0312 - val_mae: 271.0119\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 72499.8828 - mse: 72499.8828 - mae: 261.7603 - val_loss: 76644.3828 - val_mse: 76644.3828 - val_mae: 270.9397\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 72461.0547 - mse: 72461.0547 - mae: 261.6889 - val_loss: 76603.5469 - val_mse: 76603.5469 - val_mae: 270.8672\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 72421.6172 - mse: 72421.6172 - mae: 261.6171 - val_loss: 76562.8906 - val_mse: 76562.8906 - val_mae: 270.7950\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 72382.8828 - mse: 72382.8828 - mae: 261.5458 - val_loss: 76522.0703 - val_mse: 76522.0703 - val_mae: 270.7224\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 72343.4766 - mse: 72343.4766 - mae: 261.4741 - val_loss: 76481.6562 - val_mse: 76481.6562 - val_mae: 270.6506\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 72304.8438 - mse: 72304.8438 - mae: 261.4030 - val_loss: 76441.0938 - val_mse: 76441.0938 - val_mae: 270.5785\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 72265.7266 - mse: 72265.7266 - mae: 261.3316 - val_loss: 76400.6172 - val_mse: 76400.6172 - val_mae: 270.5065\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 72226.8125 - mse: 72226.8125 - mae: 261.2602 - val_loss: 76360.0234 - val_mse: 76360.0234 - val_mae: 270.4342\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 72188.1094 - mse: 72188.1094 - mae: 261.1888 - val_loss: 76319.0547 - val_mse: 76319.0547 - val_mae: 270.3614\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 72149.2188 - mse: 72149.2188 - mae: 261.1170 - val_loss: 76278.2188 - val_mse: 76278.2188 - val_mae: 270.2887\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 72109.8047 - mse: 72109.8047 - mae: 261.0451 - val_loss: 76237.7969 - val_mse: 76237.7969 - val_mae: 270.2167\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 72071.4219 - mse: 72071.4219 - mae: 260.9740 - val_loss: 76196.9375 - val_mse: 76196.9375 - val_mae: 270.1439\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 72032.2500 - mse: 72032.2500 - mae: 260.9022 - val_loss: 76156.3438 - val_mse: 76156.3438 - val_mae: 270.0716\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 71993.2656 - mse: 71993.2656 - mae: 260.8308 - val_loss: 76115.9375 - val_mse: 76115.9375 - val_mae: 269.9996\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 71954.6328 - mse: 71954.6328 - mae: 260.7595 - val_loss: 76075.4609 - val_mse: 76075.4609 - val_mae: 269.9275\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 71915.7500 - mse: 71915.7500 - mae: 260.6882 - val_loss: 76034.9453 - val_mse: 76034.9453 - val_mae: 269.8552\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 71876.7422 - mse: 71876.7422 - mae: 260.6168 - val_loss: 75994.6875 - val_mse: 75994.6875 - val_mae: 269.7834\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 71837.8203 - mse: 71837.8203 - mae: 260.5455 - val_loss: 75954.4141 - val_mse: 75954.4141 - val_mae: 269.7116\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 71799.4766 - mse: 71799.4766 - mae: 260.4742 - val_loss: 75913.6172 - val_mse: 75913.6172 - val_mae: 269.6388\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 71760.4688 - mse: 71760.4688 - mae: 260.4025 - val_loss: 75873.0859 - val_mse: 75873.0859 - val_mae: 269.5664\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 71721.8594 - mse: 71721.8594 - mae: 260.3312 - val_loss: 75832.5703 - val_mse: 75832.5703 - val_mae: 269.4941\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 71682.6484 - mse: 71682.6484 - mae: 260.2597 - val_loss: 75792.4844 - val_mse: 75792.4844 - val_mae: 269.4225\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 71644.4844 - mse: 71644.4844 - mae: 260.1886 - val_loss: 75751.6406 - val_mse: 75751.6406 - val_mae: 269.3496\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 71605.5312 - mse: 71605.5312 - mae: 260.1168 - val_loss: 75711.0391 - val_mse: 75711.0391 - val_mae: 269.2771\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 71566.6641 - mse: 71566.6641 - mae: 260.0452 - val_loss: 75670.6953 - val_mse: 75670.6953 - val_mae: 269.2049\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 71527.9062 - mse: 71527.9062 - mae: 259.9739 - val_loss: 75630.4297 - val_mse: 75630.4297 - val_mae: 269.1330\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 71489.3203 - mse: 71489.3203 - mae: 259.9026 - val_loss: 75590.0547 - val_mse: 75590.0547 - val_mae: 269.0608\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 71450.4609 - mse: 71450.4609 - mae: 259.8313 - val_loss: 75550.0391 - val_mse: 75550.0391 - val_mae: 268.9892\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 71412.2188 - mse: 71412.2188 - mae: 259.7606 - val_loss: 75509.6953 - val_mse: 75509.6953 - val_mae: 268.9170\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 71373.9531 - mse: 71373.9531 - mae: 259.6893 - val_loss: 75469.3438 - val_mse: 75469.3438 - val_mae: 268.8448\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 71334.9453 - mse: 71334.9453 - mae: 259.6179 - val_loss: 75429.1953 - val_mse: 75429.1953 - val_mae: 268.7730\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 71296.4453 - mse: 71296.4453 - mae: 259.5467 - val_loss: 75388.9766 - val_mse: 75388.9766 - val_mae: 268.7009\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 71257.8438 - mse: 71257.8438 - mae: 259.4756 - val_loss: 75348.8438 - val_mse: 75348.8438 - val_mae: 268.6291\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 71219.4219 - mse: 71219.4219 - mae: 259.4045 - val_loss: 75308.6172 - val_mse: 75308.6172 - val_mae: 268.5570\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 71181.2734 - mse: 71181.2734 - mae: 259.3333 - val_loss: 75267.9531 - val_mse: 75267.9531 - val_mae: 268.4842\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 71141.7188 - mse: 71141.7188 - mae: 259.2613 - val_loss: 75228.2109 - val_mse: 75228.2109 - val_mae: 268.4129\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 71103.6953 - mse: 71103.6953 - mae: 259.1907 - val_loss: 75188.0938 - val_mse: 75188.0938 - val_mae: 268.3410\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 71065.4141 - mse: 71065.4141 - mae: 259.1195 - val_loss: 75147.7656 - val_mse: 75147.7656 - val_mae: 268.2686\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 71026.5625 - mse: 71026.5625 - mae: 259.0481 - val_loss: 75107.8359 - val_mse: 75107.8359 - val_mae: 268.1970\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 70988.4688 - mse: 70988.4688 - mae: 258.9773 - val_loss: 75067.7266 - val_mse: 75067.7266 - val_mae: 268.1251\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 70949.6562 - mse: 70949.6562 - mae: 258.9059 - val_loss: 75027.8203 - val_mse: 75027.8203 - val_mae: 268.0534\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 70911.7344 - mse: 70911.7344 - mae: 258.8351 - val_loss: 74987.3672 - val_mse: 74987.3672 - val_mae: 267.9808\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 70872.7344 - mse: 70872.7344 - mae: 258.7633 - val_loss: 74947.2344 - val_mse: 74947.2344 - val_mae: 267.9087\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 70834.6562 - mse: 70834.6562 - mae: 258.6921 - val_loss: 74906.9141 - val_mse: 74906.9141 - val_mae: 267.8363\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.5, model__n_hidden=0, model__n_neurons=5, model__optimizer=adam; total time=   5.0s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 83205.8828 - mse: 83205.8828 - mae: 284.4609 - val_loss: 88116.2031 - val_mse: 88116.2031 - val_mae: 293.3722\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 83163.3750 - mse: 83163.3750 - mae: 284.3888 - val_loss: 88071.4062 - val_mse: 88071.4062 - val_mae: 293.2980\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 83121.9062 - mse: 83121.9062 - mae: 284.3174 - val_loss: 88026.2891 - val_mse: 88026.2891 - val_mae: 293.2232\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 83079.6719 - mse: 83079.6719 - mae: 284.2454 - val_loss: 87981.3438 - val_mse: 87981.3438 - val_mae: 293.1488\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 83037.7891 - mse: 83037.7891 - mae: 284.1737 - val_loss: 87936.3594 - val_mse: 87936.3594 - val_mae: 293.0742\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 82995.4844 - mse: 82995.4844 - mae: 284.1016 - val_loss: 87891.6094 - val_mse: 87891.6094 - val_mae: 293.0000\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 82953.6250 - mse: 82953.6250 - mae: 284.0300 - val_loss: 87846.8516 - val_mse: 87846.8516 - val_mae: 292.9258\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 82911.7812 - mse: 82911.7812 - mae: 283.9583 - val_loss: 87802.0469 - val_mse: 87802.0469 - val_mae: 292.8514\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 82869.7500 - mse: 82869.7500 - mae: 283.8865 - val_loss: 87757.3203 - val_mse: 87757.3203 - val_mae: 292.7772\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 82827.9844 - mse: 82827.9844 - mae: 283.8149 - val_loss: 87712.4375 - val_mse: 87712.4375 - val_mae: 292.7027\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 82786.0859 - mse: 82786.0859 - mae: 283.7431 - val_loss: 87667.5547 - val_mse: 87667.5547 - val_mae: 292.6282\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 82744.1641 - mse: 82744.1641 - mae: 283.6713 - val_loss: 87622.8516 - val_mse: 87622.8516 - val_mae: 292.5540\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 82702.8047 - mse: 82702.8047 - mae: 283.5999 - val_loss: 87577.7969 - val_mse: 87577.7969 - val_mae: 292.4791\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 82660.5703 - mse: 82660.5703 - mae: 283.5277 - val_loss: 87533.0703 - val_mse: 87533.0703 - val_mae: 292.4048\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 82618.7422 - mse: 82618.7422 - mae: 283.4561 - val_loss: 87488.4297 - val_mse: 87488.4297 - val_mae: 292.3306\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 82576.7578 - mse: 82576.7578 - mae: 283.3845 - val_loss: 87444.0547 - val_mse: 87444.0547 - val_mae: 292.2568\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 82534.8828 - mse: 82534.8828 - mae: 283.3130 - val_loss: 87399.6094 - val_mse: 87399.6094 - val_mae: 292.1829\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 82493.6562 - mse: 82493.6562 - mae: 283.2418 - val_loss: 87354.6797 - val_mse: 87354.6797 - val_mae: 292.1082\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 82451.9141 - mse: 82451.9141 - mae: 283.1699 - val_loss: 87309.7812 - val_mse: 87309.7812 - val_mae: 292.0335\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 82409.7578 - mse: 82409.7578 - mae: 283.0979 - val_loss: 87265.2188 - val_mse: 87265.2188 - val_mae: 291.9594\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 82368.3594 - mse: 82368.3594 - mae: 283.0265 - val_loss: 87220.4922 - val_mse: 87220.4922 - val_mae: 291.8849\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 82326.4688 - mse: 82326.4688 - mae: 282.9548 - val_loss: 87176.0156 - val_mse: 87176.0156 - val_mae: 291.8108\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 82284.7031 - mse: 82284.7031 - mae: 282.8833 - val_loss: 87131.6875 - val_mse: 87131.6875 - val_mae: 291.7370\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 82243.4141 - mse: 82243.4141 - mae: 282.8120 - val_loss: 87087.1094 - val_mse: 87087.1094 - val_mae: 291.6628\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 82201.5234 - mse: 82201.5234 - mae: 282.7404 - val_loss: 87042.8203 - val_mse: 87042.8203 - val_mae: 291.5890\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 82159.8125 - mse: 82159.8125 - mae: 282.6691 - val_loss: 86998.6641 - val_mse: 86998.6641 - val_mae: 291.5153\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 82118.5312 - mse: 82118.5312 - mae: 282.5979 - val_loss: 86954.1953 - val_mse: 86954.1953 - val_mae: 291.4412\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 82077.5000 - mse: 82077.5000 - mae: 282.5266 - val_loss: 86909.3672 - val_mse: 86909.3672 - val_mae: 291.3665\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 82035.3906 - mse: 82035.3906 - mae: 282.4546 - val_loss: 86865.2031 - val_mse: 86865.2031 - val_mae: 291.2928\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 81993.6641 - mse: 81993.6641 - mae: 282.3834 - val_loss: 86821.1016 - val_mse: 86821.1016 - val_mae: 291.2192\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 81952.7656 - mse: 81952.7656 - mae: 282.3124 - val_loss: 86776.4141 - val_mse: 86776.4141 - val_mae: 291.1446\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 81911.2656 - mse: 81911.2656 - mae: 282.2407 - val_loss: 86731.9141 - val_mse: 86731.9141 - val_mae: 291.0703\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 81869.5547 - mse: 81869.5547 - mae: 282.1690 - val_loss: 86687.6484 - val_mse: 86687.6484 - val_mae: 290.9964\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 81827.9688 - mse: 81827.9688 - mae: 282.0977 - val_loss: 86643.4453 - val_mse: 86643.4453 - val_mae: 290.9225\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 81786.7266 - mse: 81786.7266 - mae: 282.0266 - val_loss: 86599.2031 - val_mse: 86599.2031 - val_mae: 290.8487\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 81745.2266 - mse: 81745.2266 - mae: 281.9550 - val_loss: 86555.0000 - val_mse: 86555.0000 - val_mae: 290.7747\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 81703.4375 - mse: 81703.4375 - mae: 281.8834 - val_loss: 86510.8984 - val_mse: 86510.8984 - val_mae: 290.7010\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 81662.3750 - mse: 81662.3750 - mae: 281.8123 - val_loss: 86466.3125 - val_mse: 86466.3125 - val_mae: 290.6265\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 81621.1875 - mse: 81621.1875 - mae: 281.7407 - val_loss: 86421.5156 - val_mse: 86421.5156 - val_mae: 290.5516\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 81579.2969 - mse: 81579.2969 - mae: 281.6687 - val_loss: 86377.2188 - val_mse: 86377.2188 - val_mae: 290.4776\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 81537.6250 - mse: 81537.6250 - mae: 281.5972 - val_loss: 86333.2812 - val_mse: 86333.2812 - val_mae: 290.4040\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 81496.2578 - mse: 81496.2578 - mae: 281.5261 - val_loss: 86289.2812 - val_mse: 86289.2812 - val_mae: 290.3304\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 81455.2109 - mse: 81455.2109 - mae: 281.4550 - val_loss: 86244.9453 - val_mse: 86244.9453 - val_mae: 290.2562\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 81413.6562 - mse: 81413.6562 - mae: 281.3834 - val_loss: 86200.7578 - val_mse: 86200.7578 - val_mae: 290.1822\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 81372.4688 - mse: 81372.4688 - mae: 281.3120 - val_loss: 86156.4141 - val_mse: 86156.4141 - val_mae: 290.1079\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 81330.7188 - mse: 81330.7188 - mae: 281.2403 - val_loss: 86112.4375 - val_mse: 86112.4375 - val_mae: 290.0342\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 81289.6875 - mse: 81289.6875 - mae: 281.1692 - val_loss: 86068.1328 - val_mse: 86068.1328 - val_mae: 289.9600\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 81248.3984 - mse: 81248.3984 - mae: 281.0977 - val_loss: 86023.8047 - val_mse: 86023.8047 - val_mae: 289.8857\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 81207.5078 - mse: 81207.5078 - mae: 281.0263 - val_loss: 85979.2656 - val_mse: 85979.2656 - val_mae: 289.8110\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 81165.3828 - mse: 81165.3828 - mae: 280.9542 - val_loss: 85935.4609 - val_mse: 85935.4609 - val_mae: 289.7375\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 81124.3984 - mse: 81124.3984 - mae: 280.8832 - val_loss: 85891.3906 - val_mse: 85891.3906 - val_mae: 289.6636\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 81082.8672 - mse: 81082.8672 - mae: 280.8117 - val_loss: 85847.3125 - val_mse: 85847.3125 - val_mae: 289.5897\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 81042.1953 - mse: 81042.1953 - mae: 280.7406 - val_loss: 85802.8047 - val_mse: 85802.8047 - val_mae: 289.5150\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 81000.6562 - mse: 81000.6562 - mae: 280.6689 - val_loss: 85758.8828 - val_mse: 85758.8828 - val_mae: 289.4412\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 80959.2812 - mse: 80959.2812 - mae: 280.5977 - val_loss: 85715.3125 - val_mse: 85715.3125 - val_mae: 289.3681\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 80918.5625 - mse: 80918.5625 - mae: 280.5269 - val_loss: 85671.3047 - val_mse: 85671.3047 - val_mae: 289.2942\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 80877.4062 - mse: 80877.4062 - mae: 280.4556 - val_loss: 85627.2344 - val_mse: 85627.2344 - val_mae: 289.2202\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 80836.2109 - mse: 80836.2109 - mae: 280.3842 - val_loss: 85583.2031 - val_mse: 85583.2031 - val_mae: 289.1461\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 80794.9141 - mse: 80794.9141 - mae: 280.3129 - val_loss: 85539.4453 - val_mse: 85539.4453 - val_mae: 289.0726\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 80754.0234 - mse: 80754.0234 - mae: 280.2419 - val_loss: 85495.5156 - val_mse: 85495.5156 - val_mae: 288.9987\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 80713.2656 - mse: 80713.2656 - mae: 280.1709 - val_loss: 85451.3906 - val_mse: 85451.3906 - val_mae: 288.9245\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 80671.8828 - mse: 80671.8828 - mae: 280.0992 - val_loss: 85407.6875 - val_mse: 85407.6875 - val_mae: 288.8510\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 80630.7969 - mse: 80630.7969 - mae: 280.0281 - val_loss: 85364.0703 - val_mse: 85364.0703 - val_mae: 288.7776\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 80590.3359 - mse: 80590.3359 - mae: 279.9574 - val_loss: 85319.8359 - val_mse: 85319.8359 - val_mae: 288.7031\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 80548.3984 - mse: 80548.3984 - mae: 279.8855 - val_loss: 85276.3906 - val_mse: 85276.3906 - val_mae: 288.6299\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 80507.9062 - mse: 80507.9062 - mae: 279.8148 - val_loss: 85232.4922 - val_mse: 85232.4922 - val_mae: 288.5560\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 80466.9844 - mse: 80466.9844 - mae: 279.7436 - val_loss: 85188.5859 - val_mse: 85188.5859 - val_mae: 288.4821\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 80425.9141 - mse: 80425.9141 - mae: 279.6723 - val_loss: 85144.7109 - val_mse: 85144.7109 - val_mae: 288.4082\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 80384.7969 - mse: 80384.7969 - mae: 279.6009 - val_loss: 85100.7812 - val_mse: 85100.7812 - val_mae: 288.3341\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 80343.7734 - mse: 80343.7734 - mae: 279.5297 - val_loss: 85057.0078 - val_mse: 85057.0078 - val_mae: 288.2604\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 80303.0781 - mse: 80303.0781 - mae: 279.4586 - val_loss: 85013.0938 - val_mse: 85013.0938 - val_mae: 288.1863\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 80261.8047 - mse: 80261.8047 - mae: 279.3871 - val_loss: 84969.6016 - val_mse: 84969.6016 - val_mae: 288.1130\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 80221.3594 - mse: 80221.3594 - mae: 279.3164 - val_loss: 84925.8125 - val_mse: 84925.8125 - val_mae: 288.0391\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 80180.0078 - mse: 80180.0078 - mae: 279.2450 - val_loss: 84882.2891 - val_mse: 84882.2891 - val_mae: 287.9657\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 80139.3125 - mse: 80139.3125 - mae: 279.1741 - val_loss: 84838.3906 - val_mse: 84838.3906 - val_mae: 287.8915\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 80098.5078 - mse: 80098.5078 - mae: 279.1026 - val_loss: 84794.4922 - val_mse: 84794.4922 - val_mae: 287.8174\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 80057.2734 - mse: 80057.2734 - mae: 279.0312 - val_loss: 84750.9375 - val_mse: 84750.9375 - val_mae: 287.7439\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 80016.5000 - mse: 80016.5000 - mae: 278.9602 - val_loss: 84707.3359 - val_mse: 84707.3359 - val_mae: 287.6702\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 79975.8125 - mse: 79975.8125 - mae: 278.8892 - val_loss: 84663.7422 - val_mse: 84663.7422 - val_mae: 287.5966\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 79935.1484 - mse: 79935.1484 - mae: 278.8181 - val_loss: 84619.8984 - val_mse: 84619.8984 - val_mae: 287.5225\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 79894.3203 - mse: 79894.3203 - mae: 278.7468 - val_loss: 84575.9844 - val_mse: 84575.9844 - val_mae: 287.4483\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 79853.1797 - mse: 79853.1797 - mae: 278.6753 - val_loss: 84532.4453 - val_mse: 84532.4453 - val_mae: 287.3746\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 79812.3516 - mse: 79812.3516 - mae: 278.6043 - val_loss: 84489.0703 - val_mse: 84489.0703 - val_mae: 287.3012\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 79771.6094 - mse: 79771.6094 - mae: 278.5334 - val_loss: 84445.7812 - val_mse: 84445.7812 - val_mae: 287.2280\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 79730.7266 - mse: 79730.7266 - mae: 278.4624 - val_loss: 84402.5391 - val_mse: 84402.5391 - val_mae: 287.1548\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 79690.2500 - mse: 79690.2500 - mae: 278.3916 - val_loss: 84358.9453 - val_mse: 84358.9453 - val_mae: 287.0810\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 79649.9062 - mse: 79649.9062 - mae: 278.3206 - val_loss: 84314.8906 - val_mse: 84314.8906 - val_mae: 287.0064\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 79608.7188 - mse: 79608.7188 - mae: 278.2488 - val_loss: 84271.3125 - val_mse: 84271.3125 - val_mae: 286.9326\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 79567.8203 - mse: 79567.8203 - mae: 278.1777 - val_loss: 84227.9531 - val_mse: 84227.9531 - val_mae: 286.8592\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 79527.2109 - mse: 79527.2109 - mae: 278.1067 - val_loss: 84184.4922 - val_mse: 84184.4922 - val_mae: 286.7856\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 79486.6328 - mse: 79486.6328 - mae: 278.0358 - val_loss: 84140.9453 - val_mse: 84140.9453 - val_mae: 286.7117\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 79446.0469 - mse: 79446.0469 - mae: 277.9647 - val_loss: 84097.5078 - val_mse: 84097.5078 - val_mae: 286.6381\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 79405.5000 - mse: 79405.5000 - mae: 277.8937 - val_loss: 84054.0703 - val_mse: 84054.0703 - val_mae: 286.5644\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 79364.4062 - mse: 79364.4062 - mae: 277.8224 - val_loss: 84010.9922 - val_mse: 84010.9922 - val_mae: 286.4914\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 79324.3750 - mse: 79324.3750 - mae: 277.7517 - val_loss: 83967.1328 - val_mse: 83967.1328 - val_mae: 286.4170\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 79283.3828 - mse: 79283.3828 - mae: 277.6801 - val_loss: 83923.5938 - val_mse: 83923.5938 - val_mae: 286.3430\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 79242.8984 - mse: 79242.8984 - mae: 277.6092 - val_loss: 83880.1484 - val_mse: 83880.1484 - val_mae: 286.2693\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 79201.8984 - mse: 79201.8984 - mae: 277.5379 - val_loss: 83837.0703 - val_mse: 83837.0703 - val_mae: 286.1962\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 79161.8750 - mse: 79161.8750 - mae: 277.4673 - val_loss: 83793.6016 - val_mse: 83793.6016 - val_mae: 286.1223\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 79121.0234 - mse: 79121.0234 - mae: 277.3961 - val_loss: 83750.5859 - val_mse: 83750.5859 - val_mae: 286.0492\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.5, model__n_hidden=0, model__n_neurons=5, model__optimizer=adam; total time=   5.2s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 219248148480.0000 - mse: 219248148480.0000 - mae: 165104.0781 - val_loss: 85220522983424.0000 - val_mse: 85220522983424.0000 - val_mae: 9035524.0000\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 81759860844758780870656.0000 - mse: 81759860844758780870656.0000 - mae: 105371533312.0000 - val_loss: 30699272095809237390721024.0000 - val_mse: 30699272095809237390721024.0000 - val_mae: 5434243547136.0000\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 29887749776552121647085842824429568.0000 - mse: 29887749776552121647085842824429568.0000 - mae: 61840979237273600.0000 - val_loss: 12167594992314669039442165584929751040.0000 - val_mse: 12167594992314669039442165584929751040.0000 - val_mae: 3409410735384559616.0000\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: inf - mse: inf - mae: 40020058145525541634048.0000 - val_loss: inf - val_mse: inf - val_mae: 1889995031141109936947200.0000\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: inf - mse: inf - mae: 22672910259574910293689499648.0000 - val_loss: inf - val_mse: inf - val_mae: 1145066051024352084581760892928.0000\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: inf - mse: inf - mae: 12775239148055538804171886605369344.0000 - val_loss: inf - val_mse: inf - val_mae: 683312478071124434238680650771070976.0000\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.1, model__n_hidden=0, model__n_neurons=25, model__optimizer=sgd; total time=   1.0s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 19ms/step - loss: 9284910841856.0000 - mse: 9284910841856.0000 - mae: 1123313.8750 - val_loss: 3010331035041792.0000 - val_mse: 3010331035041792.0000 - val_mae: 53877468.0000\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1949353483036169605218304.0000 - mse: 1949353483036169605218304.0000 - mae: 508005613568.0000 - val_loss: 717036379126415971235921920.0000 - val_mse: 717036379126415971235921920.0000 - val_mae: 26294462971904.0000\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 461642220561014643487156651356585984.0000 - mse: 461642220561014643487156651356585984.0000 - mae: 248333325427539968.0000 - val_loss: inf - val_mse: inf - val_mae: 11867877821563011072.0000\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: inf - mse: inf - mae: 115561285574416358440960.0000 - val_loss: inf - val_mse: inf - val_mae: 5245721941288620419710976.0000\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: inf - mse: inf - mae: 50549901439837549700388487168.0000 - val_loss: inf - val_mse: inf - val_mae: 2417108603197377707955848216576.0000\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: inf - mse: inf - mae: 21380219782250102793248894244356096.0000 - val_loss: inf - val_mse: inf - val_mae: 1084351492561728000613783663636643840.0000\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.1, model__n_hidden=0, model__n_neurons=25, model__optimizer=sgd; total time=   1.0s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 22ms/step - loss: 9205408858112.0000 - mse: 9205408858112.0000 - mae: 1146715.3750 - val_loss: 2604602386022400.0000 - val_mse: 2604602386022400.0000 - val_mae: 49916996.0000\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1308850823455597284818944.0000 - mse: 1308850823455597284818944.0000 - mae: 411122991104.0000 - val_loss: 454762268986868347638382592.0000 - val_mse: 454762268986868347638382592.0000 - val_mae: 20835196731392.0000\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 241504493205458896383788970959437824.0000 - mse: 241504493205458896383788970959437824.0000 - mae: 187071629904314368.0000 - val_loss: inf - val_mse: inf - val_mae: 8511602085102354432.0000\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: inf - mse: inf - mae: 76087793286867754942464.0000 - val_loss: inf - val_mse: inf - val_mae: 3588088583683419408433152.0000\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: inf - mse: inf - mae: 33792617031939993747104727040.0000 - val_loss: inf - val_mse: inf - val_mae: 1498617389222878823610786512896.0000\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: inf - mse: inf - mae: 13595280340639117547391664070328320.0000 - val_loss: inf - val_mse: inf - val_mae: 601992850522808546661342327234101248.0000\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.1, model__n_hidden=0, model__n_neurons=25, model__optimizer=sgd; total time=   1.2s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 18ms/step - loss: 5230433927168.0000 - mse: 5230433927168.0000 - mae: 806404.1250 - val_loss: 2033038473035776.0000 - val_mse: 2033038473035776.0000 - val_mae: 44132040.0000\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1950479310885418191290368.0000 - mse: 1950479310885418191290368.0000 - mae: 514664103936.0000 - val_loss: 732368058421886338289631232.0000 - val_mse: 732368058421886338289631232.0000 - val_mae: 26542373601280.0000\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 713007510294120765026091297532805120.0000 - mse: 713007510294120765026091297532805120.0000 - mae: 302048591613001728.0000 - val_loss: inf - val_mse: inf - val_mae: 16652511521481424896.0000\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: inf - mse: inf - mae: 195469050079618277572608.0000 - val_loss: inf - val_mse: inf - val_mae: 9231260902250048158433280.0000\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: inf - mse: inf - mae: 110740835175374315240411889664.0000 - val_loss: inf - val_mse: inf - val_mae: 5592822087211848970584639668224.0000\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: inf - mse: inf - mae: 62397798227761521481363902913576960.0000 - val_loss: inf - val_mse: inf - val_mae: inf\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.1, model__n_hidden=0, model__n_neurons=125, model__optimizer=nesterov; total time=   1.0s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 19ms/step - loss: 89362222022656.0000 - mse: 89362222022656.0000 - mae: 3484886.2500 - val_loss: 28972790669377536.0000 - val_mse: 28972790669377536.0000 - val_mae: 167145680.0000\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 18761467433115624096661504.0000 - mse: 18761467433115624096661504.0000 - mae: 1576001339392.0000 - val_loss: 6901082796068677957837651968.0000 - val_mse: 6901082796068677957837651968.0000 - val_mae: 81574112526336.0000\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: inf - mse: inf - mae: 770411923383517184.0000 - val_loss: inf - val_mse: inf - val_mae: 36818081440962969600.0000\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: inf - mse: inf - mae: 358509336567204485791744.0000 - val_loss: inf - val_mse: inf - val_mae: 16273965499950056907735040.0000\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: inf - mse: inf - mae: 156822575120115092728957632512.0000 - val_loss: inf - val_mse: inf - val_mae: 7498673694558288223127543480320.0000\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: inf - mse: inf - mae: 66328540102821560920871299325100032.0000 - val_loss: inf - val_mse: inf - val_mae: inf\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.1, model__n_hidden=0, model__n_neurons=125, model__optimizer=nesterov; total time=   1.0s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 18ms/step - loss: 128682194632704.0000 - mse: 128682194632704.0000 - mae: 4287393.0000 - val_loss: 36409700488577024.0000 - val_mse: 36409700488577024.0000 - val_mae: 186632048.0000\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 18296386968607754274471936.0000 - mse: 18296386968607754274471936.0000 - mae: 1537125777408.0000 - val_loss: 6357108678540440656026796032.0000 - val_mse: 6357108678540440656026796032.0000 - val_mae: 77899608621056.0000\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: inf - mse: inf - mae: 699432469216100352.0000 - val_loss: inf - val_mse: inf - val_mae: 31823585056162578432.0000\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: inf - mse: inf - mae: 284480643073166050066432.0000 - val_loss: inf - val_mse: inf - val_mae: 13415313923099948933447680.0000\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: inf - mse: inf - mae: 126345394647169873666677669888.0000 - val_loss: inf - val_mse: inf - val_mae: 5603099165604392933198816870400.0000\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: inf - mse: inf - mae: 50830689522845370995071569620893696.0000 - val_loss: inf - val_mse: inf - val_mae: inf\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.1, model__n_hidden=0, model__n_neurons=125, model__optimizer=nesterov; total time=   1.0s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 19ms/step - loss: 1403.9071 - mse: 1403.9071 - mae: 27.7483 - val_loss: 38.4349 - val_mse: 38.4349 - val_mae: 5.1701\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 111.5846 - mse: 111.5846 - mae: 7.9720 - val_loss: 26.8164 - val_mse: 26.8164 - val_mae: 3.8682\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 92.1285 - mse: 92.1285 - mae: 6.9739 - val_loss: 20.8890 - val_mse: 20.8890 - val_mae: 3.5218\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 84.9688 - mse: 84.9688 - mae: 6.7216 - val_loss: 20.0273 - val_mse: 20.0273 - val_mae: 3.5552\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 85.2958 - mse: 85.2958 - mae: 6.7114 - val_loss: 12.6719 - val_mse: 12.6719 - val_mae: 2.5444\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 80.4400 - mse: 80.4400 - mae: 6.3944 - val_loss: 10.8884 - val_mse: 10.8884 - val_mae: 2.3317\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 72.4568 - mse: 72.4568 - mae: 5.9246 - val_loss: 11.7588 - val_mse: 11.7588 - val_mae: 2.6033\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 73.9827 - mse: 73.9827 - mae: 6.0364 - val_loss: 43.6821 - val_mse: 43.6821 - val_mae: 5.9585\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 71.6545 - mse: 71.6545 - mae: 5.9883 - val_loss: 36.4525 - val_mse: 36.4525 - val_mae: 5.3034\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 70.3079 - mse: 70.3079 - mae: 5.9446 - val_loss: 33.3119 - val_mse: 33.3119 - val_mae: 5.0408\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 71.2803 - mse: 71.2803 - mae: 5.9819 - val_loss: 11.7535 - val_mse: 11.7535 - val_mae: 2.6148\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 67.9337 - mse: 67.9337 - mae: 5.7825 - val_loss: 10.6687 - val_mse: 10.6687 - val_mae: 2.5786\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 65.4636 - mse: 65.4636 - mae: 5.6671 - val_loss: 40.1686 - val_mse: 40.1686 - val_mae: 5.6901\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 74.0125 - mse: 74.0125 - mae: 6.2271 - val_loss: 11.7000 - val_mse: 11.7000 - val_mae: 2.6432\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 63.8788 - mse: 63.8788 - mae: 5.7112 - val_loss: 10.4464 - val_mse: 10.4464 - val_mae: 2.4809\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 64.0787 - mse: 64.0787 - mae: 5.7309 - val_loss: 15.5086 - val_mse: 15.5086 - val_mae: 3.1258\n",
      "Epoch 16: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.5, model__n_hidden=1, model__n_neurons=125, model__optimizer=momentum; total time=   1.2s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 349.6347 - mse: 349.6347 - mae: 14.2732 - val_loss: 75.4748 - val_mse: 75.4748 - val_mae: 7.3339\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 126.9568 - mse: 126.9568 - mae: 8.7291 - val_loss: 44.7519 - val_mse: 44.7519 - val_mae: 5.3200\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 128.4337 - mse: 128.4337 - mae: 8.4934 - val_loss: 61.9767 - val_mse: 61.9767 - val_mae: 6.6698\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 114.3869 - mse: 114.3869 - mae: 8.1265 - val_loss: 37.6076 - val_mse: 37.6076 - val_mae: 4.8086\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 112.6826 - mse: 112.6826 - mae: 8.0476 - val_loss: 33.5668 - val_mse: 33.5668 - val_mae: 4.6019\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 110.9765 - mse: 110.9765 - mae: 7.8345 - val_loss: 27.1965 - val_mse: 27.1965 - val_mae: 4.1962\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 104.9599 - mse: 104.9599 - mae: 7.5586 - val_loss: 28.5371 - val_mse: 28.5371 - val_mae: 4.3138\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 108.1199 - mse: 108.1199 - mae: 7.6015 - val_loss: 39.1585 - val_mse: 39.1585 - val_mae: 5.0679\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 99.1630 - mse: 99.1630 - mae: 7.4765 - val_loss: 30.8901 - val_mse: 30.8901 - val_mae: 4.4843\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 94.9384 - mse: 94.9384 - mae: 7.1300 - val_loss: 50.4365 - val_mse: 50.4365 - val_mae: 6.0058\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 95.3920 - mse: 95.3920 - mae: 7.4511 - val_loss: 21.3467 - val_mse: 21.3467 - val_mae: 3.7500\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 93.5290 - mse: 93.5290 - mae: 7.1131 - val_loss: 33.5194 - val_mse: 33.5194 - val_mae: 4.7840\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 90.6007 - mse: 90.6007 - mae: 7.0242 - val_loss: 29.3072 - val_mse: 29.3072 - val_mae: 4.4759\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 90.6676 - mse: 90.6676 - mae: 7.0832 - val_loss: 24.8660 - val_mse: 24.8660 - val_mae: 4.0787\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 88.6682 - mse: 88.6682 - mae: 6.9922 - val_loss: 18.5663 - val_mse: 18.5663 - val_mae: 3.4886\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 86.6795 - mse: 86.6795 - mae: 6.7927 - val_loss: 19.6113 - val_mse: 19.6113 - val_mae: 3.5892\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 88.1143 - mse: 88.1143 - mae: 6.8189 - val_loss: 29.4235 - val_mse: 29.4235 - val_mae: 4.5228\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 87.2858 - mse: 87.2858 - mae: 6.9047 - val_loss: 19.4038 - val_mse: 19.4038 - val_mae: 3.6650\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 83.9278 - mse: 83.9278 - mae: 6.7041 - val_loss: 19.0760 - val_mse: 19.0760 - val_mae: 3.6091\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 83.8442 - mse: 83.8442 - mae: 6.6350 - val_loss: 26.5749 - val_mse: 26.5749 - val_mae: 4.3055\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 80.4790 - mse: 80.4790 - mae: 6.5207 - val_loss: 20.5476 - val_mse: 20.5476 - val_mae: 3.8086\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 83.9689 - mse: 83.9689 - mae: 6.7678 - val_loss: 17.8910 - val_mse: 17.8910 - val_mae: 3.5504\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 80.5868 - mse: 80.5868 - mae: 6.4633 - val_loss: 17.1169 - val_mse: 17.1169 - val_mae: 3.4656\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 85.5543 - mse: 85.5543 - mae: 6.5627 - val_loss: 37.8797 - val_mse: 37.8797 - val_mae: 5.1937\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 80.4358 - mse: 80.4358 - mae: 6.6365 - val_loss: 27.0558 - val_mse: 27.0558 - val_mae: 4.3996\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 85.3267 - mse: 85.3267 - mae: 6.8546 - val_loss: 23.8160 - val_mse: 23.8160 - val_mae: 4.1008\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 82.1725 - mse: 82.1725 - mae: 6.6218 - val_loss: 14.6810 - val_mse: 14.6810 - val_mae: 3.0973\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 84.0058 - mse: 84.0058 - mae: 6.6319 - val_loss: 15.2144 - val_mse: 15.2144 - val_mae: 3.0670\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 80.0870 - mse: 80.0870 - mae: 6.4095 - val_loss: 15.4171 - val_mse: 15.4171 - val_mae: 3.1757\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 78.6771 - mse: 78.6771 - mae: 6.4616 - val_loss: 15.2959 - val_mse: 15.2959 - val_mae: 3.2019\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 81.3962 - mse: 81.3962 - mae: 6.5956 - val_loss: 15.4821 - val_mse: 15.4821 - val_mae: 3.1414\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 81.2479 - mse: 81.2479 - mae: 6.1937 - val_loss: 48.0304 - val_mse: 48.0304 - val_mae: 5.8717\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 79.4444 - mse: 79.4444 - mae: 6.4888 - val_loss: 18.5457 - val_mse: 18.5457 - val_mae: 3.5916\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 76.4774 - mse: 76.4774 - mae: 6.4023 - val_loss: 34.9099 - val_mse: 34.9099 - val_mae: 5.0194\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 77.5147 - mse: 77.5147 - mae: 6.5139 - val_loss: 27.7164 - val_mse: 27.7164 - val_mae: 4.4600\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 77.2861 - mse: 77.2861 - mae: 6.3632 - val_loss: 23.4048 - val_mse: 23.4048 - val_mae: 4.0817\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 80.6892 - mse: 80.6892 - mae: 6.6526 - val_loss: 15.9062 - val_mse: 15.9062 - val_mae: 3.2789\n",
      "Epoch 37: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.5, model__n_hidden=1, model__n_neurons=125, model__optimizer=momentum; total time=   2.2s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 3162.6582 - mse: 3162.6582 - mae: 36.0711 - val_loss: 95.0373 - val_mse: 95.0373 - val_mae: 7.6646\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 114.1538 - mse: 114.1538 - mae: 8.1924 - val_loss: 117.3809 - val_mse: 117.3809 - val_mae: 8.2459\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 113.0588 - mse: 113.0588 - mae: 7.9813 - val_loss: 78.5515 - val_mse: 78.5515 - val_mae: 6.7015\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 88.2010 - mse: 88.2010 - mae: 6.9794 - val_loss: 69.0395 - val_mse: 69.0395 - val_mae: 6.3309\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 83.9958 - mse: 83.9958 - mae: 6.5547 - val_loss: 66.1345 - val_mse: 66.1345 - val_mae: 6.1745\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 80.5913 - mse: 80.5913 - mae: 6.4847 - val_loss: 62.4231 - val_mse: 62.4231 - val_mae: 5.8709\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 88.3994 - mse: 88.3994 - mae: 6.8624 - val_loss: 60.1462 - val_mse: 60.1462 - val_mae: 5.7045\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 82.4901 - mse: 82.4901 - mae: 6.5874 - val_loss: 68.4084 - val_mse: 68.4084 - val_mae: 6.4313\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 79.7122 - mse: 79.7122 - mae: 6.5854 - val_loss: 58.7977 - val_mse: 58.7977 - val_mae: 5.4900\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 71.0051 - mse: 71.0051 - mae: 5.9670 - val_loss: 55.7475 - val_mse: 55.7475 - val_mae: 5.4583\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 68.8019 - mse: 68.8019 - mae: 5.9705 - val_loss: 73.9023 - val_mse: 73.9023 - val_mae: 6.7747\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 77.1202 - mse: 77.1202 - mae: 6.4927 - val_loss: 80.1077 - val_mse: 80.1077 - val_mae: 7.2339\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 74.4985 - mse: 74.4985 - mae: 6.1675 - val_loss: 52.8928 - val_mse: 52.8928 - val_mae: 5.1848\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 67.5066 - mse: 67.5066 - mae: 5.6697 - val_loss: 52.2951 - val_mse: 52.2951 - val_mae: 5.1322\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 65.0778 - mse: 65.0778 - mae: 5.7283 - val_loss: 52.6244 - val_mse: 52.6244 - val_mae: 5.1967\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 68.2639 - mse: 68.2639 - mae: 5.9127 - val_loss: 68.3021 - val_mse: 68.3021 - val_mae: 6.2545\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 65.6702 - mse: 65.6702 - mae: 5.7849 - val_loss: 55.6846 - val_mse: 55.6846 - val_mae: 5.5780\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 73.0131 - mse: 73.0131 - mae: 6.0097 - val_loss: 49.0267 - val_mse: 49.0267 - val_mae: 4.9377\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 67.1057 - mse: 67.1057 - mae: 5.8529 - val_loss: 49.0480 - val_mse: 49.0480 - val_mae: 4.9198\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 66.1678 - mse: 66.1678 - mae: 5.7507 - val_loss: 48.1792 - val_mse: 48.1792 - val_mae: 4.9627\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 74.2318 - mse: 74.2318 - mae: 6.3127 - val_loss: 55.9777 - val_mse: 55.9777 - val_mae: 5.7931\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 67.3052 - mse: 67.3052 - mae: 5.6380 - val_loss: 56.2912 - val_mse: 56.2912 - val_mae: 5.5791\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 64.3195 - mse: 64.3195 - mae: 6.0541 - val_loss: 49.2334 - val_mse: 49.2334 - val_mae: 5.0871\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 63.9576 - mse: 63.9576 - mae: 5.5898 - val_loss: 56.8785 - val_mse: 56.8785 - val_mae: 5.6240\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 62.2369 - mse: 62.2369 - mae: 5.5751 - val_loss: 45.9886 - val_mse: 45.9886 - val_mae: 4.7931\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 62.3056 - mse: 62.3056 - mae: 5.5698 - val_loss: 50.7709 - val_mse: 50.7709 - val_mae: 5.1534\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 62.2209 - mse: 62.2209 - mae: 5.6817 - val_loss: 50.4971 - val_mse: 50.4971 - val_mae: 5.4630\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 66.5693 - mse: 66.5693 - mae: 5.6691 - val_loss: 49.3858 - val_mse: 49.3858 - val_mae: 5.0786\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 61.9324 - mse: 61.9324 - mae: 5.4620 - val_loss: 44.2641 - val_mse: 44.2641 - val_mae: 4.7610\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 62.5119 - mse: 62.5119 - mae: 5.4433 - val_loss: 61.5929 - val_mse: 61.5929 - val_mae: 5.9927\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 61.3175 - mse: 61.3175 - mae: 5.4603 - val_loss: 43.1194 - val_mse: 43.1194 - val_mae: 4.7508\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 63.4340 - mse: 63.4340 - mae: 5.6319 - val_loss: 49.5445 - val_mse: 49.5445 - val_mae: 5.0928\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 61.5601 - mse: 61.5601 - mae: 5.5954 - val_loss: 44.0950 - val_mse: 44.0950 - val_mae: 4.9218\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 64.3198 - mse: 64.3198 - mae: 5.7100 - val_loss: 42.1058 - val_mse: 42.1058 - val_mae: 4.6571\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 57.7840 - mse: 57.7840 - mae: 5.3191 - val_loss: 42.0170 - val_mse: 42.0170 - val_mae: 4.5869\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 58.7014 - mse: 58.7014 - mae: 5.3013 - val_loss: 45.5596 - val_mse: 45.5596 - val_mae: 5.2281\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 63.5928 - mse: 63.5928 - mae: 5.7260 - val_loss: 46.9270 - val_mse: 46.9270 - val_mae: 5.3586\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 62.5718 - mse: 62.5718 - mae: 5.5757 - val_loss: 53.1319 - val_mse: 53.1319 - val_mae: 5.9736\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 66.0049 - mse: 66.0049 - mae: 6.0430 - val_loss: 60.5358 - val_mse: 60.5358 - val_mae: 5.9936\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 57.8012 - mse: 57.8012 - mae: 5.4899 - val_loss: 40.4881 - val_mse: 40.4881 - val_mae: 4.5231\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 57.8337 - mse: 57.8337 - mae: 5.4327 - val_loss: 45.8198 - val_mse: 45.8198 - val_mae: 5.3708\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 55.3801 - mse: 55.3801 - mae: 5.0877 - val_loss: 125.3661 - val_mse: 125.3661 - val_mae: 9.5538\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 74.0568 - mse: 74.0568 - mae: 6.1622 - val_loss: 39.8749 - val_mse: 39.8749 - val_mae: 4.6129\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 60.4843 - mse: 60.4843 - mae: 5.5831 - val_loss: 50.9441 - val_mse: 50.9441 - val_mae: 5.8690\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 58.3018 - mse: 58.3018 - mae: 5.2830 - val_loss: 40.1806 - val_mse: 40.1806 - val_mae: 4.5540\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 54.6906 - mse: 54.6906 - mae: 5.1865 - val_loss: 50.7924 - val_mse: 50.7924 - val_mae: 5.3368\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 54.5227 - mse: 54.5227 - mae: 5.1346 - val_loss: 43.8854 - val_mse: 43.8854 - val_mae: 5.2072\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 58.0042 - mse: 58.0042 - mae: 5.5165 - val_loss: 55.1609 - val_mse: 55.1609 - val_mae: 6.1926\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 63.0884 - mse: 63.0884 - mae: 5.6377 - val_loss: 44.1690 - val_mse: 44.1690 - val_mae: 4.8013\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 54.3979 - mse: 54.3979 - mae: 5.2145 - val_loss: 63.2427 - val_mse: 63.2427 - val_mae: 6.1804\n",
      "Epoch 50: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.5, model__n_hidden=1, model__n_neurons=125, model__optimizer=momentum; total time=   2.8s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 54372.5820 - mse: 54372.5820 - mae: 211.4483 - val_loss: 59946.3242 - val_mse: 59946.3242 - val_mae: 221.1736\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 54339.7461 - mse: 54339.7461 - mae: 211.3758 - val_loss: 59911.5039 - val_mse: 59911.5039 - val_mae: 221.1008\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 54306.3047 - mse: 54306.3047 - mae: 211.3031 - val_loss: 59876.8281 - val_mse: 59876.8281 - val_mae: 221.0282\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 54273.2852 - mse: 54273.2852 - mae: 211.2305 - val_loss: 59842.0703 - val_mse: 59842.0703 - val_mae: 220.9554\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 54240.7227 - mse: 54240.7227 - mae: 211.1581 - val_loss: 59807.3594 - val_mse: 59807.3594 - val_mae: 220.8828\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 54207.1172 - mse: 54207.1172 - mae: 211.0854 - val_loss: 59772.9531 - val_mse: 59772.9531 - val_mae: 220.8107\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 54174.5312 - mse: 54174.5312 - mae: 211.0133 - val_loss: 59738.2031 - val_mse: 59738.2031 - val_mae: 220.7378\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 54141.3164 - mse: 54141.3164 - mae: 210.9403 - val_loss: 59703.5352 - val_mse: 59703.5352 - val_mae: 220.6652\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 54108.1406 - mse: 54108.1406 - mae: 210.8680 - val_loss: 59668.8398 - val_mse: 59668.8398 - val_mae: 220.5925\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 54075.0586 - mse: 54075.0586 - mae: 210.7953 - val_loss: 59634.3672 - val_mse: 59634.3672 - val_mae: 220.5201\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 54042.6172 - mse: 54042.6172 - mae: 210.7230 - val_loss: 59599.4922 - val_mse: 59599.4922 - val_mae: 220.4470\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 54009.4453 - mse: 54009.4453 - mae: 210.6504 - val_loss: 59565.2656 - val_mse: 59565.2656 - val_mae: 220.3751\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 53976.4102 - mse: 53976.4102 - mae: 210.5784 - val_loss: 59531.0664 - val_mse: 59531.0664 - val_mae: 220.3033\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 53944.4023 - mse: 53944.4023 - mae: 210.5064 - val_loss: 59496.2812 - val_mse: 59496.2812 - val_mae: 220.2303\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 53910.7227 - mse: 53910.7227 - mae: 210.4337 - val_loss: 59462.1680 - val_mse: 59462.1680 - val_mae: 220.1586\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 53877.8867 - mse: 53877.8867 - mae: 210.3615 - val_loss: 59427.7852 - val_mse: 59427.7852 - val_mae: 220.0863\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 53845.7422 - mse: 53845.7422 - mae: 210.2890 - val_loss: 59392.7852 - val_mse: 59392.7852 - val_mae: 220.0127\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 53812.7305 - mse: 53812.7305 - mae: 210.2162 - val_loss: 59358.0820 - val_mse: 59358.0820 - val_mae: 219.9398\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 53779.5508 - mse: 53779.5508 - mae: 210.1435 - val_loss: 59324.0312 - val_mse: 59324.0312 - val_mae: 219.8681\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 53746.8672 - mse: 53746.8672 - mae: 210.0718 - val_loss: 59289.9258 - val_mse: 59289.9258 - val_mae: 219.7964\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 53714.2461 - mse: 53714.2461 - mae: 209.9998 - val_loss: 59255.6211 - val_mse: 59255.6211 - val_mae: 219.7242\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 53681.5664 - mse: 53681.5664 - mae: 209.9277 - val_loss: 59221.3164 - val_mse: 59221.3164 - val_mae: 219.6518\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 53649.3125 - mse: 53649.3125 - mae: 209.8555 - val_loss: 59186.7344 - val_mse: 59186.7344 - val_mae: 219.5790\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 53616.4258 - mse: 53616.4258 - mae: 209.7828 - val_loss: 59152.4062 - val_mse: 59152.4062 - val_mae: 219.5067\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 53583.6914 - mse: 53583.6914 - mae: 209.7108 - val_loss: 59118.3242 - val_mse: 59118.3242 - val_mae: 219.4349\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 53551.0039 - mse: 53551.0039 - mae: 209.6390 - val_loss: 59084.2969 - val_mse: 59084.2969 - val_mae: 219.3631\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 53518.6953 - mse: 53518.6953 - mae: 209.5668 - val_loss: 59049.9414 - val_mse: 59049.9414 - val_mae: 219.2906\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 53486.0664 - mse: 53486.0664 - mae: 209.4948 - val_loss: 59015.8242 - val_mse: 59015.8242 - val_mae: 219.2187\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 53453.9258 - mse: 53453.9258 - mae: 209.4229 - val_loss: 58981.4180 - val_mse: 58981.4180 - val_mae: 219.1460\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 53420.5273 - mse: 53420.5273 - mae: 209.3503 - val_loss: 58947.5664 - val_mse: 58947.5664 - val_mae: 219.0745\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 53388.4375 - mse: 53388.4375 - mae: 209.2786 - val_loss: 58913.2578 - val_mse: 58913.2578 - val_mae: 219.0021\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 53355.8477 - mse: 53355.8477 - mae: 209.2063 - val_loss: 58878.8789 - val_mse: 58878.8789 - val_mae: 218.9295\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 53323.5430 - mse: 53323.5430 - mae: 209.1341 - val_loss: 58844.5078 - val_mse: 58844.5078 - val_mae: 218.8568\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 53290.5430 - mse: 53290.5430 - mae: 209.0617 - val_loss: 58810.8008 - val_mse: 58810.8008 - val_mae: 218.7856\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 53258.4961 - mse: 53258.4961 - mae: 208.9903 - val_loss: 58776.9688 - val_mse: 58776.9688 - val_mae: 218.7140\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 53225.4141 - mse: 53225.4141 - mae: 208.9188 - val_loss: 58743.5195 - val_mse: 58743.5195 - val_mae: 218.6432\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 53194.0234 - mse: 53194.0234 - mae: 208.8472 - val_loss: 58708.6719 - val_mse: 58708.6719 - val_mae: 218.5695\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 53160.8281 - mse: 53160.8281 - mae: 208.7740 - val_loss: 58674.3789 - val_mse: 58674.3789 - val_mae: 218.4970\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 53128.6250 - mse: 53128.6250 - mae: 208.7015 - val_loss: 58639.8984 - val_mse: 58639.8984 - val_mae: 218.4240\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 53095.8008 - mse: 53095.8008 - mae: 208.6291 - val_loss: 58605.8828 - val_mse: 58605.8828 - val_mae: 218.3520\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 53063.1953 - mse: 53063.1953 - mae: 208.5570 - val_loss: 58571.9766 - val_mse: 58571.9766 - val_mae: 218.2801\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 53030.6328 - mse: 53030.6328 - mae: 208.4851 - val_loss: 58538.2969 - val_mse: 58538.2969 - val_mae: 218.2087\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 52998.1602 - mse: 52998.1602 - mae: 208.4135 - val_loss: 58504.5195 - val_mse: 58504.5195 - val_mae: 218.1371\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 52966.6172 - mse: 52966.6172 - mae: 208.3416 - val_loss: 58469.8828 - val_mse: 58469.8828 - val_mae: 218.0638\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 52933.2539 - mse: 52933.2539 - mae: 208.2689 - val_loss: 58436.1836 - val_mse: 58436.1836 - val_mae: 217.9923\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 52901.3750 - mse: 52901.3750 - mae: 208.1974 - val_loss: 58402.2305 - val_mse: 58402.2305 - val_mae: 217.9203\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 52869.1523 - mse: 52869.1523 - mae: 208.1257 - val_loss: 58368.3164 - val_mse: 58368.3164 - val_mae: 217.8482\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 52836.6289 - mse: 52836.6289 - mae: 208.0535 - val_loss: 58334.5586 - val_mse: 58334.5586 - val_mae: 217.7766\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 52804.5039 - mse: 52804.5039 - mae: 207.9820 - val_loss: 58300.8047 - val_mse: 58300.8047 - val_mae: 217.7049\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 52772.4219 - mse: 52772.4219 - mae: 207.9101 - val_loss: 58266.7422 - val_mse: 58266.7422 - val_mae: 217.6326\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 52740.0000 - mse: 52740.0000 - mae: 207.8380 - val_loss: 58232.9336 - val_mse: 58232.9336 - val_mae: 217.5607\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 52707.7695 - mse: 52707.7695 - mae: 207.7661 - val_loss: 58198.9570 - val_mse: 58198.9570 - val_mae: 217.4885\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 52675.3672 - mse: 52675.3672 - mae: 207.6941 - val_loss: 58165.2305 - val_mse: 58165.2305 - val_mae: 217.4168\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 52643.1328 - mse: 52643.1328 - mae: 207.6222 - val_loss: 58131.3711 - val_mse: 58131.3711 - val_mae: 217.3447\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 52611.1172 - mse: 52611.1172 - mae: 207.5501 - val_loss: 58097.1680 - val_mse: 58097.1680 - val_mae: 217.2720\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 52578.1641 - mse: 52578.1641 - mae: 207.4777 - val_loss: 58063.4258 - val_mse: 58063.4258 - val_mae: 217.2002\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 52546.2383 - mse: 52546.2383 - mae: 207.4059 - val_loss: 58029.4922 - val_mse: 58029.4922 - val_mae: 217.1280\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 52514.1016 - mse: 52514.1016 - mae: 207.3334 - val_loss: 57995.2500 - val_mse: 57995.2500 - val_mae: 217.0552\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 52481.1172 - mse: 52481.1172 - mae: 207.2610 - val_loss: 57961.6602 - val_mse: 57961.6602 - val_mae: 216.9836\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 52449.6016 - mse: 52449.6016 - mae: 207.1895 - val_loss: 57927.5039 - val_mse: 57927.5039 - val_mae: 216.9108\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 52416.9336 - mse: 52416.9336 - mae: 207.1168 - val_loss: 57893.8047 - val_mse: 57893.8047 - val_mae: 216.8390\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 52384.6758 - mse: 52384.6758 - mae: 207.0452 - val_loss: 57860.2539 - val_mse: 57860.2539 - val_mae: 216.7674\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 52352.6602 - mse: 52352.6602 - mae: 206.9734 - val_loss: 57826.5430 - val_mse: 57826.5430 - val_mae: 216.6955\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 52320.4844 - mse: 52320.4844 - mae: 206.9013 - val_loss: 57792.6523 - val_mse: 57792.6523 - val_mae: 216.6232\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 52288.1172 - mse: 52288.1172 - mae: 206.8292 - val_loss: 57758.8320 - val_mse: 57758.8320 - val_mae: 216.5510\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 52256.2344 - mse: 52256.2344 - mae: 206.7567 - val_loss: 57724.5078 - val_mse: 57724.5078 - val_mae: 216.4779\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 52223.6914 - mse: 52223.6914 - mae: 206.6839 - val_loss: 57690.4258 - val_mse: 57690.4258 - val_mae: 216.4051\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 52191.4531 - mse: 52191.4531 - mae: 206.6115 - val_loss: 57656.5234 - val_mse: 57656.5234 - val_mae: 216.3327\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 52159.1953 - mse: 52159.1953 - mae: 206.5397 - val_loss: 57623.0469 - val_mse: 57623.0469 - val_mae: 216.2611\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 52127.2148 - mse: 52127.2148 - mae: 206.4678 - val_loss: 57589.4570 - val_mse: 57589.4570 - val_mae: 216.1893\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 52094.9258 - mse: 52094.9258 - mae: 206.3961 - val_loss: 57556.2305 - val_mse: 57556.2305 - val_mae: 216.1182\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 52063.5859 - mse: 52063.5859 - mae: 206.3246 - val_loss: 57522.1172 - val_mse: 57522.1172 - val_mae: 216.0453\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 52030.9766 - mse: 52030.9766 - mae: 206.2524 - val_loss: 57488.9336 - val_mse: 57488.9336 - val_mae: 215.9742\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 51999.3516 - mse: 51999.3516 - mae: 206.1808 - val_loss: 57455.2188 - val_mse: 57455.2188 - val_mae: 215.9021\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 51967.5312 - mse: 51967.5312 - mae: 206.1089 - val_loss: 57421.3711 - val_mse: 57421.3711 - val_mae: 215.8296\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 51935.3047 - mse: 51935.3047 - mae: 206.0366 - val_loss: 57387.8398 - val_mse: 57387.8398 - val_mae: 215.7578\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 51903.0781 - mse: 51903.0781 - mae: 205.9646 - val_loss: 57354.2305 - val_mse: 57354.2305 - val_mae: 215.6858\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 51871.0312 - mse: 51871.0312 - mae: 205.8929 - val_loss: 57320.7344 - val_mse: 57320.7344 - val_mae: 215.6140\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 51839.2461 - mse: 51839.2461 - mae: 205.8209 - val_loss: 57286.9961 - val_mse: 57286.9961 - val_mae: 215.5417\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 51807.0117 - mse: 51807.0117 - mae: 205.7488 - val_loss: 57253.4648 - val_mse: 57253.4648 - val_mae: 215.4698\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 51775.1875 - mse: 51775.1875 - mae: 205.6769 - val_loss: 57219.8203 - val_mse: 57219.8203 - val_mae: 215.3977\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 51743.0586 - mse: 51743.0586 - mae: 205.6045 - val_loss: 57185.9336 - val_mse: 57185.9336 - val_mae: 215.3250\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 51711.1133 - mse: 51711.1133 - mae: 205.5324 - val_loss: 57152.2188 - val_mse: 57152.2188 - val_mae: 215.2527\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 51679.0391 - mse: 51679.0391 - mae: 205.4604 - val_loss: 57118.8477 - val_mse: 57118.8477 - val_mae: 215.1810\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 51647.3008 - mse: 51647.3008 - mae: 205.3889 - val_loss: 57085.5039 - val_mse: 57085.5039 - val_mae: 215.1094\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 51615.3555 - mse: 51615.3555 - mae: 205.3172 - val_loss: 57052.3281 - val_mse: 57052.3281 - val_mae: 215.0381\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 51583.6016 - mse: 51583.6016 - mae: 205.2459 - val_loss: 57019.1562 - val_mse: 57019.1562 - val_mae: 214.9668\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 51552.6445 - mse: 51552.6406 - mae: 205.1745 - val_loss: 56985.3828 - val_mse: 56985.3828 - val_mae: 214.8943\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 51520.7773 - mse: 51520.7773 - mae: 205.1021 - val_loss: 56951.7227 - val_mse: 56951.7227 - val_mae: 214.8220\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 51488.5273 - mse: 51488.5273 - mae: 205.0303 - val_loss: 56918.7070 - val_mse: 56918.7070 - val_mae: 214.7509\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 51457.3906 - mse: 51457.3906 - mae: 204.9590 - val_loss: 56885.1953 - val_mse: 56885.1953 - val_mae: 214.6787\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 51424.9883 - mse: 51424.9883 - mae: 204.8869 - val_loss: 56852.2148 - val_mse: 56852.2148 - val_mae: 214.6078\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 51393.4727 - mse: 51393.4727 - mae: 204.8156 - val_loss: 56818.7734 - val_mse: 56818.7734 - val_mae: 214.5358\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 51361.8125 - mse: 51361.8125 - mae: 204.7439 - val_loss: 56785.4219 - val_mse: 56785.4219 - val_mae: 214.4640\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 51329.9570 - mse: 51329.9570 - mae: 204.6723 - val_loss: 56752.1680 - val_mse: 56752.1680 - val_mae: 214.3923\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 51298.2539 - mse: 51298.2539 - mae: 204.6005 - val_loss: 56718.9102 - val_mse: 56718.9102 - val_mae: 214.3206\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 51266.9375 - mse: 51266.9375 - mae: 204.5288 - val_loss: 56685.2422 - val_mse: 56685.2422 - val_mae: 214.2480\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 51234.5781 - mse: 51234.5781 - mae: 204.4567 - val_loss: 56652.1445 - val_mse: 56652.1445 - val_mae: 214.1766\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 51203.2148 - mse: 51203.2148 - mae: 204.3849 - val_loss: 56618.6055 - val_mse: 56618.6055 - val_mae: 214.1043\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 51171.5898 - mse: 51171.5898 - mae: 204.3132 - val_loss: 56585.4453 - val_mse: 56585.4453 - val_mae: 214.0327\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.5, model__n_hidden=0, model__n_neurons=25, model__optimizer=adam; total time=   5.1s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 19ms/step - loss: 34611.4570 - mse: 34611.4570 - mae: 148.6405 - val_loss: 38918.7969 - val_mse: 38918.7969 - val_mae: 158.4601\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 34587.9219 - mse: 34587.9219 - mae: 148.5687 - val_loss: 38893.3164 - val_mse: 38893.3164 - val_mae: 158.3876\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 34564.6758 - mse: 34564.6758 - mae: 148.4971 - val_loss: 38867.7930 - val_mse: 38867.7930 - val_mae: 158.3150\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 34541.0273 - mse: 34541.0273 - mae: 148.4257 - val_loss: 38842.4336 - val_mse: 38842.4336 - val_mae: 158.2427\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 34517.5508 - mse: 34517.5508 - mae: 148.3540 - val_loss: 38817.1797 - val_mse: 38817.1797 - val_mae: 158.1708\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 34494.2266 - mse: 34494.2266 - mae: 148.2827 - val_loss: 38791.9062 - val_mse: 38791.9062 - val_mae: 158.0987\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 34470.9727 - mse: 34470.9727 - mae: 148.2115 - val_loss: 38766.5742 - val_mse: 38766.5742 - val_mae: 158.0265\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 34447.9375 - mse: 34447.9375 - mae: 148.1397 - val_loss: 38740.8633 - val_mse: 38740.8633 - val_mae: 157.9532\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 34423.8008 - mse: 34423.8008 - mae: 148.0678 - val_loss: 38715.6914 - val_mse: 38715.6914 - val_mae: 157.8813\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 34400.8125 - mse: 34400.8125 - mae: 147.9965 - val_loss: 38690.3516 - val_mse: 38690.3516 - val_mae: 157.8089\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 34377.4609 - mse: 34377.4609 - mae: 147.9249 - val_loss: 38664.9336 - val_mse: 38664.9336 - val_mae: 157.7364\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 34354.0391 - mse: 34354.0391 - mae: 147.8536 - val_loss: 38639.9062 - val_mse: 38639.9062 - val_mae: 157.6648\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 34330.8359 - mse: 34330.8359 - mae: 147.7824 - val_loss: 38614.7422 - val_mse: 38614.7422 - val_mae: 157.5928\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 34308.4297 - mse: 34308.4297 - mae: 147.7109 - val_loss: 38589.0312 - val_mse: 38589.0312 - val_mae: 157.5193\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 34284.7109 - mse: 34284.7109 - mae: 147.6389 - val_loss: 38563.8203 - val_mse: 38563.8203 - val_mae: 157.4472\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 34261.2422 - mse: 34261.2422 - mae: 147.5678 - val_loss: 38539.0469 - val_mse: 38539.0469 - val_mae: 157.3762\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 34238.7227 - mse: 34238.7227 - mae: 147.4971 - val_loss: 38513.8828 - val_mse: 38513.8828 - val_mae: 157.3041\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 34215.4688 - mse: 34215.4688 - mae: 147.4261 - val_loss: 38488.7930 - val_mse: 38488.7930 - val_mae: 157.2321\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 34192.3086 - mse: 34192.3086 - mae: 147.3548 - val_loss: 38463.7852 - val_mse: 38463.7852 - val_mae: 157.1605\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 34169.0703 - mse: 34169.0703 - mae: 147.2840 - val_loss: 38438.8203 - val_mse: 38438.8203 - val_mae: 157.0889\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 34146.3281 - mse: 34146.3281 - mae: 147.2127 - val_loss: 38413.4453 - val_mse: 38413.4453 - val_mae: 157.0162\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 34122.9531 - mse: 34122.9531 - mae: 147.1411 - val_loss: 38388.2617 - val_mse: 38388.2617 - val_mae: 156.9438\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 34099.5977 - mse: 34099.5977 - mae: 147.0697 - val_loss: 38363.2305 - val_mse: 38363.2305 - val_mae: 156.8718\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 34076.7969 - mse: 34076.7969 - mae: 146.9983 - val_loss: 38338.0547 - val_mse: 38338.0547 - val_mae: 156.7995\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 34053.8164 - mse: 34053.8164 - mae: 146.9274 - val_loss: 38313.2031 - val_mse: 38313.2031 - val_mae: 156.7280\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 34030.8555 - mse: 34030.8555 - mae: 146.8565 - val_loss: 38288.1602 - val_mse: 38288.1602 - val_mae: 156.6560\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 34007.4961 - mse: 34007.4961 - mae: 146.7854 - val_loss: 38263.3789 - val_mse: 38263.3789 - val_mae: 156.5847\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33985.2891 - mse: 33985.2891 - mae: 146.7143 - val_loss: 38237.9258 - val_mse: 38237.9258 - val_mae: 156.5117\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33961.7227 - mse: 33961.7227 - mae: 146.6429 - val_loss: 38213.1055 - val_mse: 38213.1055 - val_mae: 156.4401\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33938.9609 - mse: 33938.9609 - mae: 146.5720 - val_loss: 38188.1289 - val_mse: 38188.1289 - val_mae: 156.3682\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 33915.8789 - mse: 33915.8789 - mae: 146.5011 - val_loss: 38163.5312 - val_mse: 38163.5312 - val_mae: 156.2973\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 33893.1875 - mse: 33893.1875 - mae: 146.4311 - val_loss: 38138.9102 - val_mse: 38138.9102 - val_mae: 156.2263\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 33870.5430 - mse: 33870.5430 - mae: 146.3609 - val_loss: 38114.2461 - val_mse: 38114.2461 - val_mae: 156.1551\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33847.5117 - mse: 33847.5117 - mae: 146.2904 - val_loss: 38089.7109 - val_mse: 38089.7109 - val_mae: 156.0844\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33825.5469 - mse: 33825.5469 - mae: 146.2197 - val_loss: 38064.3828 - val_mse: 38064.3828 - val_mae: 156.0115\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 33801.9102 - mse: 33801.9102 - mae: 146.1486 - val_loss: 38040.0039 - val_mse: 38040.0039 - val_mae: 155.9410\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33779.6172 - mse: 33779.6172 - mae: 146.0785 - val_loss: 38015.1172 - val_mse: 38015.1172 - val_mae: 155.8691\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 33756.7227 - mse: 33756.7227 - mae: 146.0074 - val_loss: 37990.3320 - val_mse: 37990.3320 - val_mae: 155.7974\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 33734.1953 - mse: 33734.1953 - mae: 145.9365 - val_loss: 37965.4414 - val_mse: 37965.4414 - val_mae: 155.7255\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33711.0664 - mse: 33711.0664 - mae: 145.8656 - val_loss: 37940.7148 - val_mse: 37940.7148 - val_mae: 155.6540\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33688.1641 - mse: 33688.1641 - mae: 145.7946 - val_loss: 37915.9766 - val_mse: 37915.9766 - val_mae: 155.5826\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 33665.5234 - mse: 33665.5234 - mae: 145.7240 - val_loss: 37891.2539 - val_mse: 37891.2539 - val_mae: 155.5110\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33642.7461 - mse: 33642.7461 - mae: 145.6532 - val_loss: 37866.6289 - val_mse: 37866.6289 - val_mae: 155.4397\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 33620.6055 - mse: 33620.6055 - mae: 145.5824 - val_loss: 37841.5938 - val_mse: 37841.5938 - val_mae: 155.3672\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 33597.1367 - mse: 33597.1367 - mae: 145.5113 - val_loss: 37817.2695 - val_mse: 37817.2695 - val_mae: 155.2966\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 33574.8789 - mse: 33574.8789 - mae: 145.4415 - val_loss: 37792.7344 - val_mse: 37792.7344 - val_mae: 155.2253\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33552.0312 - mse: 33552.0312 - mae: 145.3712 - val_loss: 37768.3281 - val_mse: 37768.3281 - val_mae: 155.1544\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33529.8242 - mse: 33529.8242 - mae: 145.3006 - val_loss: 37743.5039 - val_mse: 37743.5039 - val_mae: 155.0826\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 33507.1250 - mse: 33507.1250 - mae: 145.2299 - val_loss: 37718.8320 - val_mse: 37718.8320 - val_mae: 155.0109\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 33484.3203 - mse: 33484.3203 - mae: 145.1586 - val_loss: 37694.0586 - val_mse: 37694.0586 - val_mae: 154.9391\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 33461.3789 - mse: 33461.3789 - mae: 145.0873 - val_loss: 37669.2109 - val_mse: 37669.2109 - val_mae: 154.8670\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 33438.4102 - mse: 33438.4102 - mae: 145.0161 - val_loss: 37644.2695 - val_mse: 37644.2695 - val_mae: 154.7945\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 33415.5859 - mse: 33415.5859 - mae: 144.9443 - val_loss: 37619.3789 - val_mse: 37619.3789 - val_mae: 154.7223\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33392.5391 - mse: 33392.5391 - mae: 144.8731 - val_loss: 37594.7148 - val_mse: 37594.7148 - val_mae: 154.6506\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 33369.8438 - mse: 33369.8438 - mae: 144.8022 - val_loss: 37569.9297 - val_mse: 37569.9297 - val_mae: 154.5784\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33347.4844 - mse: 33347.4844 - mae: 144.7309 - val_loss: 37545.0508 - val_mse: 37545.0508 - val_mae: 154.5060\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 33324.5859 - mse: 33324.5859 - mae: 144.6599 - val_loss: 37520.6953 - val_mse: 37520.6953 - val_mae: 154.4350\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33302.3281 - mse: 33302.3281 - mae: 144.5894 - val_loss: 37496.0586 - val_mse: 37496.0586 - val_mae: 154.3632\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33279.4766 - mse: 33279.4766 - mae: 144.5185 - val_loss: 37471.5312 - val_mse: 37471.5312 - val_mae: 154.2918\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 33257.0039 - mse: 33257.0039 - mae: 144.4483 - val_loss: 37447.1602 - val_mse: 37447.1602 - val_mae: 154.2205\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33234.4180 - mse: 33234.4180 - mae: 144.3778 - val_loss: 37423.0039 - val_mse: 37423.0039 - val_mae: 154.1500\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 33212.4375 - mse: 33212.4375 - mae: 144.3078 - val_loss: 37398.6445 - val_mse: 37398.6445 - val_mae: 154.0789\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 33189.9141 - mse: 33189.9141 - mae: 144.2374 - val_loss: 37374.3047 - val_mse: 37374.3047 - val_mae: 154.0079\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 33167.2969 - mse: 33167.2969 - mae: 144.1672 - val_loss: 37350.1211 - val_mse: 37350.1211 - val_mae: 153.9371\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33145.0469 - mse: 33145.0469 - mae: 144.0971 - val_loss: 37325.5508 - val_mse: 37325.5508 - val_mae: 153.8653\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 33122.9609 - mse: 33122.9609 - mae: 144.0258 - val_loss: 37300.7266 - val_mse: 37300.7266 - val_mae: 153.7929\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33099.7188 - mse: 33099.7188 - mae: 143.9550 - val_loss: 37276.5938 - val_mse: 37276.5938 - val_mae: 153.7222\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33078.0508 - mse: 33078.0508 - mae: 143.8846 - val_loss: 37251.8789 - val_mse: 37251.8789 - val_mae: 153.6500\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33055.2109 - mse: 33055.2109 - mae: 143.8137 - val_loss: 37227.6484 - val_mse: 37227.6484 - val_mae: 153.5789\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 33032.9023 - mse: 33032.9023 - mae: 143.7435 - val_loss: 37203.4492 - val_mse: 37203.4492 - val_mae: 153.5081\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 33010.9023 - mse: 33010.9023 - mae: 143.6732 - val_loss: 37179.1797 - val_mse: 37179.1797 - val_mae: 153.4369\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 32988.6875 - mse: 32988.6875 - mae: 143.6028 - val_loss: 37154.7969 - val_mse: 37154.7969 - val_mae: 153.3656\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 32965.9609 - mse: 32965.9609 - mae: 143.5329 - val_loss: 37131.0000 - val_mse: 37131.0000 - val_mae: 153.2956\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 32943.7734 - mse: 32943.7734 - mae: 143.4633 - val_loss: 37107.0508 - val_mse: 37107.0508 - val_mae: 153.2254\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 32921.8164 - mse: 32921.8164 - mae: 143.3934 - val_loss: 37082.6562 - val_mse: 37082.6562 - val_mae: 153.1537\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 32899.5508 - mse: 32899.5508 - mae: 143.3225 - val_loss: 37058.3086 - val_mse: 37058.3086 - val_mae: 153.0822\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 32877.6367 - mse: 32877.6367 - mae: 143.2520 - val_loss: 37033.8242 - val_mse: 37033.8242 - val_mae: 153.0105\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 32854.6875 - mse: 32854.6875 - mae: 143.1815 - val_loss: 37010.0039 - val_mse: 37010.0039 - val_mae: 152.9403\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 32833.0742 - mse: 32833.0742 - mae: 143.1114 - val_loss: 36985.4297 - val_mse: 36985.4297 - val_mae: 152.8681\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 32810.5391 - mse: 32810.5391 - mae: 143.0407 - val_loss: 36961.2500 - val_mse: 36961.2500 - val_mae: 152.7969\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 32788.5781 - mse: 32788.5781 - mae: 142.9700 - val_loss: 36937.0156 - val_mse: 36937.0156 - val_mae: 152.7256\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 32766.1445 - mse: 32766.1445 - mae: 142.8997 - val_loss: 36912.9727 - val_mse: 36912.9727 - val_mae: 152.6549\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 32743.9160 - mse: 32743.9160 - mae: 142.8296 - val_loss: 36888.8008 - val_mse: 36888.8008 - val_mae: 152.5837\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 32721.4043 - mse: 32721.4043 - mae: 142.7598 - val_loss: 36865.0234 - val_mse: 36865.0234 - val_mae: 152.5136\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 32699.5547 - mse: 32699.5547 - mae: 142.6902 - val_loss: 36840.8594 - val_mse: 36840.8594 - val_mae: 152.4425\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 32677.7559 - mse: 32677.7559 - mae: 142.6197 - val_loss: 36816.5820 - val_mse: 36816.5820 - val_mae: 152.3709\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 32655.5156 - mse: 32655.5156 - mae: 142.5489 - val_loss: 36792.3359 - val_mse: 36792.3359 - val_mae: 152.2996\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 32633.2305 - mse: 32633.2305 - mae: 142.4786 - val_loss: 36768.3945 - val_mse: 36768.3945 - val_mae: 152.2289\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 32611.5352 - mse: 32611.5352 - mae: 142.4084 - val_loss: 36744.2227 - val_mse: 36744.2227 - val_mae: 152.1577\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 32589.2148 - mse: 32589.2148 - mae: 142.3384 - val_loss: 36720.4258 - val_mse: 36720.4258 - val_mae: 152.0872\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 32567.7266 - mse: 32567.7266 - mae: 142.2684 - val_loss: 36696.0625 - val_mse: 36696.0625 - val_mae: 152.0152\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 32544.6406 - mse: 32544.6406 - mae: 142.1981 - val_loss: 36672.6445 - val_mse: 36672.6445 - val_mae: 151.9460\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 32523.4922 - mse: 32523.4922 - mae: 142.1288 - val_loss: 36648.6680 - val_mse: 36648.6680 - val_mae: 151.8752\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 32501.1191 - mse: 32501.1191 - mae: 142.0595 - val_loss: 36625.0664 - val_mse: 36625.0664 - val_mae: 151.8051\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 32479.0742 - mse: 32479.0742 - mae: 141.9899 - val_loss: 36601.3828 - val_mse: 36601.3828 - val_mae: 151.7349\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 32457.8047 - mse: 32457.8047 - mae: 141.9200 - val_loss: 36577.2031 - val_mse: 36577.2031 - val_mae: 151.6635\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 32435.2617 - mse: 32435.2617 - mae: 141.8498 - val_loss: 36553.5469 - val_mse: 36553.5469 - val_mae: 151.5934\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 32413.6719 - mse: 32413.6719 - mae: 141.7803 - val_loss: 36529.5000 - val_mse: 36529.5000 - val_mae: 151.5220\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 32391.4062 - mse: 32391.4062 - mae: 141.7096 - val_loss: 36505.4727 - val_mse: 36505.4727 - val_mae: 151.4509\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 32369.6895 - mse: 32369.6895 - mae: 141.6395 - val_loss: 36481.5469 - val_mse: 36481.5469 - val_mae: 151.3799\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.5, model__n_hidden=0, model__n_neurons=25, model__optimizer=adam; total time=   5.7s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 16554.8164 - mse: 16554.8164 - mae: 108.7366 - val_loss: 9278.6846 - val_mse: 9278.6846 - val_mae: 85.2255\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16550.6250 - mse: 16550.6250 - mae: 108.7210 - val_loss: 9275.9600 - val_mse: 9275.9600 - val_mae: 85.2128\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16546.2461 - mse: 16546.2461 - mae: 108.7055 - val_loss: 9273.3252 - val_mse: 9273.3252 - val_mae: 85.2005\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 16542.1016 - mse: 16542.1016 - mae: 108.6907 - val_loss: 9270.6436 - val_mse: 9270.6436 - val_mae: 85.1880\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 16537.8438 - mse: 16537.8438 - mae: 108.6761 - val_loss: 9267.9678 - val_mse: 9267.9678 - val_mae: 85.1755\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16533.5254 - mse: 16533.5254 - mae: 108.6608 - val_loss: 9265.2324 - val_mse: 9265.2324 - val_mae: 85.1627\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16529.1094 - mse: 16529.1094 - mae: 108.6445 - val_loss: 9262.5254 - val_mse: 9262.5254 - val_mae: 85.1501\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16524.9316 - mse: 16524.9316 - mae: 108.6296 - val_loss: 9259.8027 - val_mse: 9259.8027 - val_mae: 85.1374\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16520.4863 - mse: 16520.4863 - mae: 108.6140 - val_loss: 9257.0947 - val_mse: 9257.0947 - val_mae: 85.1247\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16516.3145 - mse: 16516.3145 - mae: 108.5987 - val_loss: 9254.3994 - val_mse: 9254.3994 - val_mae: 85.1121\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16511.8457 - mse: 16511.8457 - mae: 108.5830 - val_loss: 9251.7490 - val_mse: 9251.7490 - val_mae: 85.0998\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 16507.6621 - mse: 16507.6621 - mae: 108.5676 - val_loss: 9249.0391 - val_mse: 9249.0391 - val_mae: 85.0871\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16503.3398 - mse: 16503.3398 - mae: 108.5527 - val_loss: 9246.3818 - val_mse: 9246.3818 - val_mae: 85.0747\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16499.0762 - mse: 16499.0762 - mae: 108.5376 - val_loss: 9243.6631 - val_mse: 9243.6631 - val_mae: 85.0620\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 16494.7246 - mse: 16494.7246 - mae: 108.5220 - val_loss: 9240.9414 - val_mse: 9240.9414 - val_mae: 85.0493\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16490.3633 - mse: 16490.3633 - mae: 108.5063 - val_loss: 9238.2354 - val_mse: 9238.2354 - val_mae: 85.0366\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16486.2051 - mse: 16486.2051 - mae: 108.4911 - val_loss: 9235.4570 - val_mse: 9235.4570 - val_mae: 85.0236\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 16481.6211 - mse: 16481.6211 - mae: 108.4753 - val_loss: 9232.7705 - val_mse: 9232.7705 - val_mae: 85.0111\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16477.1797 - mse: 16477.1797 - mae: 108.4601 - val_loss: 9230.0762 - val_mse: 9230.0762 - val_mae: 84.9985\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16473.1250 - mse: 16473.1250 - mae: 108.4462 - val_loss: 9227.3203 - val_mse: 9227.3203 - val_mae: 84.9856\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16468.2852 - mse: 16468.2852 - mae: 108.4305 - val_loss: 9224.6211 - val_mse: 9224.6211 - val_mae: 84.9730\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16464.2383 - mse: 16464.2383 - mae: 108.4157 - val_loss: 9221.8672 - val_mse: 9221.8672 - val_mae: 84.9601\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16459.7051 - mse: 16459.7051 - mae: 108.4003 - val_loss: 9219.1299 - val_mse: 9219.1299 - val_mae: 84.9473\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 16455.2617 - mse: 16455.2617 - mae: 108.3847 - val_loss: 9216.4014 - val_mse: 9216.4014 - val_mae: 84.9345\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16451.0352 - mse: 16451.0352 - mae: 108.3689 - val_loss: 9213.6250 - val_mse: 9213.6250 - val_mae: 84.9215\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16446.6289 - mse: 16446.6289 - mae: 108.3529 - val_loss: 9210.8711 - val_mse: 9210.8711 - val_mae: 84.9086\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16442.3887 - mse: 16442.3887 - mae: 108.3374 - val_loss: 9208.1416 - val_mse: 9208.1416 - val_mae: 84.8958\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16437.8242 - mse: 16437.8242 - mae: 108.3223 - val_loss: 9205.4443 - val_mse: 9205.4443 - val_mae: 84.8832\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16433.4570 - mse: 16433.4570 - mae: 108.3064 - val_loss: 9202.7217 - val_mse: 9202.7217 - val_mae: 84.8704\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 16429.4043 - mse: 16429.4043 - mae: 108.2906 - val_loss: 9199.9775 - val_mse: 9199.9775 - val_mae: 84.8576\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16424.8809 - mse: 16424.8809 - mae: 108.2748 - val_loss: 9197.3115 - val_mse: 9197.3115 - val_mae: 84.8451\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16420.6328 - mse: 16420.6328 - mae: 108.2596 - val_loss: 9194.6211 - val_mse: 9194.6211 - val_mae: 84.8325\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16416.3711 - mse: 16416.3711 - mae: 108.2443 - val_loss: 9191.8867 - val_mse: 9191.8867 - val_mae: 84.8197\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16412.1621 - mse: 16412.1621 - mae: 108.2294 - val_loss: 9189.1729 - val_mse: 9189.1729 - val_mae: 84.8069\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16407.7656 - mse: 16407.7656 - mae: 108.2139 - val_loss: 9186.4951 - val_mse: 9186.4951 - val_mae: 84.7944\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 16403.3672 - mse: 16403.3652 - mae: 108.1980 - val_loss: 9183.8311 - val_mse: 9183.8311 - val_mae: 84.7819\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 16399.3105 - mse: 16399.3105 - mae: 108.1824 - val_loss: 9181.1152 - val_mse: 9181.1152 - val_mae: 84.7692\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16394.8633 - mse: 16394.8633 - mae: 108.1671 - val_loss: 9178.4863 - val_mse: 9178.4863 - val_mae: 84.7568\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16390.5469 - mse: 16390.5469 - mae: 108.1526 - val_loss: 9175.8750 - val_mse: 9175.8750 - val_mae: 84.7446\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 16386.3496 - mse: 16386.3496 - mae: 108.1385 - val_loss: 9173.2314 - val_mse: 9173.2314 - val_mae: 84.7322\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 16382.0908 - mse: 16382.0908 - mae: 108.1236 - val_loss: 9170.4912 - val_mse: 9170.4912 - val_mae: 84.7193\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16377.7354 - mse: 16377.7354 - mae: 108.1078 - val_loss: 9167.7988 - val_mse: 9167.7988 - val_mae: 84.7067\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16373.5166 - mse: 16373.5166 - mae: 108.0931 - val_loss: 9165.1250 - val_mse: 9165.1250 - val_mae: 84.6942\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16369.1641 - mse: 16369.1641 - mae: 108.0780 - val_loss: 9162.4336 - val_mse: 9162.4336 - val_mae: 84.6815\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16364.8662 - mse: 16364.8662 - mae: 108.0629 - val_loss: 9159.7480 - val_mse: 9159.7480 - val_mae: 84.6689\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16360.5713 - mse: 16360.5713 - mae: 108.0473 - val_loss: 9157.0244 - val_mse: 9157.0244 - val_mae: 84.6561\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16356.3906 - mse: 16356.3906 - mae: 108.0322 - val_loss: 9154.3604 - val_mse: 9154.3604 - val_mae: 84.6436\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16351.9980 - mse: 16351.9980 - mae: 108.0174 - val_loss: 9151.7070 - val_mse: 9151.7070 - val_mae: 84.6311\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16347.5107 - mse: 16347.5107 - mae: 108.0023 - val_loss: 9149.1201 - val_mse: 9149.1201 - val_mae: 84.6190\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16343.4297 - mse: 16343.4297 - mae: 107.9871 - val_loss: 9146.3789 - val_mse: 9146.3789 - val_mae: 84.6061\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 16339.0859 - mse: 16339.0859 - mae: 107.9717 - val_loss: 9143.6885 - val_mse: 9143.6885 - val_mae: 84.5935\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16334.8770 - mse: 16334.8770 - mae: 107.9566 - val_loss: 9141.0156 - val_mse: 9141.0156 - val_mae: 84.5809\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16330.4092 - mse: 16330.4092 - mae: 107.9417 - val_loss: 9138.4053 - val_mse: 9138.4053 - val_mae: 84.5686\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16326.2275 - mse: 16326.2275 - mae: 107.9274 - val_loss: 9135.6836 - val_mse: 9135.6836 - val_mae: 84.5558\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16321.8721 - mse: 16321.8721 - mae: 107.9124 - val_loss: 9132.9453 - val_mse: 9132.9453 - val_mae: 84.5430\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16317.6523 - mse: 16317.6523 - mae: 107.8979 - val_loss: 9130.2578 - val_mse: 9130.2578 - val_mae: 84.5303\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16313.1191 - mse: 16313.1191 - mae: 107.8825 - val_loss: 9127.6025 - val_mse: 9127.6025 - val_mae: 84.5179\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16308.8623 - mse: 16308.8623 - mae: 107.8673 - val_loss: 9124.9238 - val_mse: 9124.9238 - val_mae: 84.5052\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 16304.6641 - mse: 16304.6641 - mae: 107.8516 - val_loss: 9122.2041 - val_mse: 9122.2041 - val_mae: 84.4924\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16300.2949 - mse: 16300.2949 - mae: 107.8356 - val_loss: 9119.5176 - val_mse: 9119.5176 - val_mae: 84.4798\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 16296.2422 - mse: 16296.2422 - mae: 107.8210 - val_loss: 9116.8232 - val_mse: 9116.8232 - val_mae: 84.4671\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16291.6094 - mse: 16291.6094 - mae: 107.8046 - val_loss: 9114.1318 - val_mse: 9114.1318 - val_mae: 84.4544\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16287.4316 - mse: 16287.4316 - mae: 107.7884 - val_loss: 9111.3984 - val_mse: 9111.3984 - val_mae: 84.4416\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 16283.1396 - mse: 16283.1396 - mae: 107.7735 - val_loss: 9108.7588 - val_mse: 9108.7588 - val_mae: 84.4291\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16278.8740 - mse: 16278.8740 - mae: 107.7586 - val_loss: 9106.0088 - val_mse: 9106.0088 - val_mae: 84.4162\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16274.4385 - mse: 16274.4385 - mae: 107.7431 - val_loss: 9103.3105 - val_mse: 9103.3105 - val_mae: 84.4035\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 16270.0469 - mse: 16270.0469 - mae: 107.7280 - val_loss: 9100.6289 - val_mse: 9100.6289 - val_mae: 84.3908\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 16265.8145 - mse: 16265.8145 - mae: 107.7126 - val_loss: 9097.9512 - val_mse: 9097.9512 - val_mae: 84.3782\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16261.4668 - mse: 16261.4668 - mae: 107.6972 - val_loss: 9095.2852 - val_mse: 9095.2852 - val_mae: 84.3657\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 16257.2676 - mse: 16257.2676 - mae: 107.6822 - val_loss: 9092.5986 - val_mse: 9092.5986 - val_mae: 84.3530\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16252.9688 - mse: 16252.9688 - mae: 107.6669 - val_loss: 9089.9111 - val_mse: 9089.9111 - val_mae: 84.3403\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16248.6768 - mse: 16248.6768 - mae: 107.6511 - val_loss: 9087.2002 - val_mse: 9087.2002 - val_mae: 84.3276\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16244.2773 - mse: 16244.2773 - mae: 107.6354 - val_loss: 9084.5449 - val_mse: 9084.5449 - val_mae: 84.3150\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 16240.1025 - mse: 16240.1025 - mae: 107.6202 - val_loss: 9081.8428 - val_mse: 9081.8428 - val_mae: 84.3023\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16235.8887 - mse: 16235.8887 - mae: 107.6059 - val_loss: 9079.1836 - val_mse: 9079.1836 - val_mae: 84.2897\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16231.3418 - mse: 16231.3418 - mae: 107.5903 - val_loss: 9076.5361 - val_mse: 9076.5361 - val_mae: 84.2773\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16227.2100 - mse: 16227.2100 - mae: 107.5749 - val_loss: 9073.8359 - val_mse: 9073.8359 - val_mae: 84.2645\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16222.8828 - mse: 16222.8828 - mae: 107.5596 - val_loss: 9071.1543 - val_mse: 9071.1543 - val_mae: 84.2519\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16218.5518 - mse: 16218.5518 - mae: 107.5444 - val_loss: 9068.4541 - val_mse: 9068.4541 - val_mae: 84.2391\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 16214.2393 - mse: 16214.2393 - mae: 107.5294 - val_loss: 9065.8135 - val_mse: 9065.8135 - val_mae: 84.2267\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16210.0527 - mse: 16210.0527 - mae: 107.5156 - val_loss: 9063.1982 - val_mse: 9063.1982 - val_mae: 84.2143\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 16205.5703 - mse: 16205.5703 - mae: 107.5002 - val_loss: 9060.5146 - val_mse: 9060.5146 - val_mae: 84.2016\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16201.3877 - mse: 16201.3877 - mae: 107.4845 - val_loss: 9057.7939 - val_mse: 9057.7939 - val_mae: 84.1888\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16197.1914 - mse: 16197.1914 - mae: 107.4690 - val_loss: 9055.0703 - val_mse: 9055.0703 - val_mae: 84.1759\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16192.8291 - mse: 16192.8291 - mae: 107.4530 - val_loss: 9052.3740 - val_mse: 9052.3740 - val_mae: 84.1632\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16188.6338 - mse: 16188.6338 - mae: 107.4371 - val_loss: 9049.6963 - val_mse: 9049.6963 - val_mae: 84.1505\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 16184.2266 - mse: 16184.2266 - mae: 107.4217 - val_loss: 9047.0986 - val_mse: 9047.0986 - val_mae: 84.1382\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 16180.0518 - mse: 16180.0518 - mae: 107.4065 - val_loss: 9044.4268 - val_mse: 9044.4268 - val_mae: 84.1256\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16175.8662 - mse: 16175.8662 - mae: 107.3918 - val_loss: 9041.7266 - val_mse: 9041.7266 - val_mae: 84.1128\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16171.4082 - mse: 16171.4082 - mae: 107.3764 - val_loss: 9039.0625 - val_mse: 9039.0625 - val_mae: 84.1002\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 16167.2109 - mse: 16167.2109 - mae: 107.3619 - val_loss: 9036.3926 - val_mse: 9036.3926 - val_mae: 84.0876\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 16162.8252 - mse: 16162.8252 - mae: 107.3479 - val_loss: 9033.7432 - val_mse: 9033.7432 - val_mae: 84.0751\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16158.5117 - mse: 16158.5117 - mae: 107.3328 - val_loss: 9031.0703 - val_mse: 9031.0703 - val_mae: 84.0625\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16154.3223 - mse: 16154.3223 - mae: 107.3172 - val_loss: 9028.3564 - val_mse: 9028.3564 - val_mae: 84.0496\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16149.9941 - mse: 16149.9941 - mae: 107.3015 - val_loss: 9025.7939 - val_mse: 9025.7939 - val_mae: 84.0375\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 16145.7461 - mse: 16145.7461 - mae: 107.2865 - val_loss: 9023.1367 - val_mse: 9023.1367 - val_mae: 84.0249\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16141.7266 - mse: 16141.7266 - mae: 107.2722 - val_loss: 9020.4951 - val_mse: 9020.4951 - val_mae: 84.0124\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 16137.2227 - mse: 16137.2227 - mae: 107.2572 - val_loss: 9017.7861 - val_mse: 9017.7861 - val_mae: 83.9996\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16132.9609 - mse: 16132.9609 - mae: 107.2414 - val_loss: 9015.1299 - val_mse: 9015.1299 - val_mae: 83.9870\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16128.7012 - mse: 16128.7012 - mae: 107.2256 - val_loss: 9012.4385 - val_mse: 9012.4385 - val_mae: 83.9742\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.5, model__n_hidden=0, model__n_neurons=25, model__optimizer=adam; total time=   4.9s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 22ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 10: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.1, model__n_hidden=3, model__n_neurons=125, model__optimizer=momentum; total time=   1.2s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 31ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 10: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.1, model__n_hidden=3, model__n_neurons=125, model__optimizer=momentum; total time=   1.3s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 22ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 10: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.1, model__n_hidden=3, model__n_neurons=125, model__optimizer=momentum; total time=   1.1s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 22ms/step - loss: 466.2820 - mse: 466.2820 - mae: 19.6049 - val_loss: 320.2179 - val_mse: 320.2179 - val_mae: 17.3469\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 465.7695 - mse: 465.7695 - mae: 19.5918 - val_loss: 319.7615 - val_mse: 319.7615 - val_mae: 17.3336\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 465.2549 - mse: 465.2549 - mae: 19.5786 - val_loss: 319.3061 - val_mse: 319.3061 - val_mae: 17.3204\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 464.7366 - mse: 464.7366 - mae: 19.5653 - val_loss: 318.8532 - val_mse: 318.8532 - val_mae: 17.3072\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 464.2262 - mse: 464.2262 - mae: 19.5522 - val_loss: 318.3986 - val_mse: 318.3986 - val_mae: 17.2939\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 463.7136 - mse: 463.7136 - mae: 19.5390 - val_loss: 317.9471 - val_mse: 317.9471 - val_mae: 17.2807\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 463.2026 - mse: 463.2026 - mae: 19.5260 - val_loss: 317.4967 - val_mse: 317.4967 - val_mae: 17.2676\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 462.6870 - mse: 462.6870 - mae: 19.5129 - val_loss: 317.0515 - val_mse: 317.0515 - val_mae: 17.2545\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 462.1797 - mse: 462.1797 - mae: 19.4998 - val_loss: 316.6012 - val_mse: 316.6012 - val_mae: 17.2414\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 461.6721 - mse: 461.6721 - mae: 19.4867 - val_loss: 316.1480 - val_mse: 316.1480 - val_mae: 17.2281\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 461.1560 - mse: 461.1560 - mae: 19.4735 - val_loss: 315.7010 - val_mse: 315.7010 - val_mae: 17.2150\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 460.6506 - mse: 460.6506 - mae: 19.4606 - val_loss: 315.2529 - val_mse: 315.2529 - val_mae: 17.2018\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 460.1408 - mse: 460.1408 - mae: 19.4475 - val_loss: 314.8051 - val_mse: 314.8051 - val_mae: 17.1887\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 459.6352 - mse: 459.6352 - mae: 19.4342 - val_loss: 314.3566 - val_mse: 314.3566 - val_mae: 17.1755\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 459.1266 - mse: 459.1266 - mae: 19.4212 - val_loss: 313.9077 - val_mse: 313.9077 - val_mae: 17.1623\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 458.6216 - mse: 458.6216 - mae: 19.4080 - val_loss: 313.4602 - val_mse: 313.4602 - val_mae: 17.1491\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 458.1127 - mse: 458.1127 - mae: 19.3950 - val_loss: 313.0190 - val_mse: 313.0190 - val_mae: 17.1361\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 457.6099 - mse: 457.6099 - mae: 19.3820 - val_loss: 312.5769 - val_mse: 312.5769 - val_mae: 17.1231\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 457.1033 - mse: 457.1033 - mae: 19.3692 - val_loss: 312.1380 - val_mse: 312.1380 - val_mae: 17.1101\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 456.6058 - mse: 456.6058 - mae: 19.3561 - val_loss: 311.6938 - val_mse: 311.6938 - val_mae: 17.0970\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 456.1021 - mse: 456.1021 - mae: 19.3428 - val_loss: 311.2466 - val_mse: 311.2466 - val_mae: 17.0838\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 455.5953 - mse: 455.5953 - mae: 19.3298 - val_loss: 310.8019 - val_mse: 310.8019 - val_mae: 17.0707\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 455.0876 - mse: 455.0876 - mae: 19.3169 - val_loss: 310.3660 - val_mse: 310.3660 - val_mae: 17.0578\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 454.5930 - mse: 454.5930 - mae: 19.3038 - val_loss: 309.9201 - val_mse: 309.9201 - val_mae: 17.0445\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 454.0926 - mse: 454.0926 - mae: 19.2907 - val_loss: 309.4722 - val_mse: 309.4722 - val_mae: 17.0313\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 453.5835 - mse: 453.5835 - mae: 19.2776 - val_loss: 309.0282 - val_mse: 309.0282 - val_mae: 17.0181\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 453.0798 - mse: 453.0798 - mae: 19.2647 - val_loss: 308.5872 - val_mse: 308.5872 - val_mae: 17.0050\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 452.5871 - mse: 452.5871 - mae: 19.2516 - val_loss: 308.1386 - val_mse: 308.1386 - val_mae: 16.9917\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 452.0826 - mse: 452.0826 - mae: 19.2387 - val_loss: 307.6946 - val_mse: 307.6946 - val_mae: 16.9784\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 451.5870 - mse: 451.5870 - mae: 19.2256 - val_loss: 307.2461 - val_mse: 307.2461 - val_mae: 16.9651\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 451.0844 - mse: 451.0844 - mae: 19.2125 - val_loss: 306.8023 - val_mse: 306.8023 - val_mae: 16.9519\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 450.5862 - mse: 450.5862 - mae: 19.1995 - val_loss: 306.3602 - val_mse: 306.3602 - val_mae: 16.9387\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 450.0898 - mse: 450.0898 - mae: 19.1865 - val_loss: 305.9165 - val_mse: 305.9165 - val_mae: 16.9254\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 449.5876 - mse: 449.5876 - mae: 19.1736 - val_loss: 305.4749 - val_mse: 305.4749 - val_mae: 16.9122\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 449.0930 - mse: 449.0930 - mae: 19.1605 - val_loss: 305.0303 - val_mse: 305.0303 - val_mae: 16.8989\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 448.5984 - mse: 448.5984 - mae: 19.1474 - val_loss: 304.5869 - val_mse: 304.5869 - val_mae: 16.8857\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 448.0992 - mse: 448.0992 - mae: 19.1344 - val_loss: 304.1458 - val_mse: 304.1458 - val_mae: 16.8725\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 447.6005 - mse: 447.6005 - mae: 19.1213 - val_loss: 303.7107 - val_mse: 303.7107 - val_mae: 16.8594\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 447.1138 - mse: 447.1138 - mae: 19.1085 - val_loss: 303.2736 - val_mse: 303.2736 - val_mae: 16.8463\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 446.6139 - mse: 446.6139 - mae: 19.0956 - val_loss: 302.8394 - val_mse: 302.8394 - val_mae: 16.8333\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 446.1322 - mse: 446.1322 - mae: 19.0826 - val_loss: 302.3933 - val_mse: 302.3933 - val_mae: 16.8199\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 445.6271 - mse: 445.6271 - mae: 19.0694 - val_loss: 301.9580 - val_mse: 301.9580 - val_mae: 16.8068\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 445.1395 - mse: 445.1395 - mae: 19.0566 - val_loss: 301.5192 - val_mse: 301.5192 - val_mae: 16.7936\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 444.6449 - mse: 444.6449 - mae: 19.0435 - val_loss: 301.0850 - val_mse: 301.0850 - val_mae: 16.7805\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 444.1579 - mse: 444.1579 - mae: 19.0306 - val_loss: 300.6469 - val_mse: 300.6469 - val_mae: 16.7673\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 443.6650 - mse: 443.6650 - mae: 19.0176 - val_loss: 300.2107 - val_mse: 300.2107 - val_mae: 16.7541\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 443.1729 - mse: 443.1729 - mae: 19.0048 - val_loss: 299.7787 - val_mse: 299.7787 - val_mae: 16.7411\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 442.6848 - mse: 442.6848 - mae: 18.9917 - val_loss: 299.3446 - val_mse: 299.3446 - val_mae: 16.7280\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 442.1925 - mse: 442.1925 - mae: 18.9790 - val_loss: 298.9098 - val_mse: 298.9098 - val_mae: 16.7148\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 441.7035 - mse: 441.7035 - mae: 18.9659 - val_loss: 298.4743 - val_mse: 298.4743 - val_mae: 16.7016\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 441.2106 - mse: 441.2106 - mae: 18.9528 - val_loss: 298.0444 - val_mse: 298.0444 - val_mae: 16.6886\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 440.7266 - mse: 440.7266 - mae: 18.9400 - val_loss: 297.6063 - val_mse: 297.6063 - val_mae: 16.6753\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 440.2278 - mse: 440.2278 - mae: 18.9269 - val_loss: 297.1760 - val_mse: 297.1760 - val_mae: 16.6622\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 439.7493 - mse: 439.7493 - mae: 18.9139 - val_loss: 296.7352 - val_mse: 296.7352 - val_mae: 16.6489\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 439.2485 - mse: 439.2485 - mae: 18.9008 - val_loss: 296.3095 - val_mse: 296.3095 - val_mae: 16.6359\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 438.7656 - mse: 438.7656 - mae: 18.8880 - val_loss: 295.8781 - val_mse: 295.8781 - val_mae: 16.6228\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 438.2797 - mse: 438.2797 - mae: 18.8751 - val_loss: 295.4469 - val_mse: 295.4469 - val_mae: 16.6097\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 437.7905 - mse: 437.7905 - mae: 18.8621 - val_loss: 295.0209 - val_mse: 295.0209 - val_mae: 16.5967\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 437.3131 - mse: 437.3131 - mae: 18.8491 - val_loss: 294.5832 - val_mse: 294.5832 - val_mae: 16.5833\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 436.8203 - mse: 436.8203 - mae: 18.8360 - val_loss: 294.1574 - val_mse: 294.1574 - val_mae: 16.5703\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 436.3369 - mse: 436.3369 - mae: 18.8233 - val_loss: 293.7315 - val_mse: 293.7315 - val_mae: 16.5573\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 435.8605 - mse: 435.8605 - mae: 18.8102 - val_loss: 293.3019 - val_mse: 293.3019 - val_mae: 16.5442\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 435.3739 - mse: 435.3739 - mae: 18.7972 - val_loss: 292.8763 - val_mse: 292.8763 - val_mae: 16.5312\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 434.8906 - mse: 434.8906 - mae: 18.7844 - val_loss: 292.4521 - val_mse: 292.4521 - val_mae: 16.5182\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 434.4048 - mse: 434.4048 - mae: 18.7717 - val_loss: 292.0276 - val_mse: 292.0276 - val_mae: 16.5052\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 433.9297 - mse: 433.9297 - mae: 18.7586 - val_loss: 291.5998 - val_mse: 291.5998 - val_mae: 16.4920\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 433.4469 - mse: 433.4469 - mae: 18.7459 - val_loss: 291.1760 - val_mse: 291.1760 - val_mae: 16.4790\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 432.9675 - mse: 432.9675 - mae: 18.7330 - val_loss: 290.7534 - val_mse: 290.7534 - val_mae: 16.4660\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 432.4942 - mse: 432.4942 - mae: 18.7201 - val_loss: 290.3223 - val_mse: 290.3223 - val_mae: 16.4528\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 432.0032 - mse: 432.0032 - mae: 18.7070 - val_loss: 289.8988 - val_mse: 289.8988 - val_mae: 16.4397\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 431.5272 - mse: 431.5272 - mae: 18.6941 - val_loss: 289.4785 - val_mse: 289.4785 - val_mae: 16.4268\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 431.0488 - mse: 431.0488 - mae: 18.6814 - val_loss: 289.0608 - val_mse: 289.0608 - val_mae: 16.4139\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 430.5799 - mse: 430.5799 - mae: 18.6685 - val_loss: 288.6380 - val_mse: 288.6380 - val_mae: 16.4009\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 430.0979 - mse: 430.0979 - mae: 18.6557 - val_loss: 288.2223 - val_mse: 288.2223 - val_mae: 16.3880\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 429.6264 - mse: 429.6264 - mae: 18.6430 - val_loss: 287.8004 - val_mse: 287.8004 - val_mae: 16.3750\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 429.1484 - mse: 429.1484 - mae: 18.6302 - val_loss: 287.3819 - val_mse: 287.3819 - val_mae: 16.3620\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 428.6787 - mse: 428.6787 - mae: 18.6173 - val_loss: 286.9647 - val_mse: 286.9647 - val_mae: 16.3491\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 428.2072 - mse: 428.2072 - mae: 18.6045 - val_loss: 286.5459 - val_mse: 286.5459 - val_mae: 16.3362\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 427.7306 - mse: 427.7306 - mae: 18.5919 - val_loss: 286.1291 - val_mse: 286.1291 - val_mae: 16.3232\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 427.2589 - mse: 427.2589 - mae: 18.5790 - val_loss: 285.7113 - val_mse: 285.7113 - val_mae: 16.3103\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 426.7876 - mse: 426.7876 - mae: 18.5660 - val_loss: 285.2894 - val_mse: 285.2894 - val_mae: 16.2972\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 426.3059 - mse: 426.3059 - mae: 18.5533 - val_loss: 284.8759 - val_mse: 284.8759 - val_mae: 16.2843\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 425.8398 - mse: 425.8398 - mae: 18.5405 - val_loss: 284.4599 - val_mse: 284.4599 - val_mae: 16.2713\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 425.3668 - mse: 425.3668 - mae: 18.5277 - val_loss: 284.0439 - val_mse: 284.0439 - val_mae: 16.2584\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 424.8896 - mse: 424.8896 - mae: 18.5149 - val_loss: 283.6305 - val_mse: 283.6305 - val_mae: 16.2455\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 424.4227 - mse: 424.4227 - mae: 18.5022 - val_loss: 283.2157 - val_mse: 283.2157 - val_mae: 16.2326\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 423.9586 - mse: 423.9586 - mae: 18.4893 - val_loss: 282.7914 - val_mse: 282.7914 - val_mae: 16.2193\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 423.4759 - mse: 423.4759 - mae: 18.4763 - val_loss: 282.3805 - val_mse: 282.3805 - val_mae: 16.2065\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 423.0101 - mse: 423.0101 - mae: 18.4635 - val_loss: 281.9654 - val_mse: 281.9654 - val_mae: 16.1935\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 422.5471 - mse: 422.5471 - mae: 18.4507 - val_loss: 281.5507 - val_mse: 281.5507 - val_mae: 16.1805\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 422.0747 - mse: 422.0747 - mae: 18.4380 - val_loss: 281.1398 - val_mse: 281.1398 - val_mae: 16.1677\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 421.6088 - mse: 421.6088 - mae: 18.4251 - val_loss: 280.7270 - val_mse: 280.7270 - val_mae: 16.1547\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 421.1393 - mse: 421.1393 - mae: 18.4124 - val_loss: 280.3169 - val_mse: 280.3169 - val_mae: 16.1418\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 420.6739 - mse: 420.6739 - mae: 18.3997 - val_loss: 279.9077 - val_mse: 279.9077 - val_mae: 16.1290\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 420.2135 - mse: 420.2135 - mae: 18.3871 - val_loss: 279.4973 - val_mse: 279.4973 - val_mae: 16.1161\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 419.7451 - mse: 419.7451 - mae: 18.3743 - val_loss: 279.0887 - val_mse: 279.0887 - val_mae: 16.1032\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 419.2774 - mse: 419.2774 - mae: 18.3616 - val_loss: 278.6814 - val_mse: 278.6814 - val_mae: 16.0904\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 418.8231 - mse: 418.8231 - mae: 18.3487 - val_loss: 278.2633 - val_mse: 278.2633 - val_mae: 16.0772\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 418.3482 - mse: 418.3482 - mae: 18.3358 - val_loss: 277.8537 - val_mse: 277.8537 - val_mae: 16.0643\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 417.8743 - mse: 417.8743 - mae: 18.3232 - val_loss: 277.4481 - val_mse: 277.4481 - val_mae: 16.0515\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.1, model__n_hidden=3, model__n_neurons=5, model__optimizer=adam; total time=   5.2s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 23ms/step - loss: 319.2419 - mse: 319.2419 - mae: 14.8449 - val_loss: 187.2368 - val_mse: 187.2368 - val_mae: 12.2542\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 319.2040 - mse: 319.2040 - mae: 14.8438 - val_loss: 187.2059 - val_mse: 187.2059 - val_mae: 12.2531\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 319.1652 - mse: 319.1652 - mae: 14.8427 - val_loss: 187.1754 - val_mse: 187.1754 - val_mae: 12.2520\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 319.1276 - mse: 319.1276 - mae: 14.8415 - val_loss: 187.1450 - val_mse: 187.1450 - val_mae: 12.2509\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 319.0885 - mse: 319.0885 - mae: 14.8404 - val_loss: 187.1149 - val_mse: 187.1149 - val_mae: 12.2498\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 319.0518 - mse: 319.0518 - mae: 14.8393 - val_loss: 187.0850 - val_mse: 187.0850 - val_mae: 12.2487\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 319.0135 - mse: 319.0135 - mae: 14.8382 - val_loss: 187.0551 - val_mse: 187.0551 - val_mae: 12.2477\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 318.9751 - mse: 318.9751 - mae: 14.8370 - val_loss: 187.0256 - val_mse: 187.0256 - val_mae: 12.2466\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 318.9390 - mse: 318.9390 - mae: 14.8360 - val_loss: 186.9958 - val_mse: 186.9958 - val_mae: 12.2456\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 318.9008 - mse: 318.9008 - mae: 14.8348 - val_loss: 186.9660 - val_mse: 186.9660 - val_mae: 12.2445\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 318.8635 - mse: 318.8635 - mae: 14.8337 - val_loss: 186.9350 - val_mse: 186.9350 - val_mae: 12.2434\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.1, model__n_hidden=3, model__n_neurons=5, model__optimizer=adam; total time=   1.2s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 21ms/step - loss: 447.1985 - mse: 447.1985 - mae: 18.9665 - val_loss: 449.0762 - val_mse: 449.0762 - val_mae: 19.0354\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 446.9879 - mse: 446.9879 - mae: 18.9598 - val_loss: 448.8707 - val_mse: 448.8707 - val_mae: 19.0285\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 446.7796 - mse: 446.7796 - mae: 18.9531 - val_loss: 448.6633 - val_mse: 448.6633 - val_mae: 19.0215\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 446.5685 - mse: 446.5685 - mae: 18.9464 - val_loss: 448.4570 - val_mse: 448.4570 - val_mae: 19.0146\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 446.3611 - mse: 446.3611 - mae: 18.9397 - val_loss: 448.2500 - val_mse: 448.2500 - val_mae: 19.0076\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 446.1480 - mse: 446.1480 - mae: 18.9329 - val_loss: 448.0450 - val_mse: 448.0450 - val_mae: 19.0007\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 445.9376 - mse: 445.9376 - mae: 18.9262 - val_loss: 447.8394 - val_mse: 447.8394 - val_mae: 18.9937\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 445.7310 - mse: 445.7310 - mae: 18.9194 - val_loss: 447.6315 - val_mse: 447.6315 - val_mae: 18.9867\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 445.5154 - mse: 445.5154 - mae: 18.9127 - val_loss: 447.4279 - val_mse: 447.4279 - val_mae: 18.9798\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 445.3049 - mse: 445.3049 - mae: 18.9060 - val_loss: 447.2246 - val_mse: 447.2246 - val_mae: 18.9730\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 445.0993 - mse: 445.0993 - mae: 18.8994 - val_loss: 447.0174 - val_mse: 447.0174 - val_mae: 18.9660\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 444.8878 - mse: 444.8878 - mae: 18.8926 - val_loss: 446.8134 - val_mse: 446.8134 - val_mae: 18.9590\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 444.6802 - mse: 444.6802 - mae: 18.8860 - val_loss: 446.6085 - val_mse: 446.6085 - val_mae: 18.9521\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 444.4705 - mse: 444.4705 - mae: 18.8792 - val_loss: 446.4040 - val_mse: 446.4040 - val_mae: 18.9452\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 444.2630 - mse: 444.2630 - mae: 18.8724 - val_loss: 446.1992 - val_mse: 446.1992 - val_mae: 18.9382\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 444.0500 - mse: 444.0500 - mae: 18.8657 - val_loss: 445.9973 - val_mse: 445.9973 - val_mae: 18.9314\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 443.8413 - mse: 443.8413 - mae: 18.8589 - val_loss: 445.7926 - val_mse: 445.7926 - val_mae: 18.9244\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 443.6323 - mse: 443.6323 - mae: 18.8522 - val_loss: 445.5876 - val_mse: 445.5876 - val_mae: 18.9174\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 443.4210 - mse: 443.4210 - mae: 18.8455 - val_loss: 445.3850 - val_mse: 445.3850 - val_mae: 18.9105\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 443.2128 - mse: 443.2128 - mae: 18.8388 - val_loss: 445.1825 - val_mse: 445.1825 - val_mae: 18.9036\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 443.0035 - mse: 443.0035 - mae: 18.8320 - val_loss: 444.9813 - val_mse: 444.9813 - val_mae: 18.8968\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 442.7947 - mse: 442.7947 - mae: 18.8254 - val_loss: 444.7806 - val_mse: 444.7806 - val_mae: 18.8900\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 442.5877 - mse: 442.5877 - mae: 18.8188 - val_loss: 444.5789 - val_mse: 444.5789 - val_mae: 18.8831\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 442.3803 - mse: 442.3803 - mae: 18.8121 - val_loss: 444.3772 - val_mse: 444.3772 - val_mae: 18.8762\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 442.1718 - mse: 442.1718 - mae: 18.8052 - val_loss: 444.1739 - val_mse: 444.1739 - val_mae: 18.8692\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 441.9606 - mse: 441.9606 - mae: 18.7986 - val_loss: 443.9738 - val_mse: 443.9738 - val_mae: 18.8624\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 441.7569 - mse: 441.7569 - mae: 18.7919 - val_loss: 443.7690 - val_mse: 443.7690 - val_mae: 18.8554\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 441.5456 - mse: 441.5456 - mae: 18.7852 - val_loss: 443.5689 - val_mse: 443.5689 - val_mae: 18.8486\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 441.3363 - mse: 441.3363 - mae: 18.7786 - val_loss: 443.3680 - val_mse: 443.3680 - val_mae: 18.8417\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 441.1277 - mse: 441.1277 - mae: 18.7719 - val_loss: 443.1660 - val_mse: 443.1660 - val_mae: 18.8348\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 440.9178 - mse: 440.9178 - mae: 18.7653 - val_loss: 442.9627 - val_mse: 442.9627 - val_mae: 18.8278\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 440.7074 - mse: 440.7074 - mae: 18.7586 - val_loss: 442.7587 - val_mse: 442.7587 - val_mae: 18.8208\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 440.4959 - mse: 440.4959 - mae: 18.7518 - val_loss: 442.5566 - val_mse: 442.5566 - val_mae: 18.8139\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 440.2868 - mse: 440.2868 - mae: 18.7452 - val_loss: 442.3549 - val_mse: 442.3549 - val_mae: 18.8069\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 440.0771 - mse: 440.0771 - mae: 18.7386 - val_loss: 442.1530 - val_mse: 442.1530 - val_mae: 18.8000\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 439.8686 - mse: 439.8686 - mae: 18.7319 - val_loss: 441.9519 - val_mse: 441.9519 - val_mae: 18.7931\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 439.6625 - mse: 439.6625 - mae: 18.7252 - val_loss: 441.7497 - val_mse: 441.7497 - val_mae: 18.7861\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 439.4525 - mse: 439.4525 - mae: 18.7185 - val_loss: 441.5483 - val_mse: 441.5483 - val_mae: 18.7792\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 439.2422 - mse: 439.2422 - mae: 18.7118 - val_loss: 441.3486 - val_mse: 441.3486 - val_mae: 18.7723\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 439.0307 - mse: 439.0307 - mae: 18.7052 - val_loss: 441.1502 - val_mse: 441.1502 - val_mae: 18.7655\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 438.8204 - mse: 438.8204 - mae: 18.6984 - val_loss: 440.9519 - val_mse: 440.9519 - val_mae: 18.7586\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 438.6098 - mse: 438.6098 - mae: 18.6917 - val_loss: 440.7532 - val_mse: 440.7532 - val_mae: 18.7518\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 438.4059 - mse: 438.4059 - mae: 18.6851 - val_loss: 440.5484 - val_mse: 440.5484 - val_mae: 18.7447\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 438.1935 - mse: 438.1935 - mae: 18.6783 - val_loss: 440.3467 - val_mse: 440.3467 - val_mae: 18.7377\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 437.9827 - mse: 437.9827 - mae: 18.6714 - val_loss: 440.1469 - val_mse: 440.1469 - val_mae: 18.7308\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 437.7703 - mse: 437.7703 - mae: 18.6646 - val_loss: 439.9494 - val_mse: 439.9494 - val_mae: 18.7240\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 437.5683 - mse: 437.5683 - mae: 18.6580 - val_loss: 439.7463 - val_mse: 439.7463 - val_mae: 18.7169\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 437.3569 - mse: 437.3569 - mae: 18.6513 - val_loss: 439.5453 - val_mse: 439.5453 - val_mae: 18.7100\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 437.1484 - mse: 437.1484 - mae: 18.6445 - val_loss: 439.3445 - val_mse: 439.3445 - val_mae: 18.7030\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 436.9357 - mse: 436.9357 - mae: 18.6377 - val_loss: 439.1490 - val_mse: 439.1490 - val_mae: 18.6962\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 436.7295 - mse: 436.7295 - mae: 18.6309 - val_loss: 438.9495 - val_mse: 438.9495 - val_mae: 18.6893\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 436.5179 - mse: 436.5179 - mae: 18.6242 - val_loss: 438.7525 - val_mse: 438.7525 - val_mae: 18.6824\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 436.3134 - mse: 436.3134 - mae: 18.6177 - val_loss: 438.5528 - val_mse: 438.5528 - val_mae: 18.6755\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 436.1064 - mse: 436.1064 - mae: 18.6109 - val_loss: 438.3515 - val_mse: 438.3515 - val_mae: 18.6685\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 435.8925 - mse: 435.8925 - mae: 18.6040 - val_loss: 438.1544 - val_mse: 438.1544 - val_mae: 18.6616\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 435.6893 - mse: 435.6893 - mae: 18.5975 - val_loss: 437.9531 - val_mse: 437.9531 - val_mae: 18.6546\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 435.4773 - mse: 435.4773 - mae: 18.5906 - val_loss: 437.7568 - val_mse: 437.7568 - val_mae: 18.6478\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 435.2709 - mse: 435.2709 - mae: 18.5839 - val_loss: 437.5611 - val_mse: 437.5611 - val_mae: 18.6409\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 435.0641 - mse: 435.0641 - mae: 18.5772 - val_loss: 437.3640 - val_mse: 437.3640 - val_mae: 18.6341\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 434.8571 - mse: 434.8571 - mae: 18.5705 - val_loss: 437.1658 - val_mse: 437.1658 - val_mae: 18.6271\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 434.6500 - mse: 434.6500 - mae: 18.5639 - val_loss: 436.9684 - val_mse: 436.9684 - val_mae: 18.6202\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 434.4428 - mse: 434.4428 - mae: 18.5570 - val_loss: 436.7699 - val_mse: 436.7699 - val_mae: 18.6133\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 434.2343 - mse: 434.2343 - mae: 18.5502 - val_loss: 436.5723 - val_mse: 436.5723 - val_mae: 18.6064\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 434.0267 - mse: 434.0267 - mae: 18.5436 - val_loss: 436.3763 - val_mse: 436.3763 - val_mae: 18.5995\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 433.8210 - mse: 433.8210 - mae: 18.5368 - val_loss: 436.1801 - val_mse: 436.1801 - val_mae: 18.5926\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 433.6111 - mse: 433.6111 - mae: 18.5301 - val_loss: 435.9863 - val_mse: 435.9863 - val_mae: 18.5858\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 433.4102 - mse: 433.4102 - mae: 18.5234 - val_loss: 435.7862 - val_mse: 435.7862 - val_mae: 18.5788\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 433.2023 - mse: 433.2023 - mae: 18.5166 - val_loss: 435.5875 - val_mse: 435.5875 - val_mae: 18.5719\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 432.9910 - mse: 432.9910 - mae: 18.5098 - val_loss: 435.3926 - val_mse: 435.3926 - val_mae: 18.5650\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 432.7884 - mse: 432.7884 - mae: 18.5032 - val_loss: 435.1963 - val_mse: 435.1963 - val_mae: 18.5581\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 432.5807 - mse: 432.5807 - mae: 18.4965 - val_loss: 435.0031 - val_mse: 435.0031 - val_mae: 18.5513\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 432.3774 - mse: 432.3774 - mae: 18.4898 - val_loss: 434.8074 - val_mse: 434.8074 - val_mae: 18.5444\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 432.1700 - mse: 432.1700 - mae: 18.4832 - val_loss: 434.6148 - val_mse: 434.6148 - val_mae: 18.5376\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 431.9655 - mse: 431.9655 - mae: 18.4764 - val_loss: 434.4217 - val_mse: 434.4217 - val_mae: 18.5308\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 431.7628 - mse: 431.7628 - mae: 18.4698 - val_loss: 434.2241 - val_mse: 434.2241 - val_mae: 18.5238\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 431.5587 - mse: 431.5587 - mae: 18.4629 - val_loss: 434.0266 - val_mse: 434.0266 - val_mae: 18.5169\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 431.3526 - mse: 431.3526 - mae: 18.4562 - val_loss: 433.8321 - val_mse: 433.8321 - val_mae: 18.5100\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 431.1456 - mse: 431.1456 - mae: 18.4494 - val_loss: 433.6406 - val_mse: 433.6406 - val_mae: 18.5032\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 430.9478 - mse: 430.9478 - mae: 18.4427 - val_loss: 433.4424 - val_mse: 433.4424 - val_mae: 18.4962\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 430.7375 - mse: 430.7375 - mae: 18.4360 - val_loss: 433.2492 - val_mse: 433.2492 - val_mae: 18.4894\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 430.5327 - mse: 430.5327 - mae: 18.4293 - val_loss: 433.0552 - val_mse: 433.0552 - val_mae: 18.4825\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 430.3295 - mse: 430.3295 - mae: 18.4225 - val_loss: 432.8605 - val_mse: 432.8605 - val_mae: 18.4756\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 430.1239 - mse: 430.1239 - mae: 18.4159 - val_loss: 432.6693 - val_mse: 432.6693 - val_mae: 18.4688\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 429.9178 - mse: 429.9178 - mae: 18.4092 - val_loss: 432.4782 - val_mse: 432.4782 - val_mae: 18.4621\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 429.7177 - mse: 429.7177 - mae: 18.4025 - val_loss: 432.2833 - val_mse: 432.2833 - val_mae: 18.4551\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 429.5132 - mse: 429.5132 - mae: 18.3958 - val_loss: 432.0893 - val_mse: 432.0893 - val_mae: 18.4482\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 429.3081 - mse: 429.3081 - mae: 18.3890 - val_loss: 431.8976 - val_mse: 431.8976 - val_mae: 18.4414\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 429.1064 - mse: 429.1064 - mae: 18.3824 - val_loss: 431.7035 - val_mse: 431.7035 - val_mae: 18.4345\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 428.8977 - mse: 428.8977 - mae: 18.3755 - val_loss: 431.5138 - val_mse: 431.5138 - val_mae: 18.4278\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 428.6956 - mse: 428.6956 - mae: 18.3689 - val_loss: 431.3215 - val_mse: 431.3215 - val_mae: 18.4209\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 428.4901 - mse: 428.4901 - mae: 18.3622 - val_loss: 431.1287 - val_mse: 431.1287 - val_mae: 18.4140\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 428.2852 - mse: 428.2852 - mae: 18.3554 - val_loss: 430.9336 - val_mse: 430.9336 - val_mae: 18.4071\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 428.0776 - mse: 428.0776 - mae: 18.3485 - val_loss: 430.7379 - val_mse: 430.7379 - val_mae: 18.4001\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 427.8680 - mse: 427.8680 - mae: 18.3415 - val_loss: 430.5443 - val_mse: 430.5443 - val_mae: 18.3932\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 427.6620 - mse: 427.6620 - mae: 18.3348 - val_loss: 430.3490 - val_mse: 430.3490 - val_mae: 18.3862\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 427.4523 - mse: 427.4523 - mae: 18.3278 - val_loss: 430.1546 - val_mse: 430.1546 - val_mae: 18.3792\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 427.2498 - mse: 427.2498 - mae: 18.3211 - val_loss: 429.9594 - val_mse: 429.9594 - val_mae: 18.3723\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 427.0393 - mse: 427.0393 - mae: 18.3142 - val_loss: 429.7690 - val_mse: 429.7690 - val_mae: 18.3654\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 426.8360 - mse: 426.8360 - mae: 18.3075 - val_loss: 429.5781 - val_mse: 429.5781 - val_mae: 18.3586\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 426.6316 - mse: 426.6316 - mae: 18.3007 - val_loss: 429.3880 - val_mse: 429.3880 - val_mae: 18.3518\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.1, model__n_hidden=3, model__n_neurons=5, model__optimizer=adam; total time=   5.3s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 2553.3369 - mse: 2553.3369 - mae: 36.9652 - val_loss: 35.5281 - val_mse: 35.5281 - val_mae: 5.0365\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 80.2850 - mse: 80.2850 - mae: 6.2838 - val_loss: 24.6661 - val_mse: 24.6661 - val_mae: 4.1547\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 74.3006 - mse: 74.3006 - mae: 6.1215 - val_loss: 23.1316 - val_mse: 23.1316 - val_mae: 3.9943\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 72.9848 - mse: 72.9848 - mae: 6.1366 - val_loss: 29.1226 - val_mse: 29.1226 - val_mae: 4.3185\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 71.4967 - mse: 71.4967 - mae: 6.1203 - val_loss: 23.8868 - val_mse: 23.8868 - val_mae: 3.9655\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 70.2948 - mse: 70.2948 - mae: 6.0345 - val_loss: 21.6313 - val_mse: 21.6313 - val_mae: 3.8715\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 69.8530 - mse: 69.8530 - mae: 5.9858 - val_loss: 22.8934 - val_mse: 22.8934 - val_mae: 3.8922\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 69.6124 - mse: 69.6124 - mae: 5.9201 - val_loss: 29.8216 - val_mse: 29.8216 - val_mae: 4.4311\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 68.2308 - mse: 68.2308 - mae: 5.9102 - val_loss: 32.9505 - val_mse: 32.9505 - val_mae: 4.7068\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 68.0815 - mse: 68.0815 - mae: 6.0180 - val_loss: 25.2078 - val_mse: 25.2078 - val_mae: 4.0375\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 67.1145 - mse: 67.1145 - mae: 5.8598 - val_loss: 23.1667 - val_mse: 23.1667 - val_mae: 3.9098\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 67.6872 - mse: 67.6872 - mae: 5.8702 - val_loss: 22.6882 - val_mse: 22.6882 - val_mae: 3.8850\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 66.7725 - mse: 66.7725 - mae: 5.7643 - val_loss: 31.5153 - val_mse: 31.5153 - val_mae: 4.6849\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 68.3608 - mse: 68.3608 - mae: 6.0694 - val_loss: 21.4420 - val_mse: 21.4420 - val_mae: 3.7896\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 65.9822 - mse: 65.9822 - mae: 5.8146 - val_loss: 21.7770 - val_mse: 21.7770 - val_mae: 3.8339\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 66.3293 - mse: 66.3293 - mae: 5.8671 - val_loss: 20.2067 - val_mse: 20.2067 - val_mae: 3.6925\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 66.8148 - mse: 66.8148 - mae: 5.6955 - val_loss: 22.9709 - val_mse: 22.9709 - val_mae: 4.0025\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 66.2635 - mse: 66.2635 - mae: 5.8105 - val_loss: 22.0110 - val_mse: 22.0110 - val_mae: 3.8649\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 65.6288 - mse: 65.6288 - mae: 5.6891 - val_loss: 28.0126 - val_mse: 28.0126 - val_mae: 4.4221\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 66.1445 - mse: 66.1445 - mae: 5.7877 - val_loss: 29.3744 - val_mse: 29.3744 - val_mae: 4.5644\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 65.2083 - mse: 65.2083 - mae: 5.8462 - val_loss: 22.0471 - val_mse: 22.0471 - val_mae: 3.9498\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 66.0658 - mse: 66.0658 - mae: 5.7971 - val_loss: 20.7543 - val_mse: 20.7543 - val_mae: 3.7894\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 65.1877 - mse: 65.1877 - mae: 5.6569 - val_loss: 25.7021 - val_mse: 25.7021 - val_mae: 4.2860\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 65.1962 - mse: 65.1962 - mae: 5.8225 - val_loss: 24.0048 - val_mse: 24.0048 - val_mae: 4.1287\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 65.0893 - mse: 65.0893 - mae: 5.8066 - val_loss: 21.6267 - val_mse: 21.6267 - val_mae: 3.8611\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 64.4959 - mse: 64.4959 - mae: 5.6429 - val_loss: 25.9162 - val_mse: 25.9162 - val_mae: 4.3204\n",
      "Epoch 26: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.1, model__n_hidden=2, model__n_neurons=25, model__optimizer=nesterov; total time=   1.8s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 177804.5469 - mse: 177804.5469 - mae: 191.7302 - val_loss: 450.0592 - val_mse: 450.0592 - val_mae: 20.2965\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 638.0620 - mse: 638.0620 - mae: 23.1386 - val_loss: 449.7831 - val_mse: 449.7831 - val_mae: 20.2897\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 637.8204 - mse: 637.8204 - mae: 23.1335 - val_loss: 449.6291 - val_mse: 449.6291 - val_mae: 20.2859\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 637.6448 - mse: 637.6448 - mae: 23.1296 - val_loss: 449.4753 - val_mse: 449.4753 - val_mae: 20.2821\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 637.4697 - mse: 637.4697 - mae: 23.1259 - val_loss: 449.3225 - val_mse: 449.3225 - val_mae: 20.2783\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 637.2955 - mse: 637.2955 - mae: 23.1220 - val_loss: 449.1703 - val_mse: 449.1703 - val_mae: 20.2746\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 637.1220 - mse: 637.1220 - mae: 23.1183 - val_loss: 449.0184 - val_mse: 449.0184 - val_mae: 20.2708\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 636.9487 - mse: 636.9487 - mae: 23.1147 - val_loss: 448.8656 - val_mse: 448.8656 - val_mae: 20.2670\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 636.7743 - mse: 636.7743 - mae: 23.1108 - val_loss: 448.7117 - val_mse: 448.7117 - val_mae: 20.2632\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 636.5989 - mse: 636.5989 - mae: 23.1070 - val_loss: 448.5582 - val_mse: 448.5582 - val_mae: 20.2594\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 636.4238 - mse: 636.4238 - mae: 23.1032 - val_loss: 448.4062 - val_mse: 448.4062 - val_mae: 20.2557\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 636.2504 - mse: 636.2504 - mae: 23.0995 - val_loss: 448.2530 - val_mse: 448.2530 - val_mae: 20.2519\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 636.0757 - mse: 636.0757 - mae: 23.0957 - val_loss: 448.1000 - val_mse: 448.1000 - val_mae: 20.2481\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 635.9015 - mse: 635.9015 - mae: 23.0920 - val_loss: 447.9479 - val_mse: 447.9479 - val_mae: 20.2444\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 635.7279 - mse: 635.7279 - mae: 23.0882 - val_loss: 447.7947 - val_mse: 447.7947 - val_mae: 20.2406\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 635.5533 - mse: 635.5533 - mae: 23.0844 - val_loss: 447.6423 - val_mse: 447.6423 - val_mae: 20.2368\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 635.3794 - mse: 635.3794 - mae: 23.0807 - val_loss: 447.4887 - val_mse: 447.4887 - val_mae: 20.2330\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 635.2043 - mse: 635.2043 - mae: 23.0769 - val_loss: 447.3366 - val_mse: 447.3366 - val_mae: 20.2293\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 635.0308 - mse: 635.0308 - mae: 23.0732 - val_loss: 447.1836 - val_mse: 447.1836 - val_mae: 20.2255\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 634.8562 - mse: 634.8562 - mae: 23.0693 - val_loss: 447.0311 - val_mse: 447.0311 - val_mae: 20.2217\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 634.6823 - mse: 634.6823 - mae: 23.0655 - val_loss: 446.8789 - val_mse: 446.8789 - val_mae: 20.2180\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 634.5089 - mse: 634.5089 - mae: 23.0618 - val_loss: 446.7273 - val_mse: 446.7273 - val_mae: 20.2142\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 634.3358 - mse: 634.3358 - mae: 23.0581 - val_loss: 446.5748 - val_mse: 446.5748 - val_mae: 20.2104\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 634.1617 - mse: 634.1617 - mae: 23.0543 - val_loss: 446.4211 - val_mse: 446.4211 - val_mae: 20.2066\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 633.9866 - mse: 633.9866 - mae: 23.0504 - val_loss: 446.2693 - val_mse: 446.2693 - val_mae: 20.2029\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 633.8134 - mse: 633.8134 - mae: 23.0467 - val_loss: 446.1168 - val_mse: 446.1168 - val_mae: 20.1991\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 633.6397 - mse: 633.6397 - mae: 23.0429 - val_loss: 445.9668 - val_mse: 445.9668 - val_mae: 20.1954\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 633.4685 - mse: 633.4685 - mae: 23.0392 - val_loss: 445.8156 - val_mse: 445.8156 - val_mae: 20.1917\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 633.2959 - mse: 633.2959 - mae: 23.0355 - val_loss: 445.6643 - val_mse: 445.6643 - val_mae: 20.1879\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 633.1232 - mse: 633.1232 - mae: 23.0317 - val_loss: 445.5125 - val_mse: 445.5125 - val_mae: 20.1841\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 632.9504 - mse: 632.9504 - mae: 23.0280 - val_loss: 445.3624 - val_mse: 445.3624 - val_mae: 20.1804\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 632.7788 - mse: 632.7788 - mae: 23.0243 - val_loss: 445.2094 - val_mse: 445.2094 - val_mae: 20.1766\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 632.6041 - mse: 632.6041 - mae: 23.0204 - val_loss: 445.0575 - val_mse: 445.0575 - val_mae: 20.1729\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 632.4310 - mse: 632.4310 - mae: 23.0167 - val_loss: 444.9060 - val_mse: 444.9060 - val_mae: 20.1691\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 632.2580 - mse: 632.2580 - mae: 23.0130 - val_loss: 444.7535 - val_mse: 444.7535 - val_mae: 20.1653\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 632.0840 - mse: 632.0840 - mae: 23.0091 - val_loss: 444.6017 - val_mse: 444.6017 - val_mae: 20.1616\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 631.9109 - mse: 631.9109 - mae: 23.0054 - val_loss: 444.4512 - val_mse: 444.4512 - val_mae: 20.1578\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 631.7394 - mse: 631.7394 - mae: 23.0017 - val_loss: 444.3008 - val_mse: 444.3008 - val_mae: 20.1541\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 631.5676 - mse: 631.5676 - mae: 22.9979 - val_loss: 444.1495 - val_mse: 444.1495 - val_mae: 20.1504\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 631.3949 - mse: 631.3949 - mae: 22.9942 - val_loss: 443.9979 - val_mse: 443.9979 - val_mae: 20.1466\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 631.2221 - mse: 631.2221 - mae: 22.9904 - val_loss: 443.8468 - val_mse: 443.8468 - val_mae: 20.1428\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 631.0496 - mse: 631.0496 - mae: 22.9866 - val_loss: 443.6959 - val_mse: 443.6959 - val_mae: 20.1391\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 630.8773 - mse: 630.8773 - mae: 22.9829 - val_loss: 443.5448 - val_mse: 443.5448 - val_mae: 20.1353\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 630.7048 - mse: 630.7048 - mae: 22.9792 - val_loss: 443.3932 - val_mse: 443.3932 - val_mae: 20.1316\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 630.5321 - mse: 630.5321 - mae: 22.9753 - val_loss: 443.2425 - val_mse: 443.2425 - val_mae: 20.1278\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 630.3600 - mse: 630.3600 - mae: 22.9716 - val_loss: 443.0925 - val_mse: 443.0925 - val_mae: 20.1241\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 630.1888 - mse: 630.1888 - mae: 22.9679 - val_loss: 442.9428 - val_mse: 442.9428 - val_mae: 20.1204\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 630.0179 - mse: 630.0179 - mae: 22.9642 - val_loss: 442.7913 - val_mse: 442.7913 - val_mae: 20.1166\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 629.8450 - mse: 629.8450 - mae: 22.9605 - val_loss: 442.6392 - val_mse: 442.6392 - val_mae: 20.1128\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 629.6715 - mse: 629.6715 - mae: 22.9567 - val_loss: 442.4894 - val_mse: 442.4894 - val_mae: 20.1091\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 629.5004 - mse: 629.5004 - mae: 22.9529 - val_loss: 442.3385 - val_mse: 442.3385 - val_mae: 20.1054\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 629.3283 - mse: 629.3283 - mae: 22.9492 - val_loss: 442.1878 - val_mse: 442.1878 - val_mae: 20.1016\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 629.1562 - mse: 629.1562 - mae: 22.9454 - val_loss: 442.0370 - val_mse: 442.0370 - val_mae: 20.0979\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 628.9843 - mse: 628.9843 - mae: 22.9417 - val_loss: 441.8877 - val_mse: 441.8877 - val_mae: 20.0942\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 628.8135 - mse: 628.8135 - mae: 22.9380 - val_loss: 441.7362 - val_mse: 441.7362 - val_mae: 20.0904\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 628.6407 - mse: 628.6407 - mae: 22.9342 - val_loss: 441.5851 - val_mse: 441.5851 - val_mae: 20.0866\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 628.4684 - mse: 628.4684 - mae: 22.9305 - val_loss: 441.4349 - val_mse: 441.4349 - val_mae: 20.0829\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 628.2968 - mse: 628.2968 - mae: 22.9267 - val_loss: 441.2848 - val_mse: 441.2848 - val_mae: 20.0791\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 628.1253 - mse: 628.1253 - mae: 22.9230 - val_loss: 441.1332 - val_mse: 441.1332 - val_mae: 20.0754\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 627.9523 - mse: 627.9523 - mae: 22.9192 - val_loss: 440.9821 - val_mse: 440.9821 - val_mae: 20.0716\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 627.7800 - mse: 627.7800 - mae: 22.9154 - val_loss: 440.8324 - val_mse: 440.8324 - val_mae: 20.0679\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 627.6090 - mse: 627.6090 - mae: 22.9117 - val_loss: 440.6828 - val_mse: 440.6828 - val_mae: 20.0641\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 627.4381 - mse: 627.4381 - mae: 22.9079 - val_loss: 440.5329 - val_mse: 440.5329 - val_mae: 20.0604\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 627.2670 - mse: 627.2670 - mae: 22.9042 - val_loss: 440.3829 - val_mse: 440.3829 - val_mae: 20.0567\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 627.0959 - mse: 627.0959 - mae: 22.9005 - val_loss: 440.2338 - val_mse: 440.2338 - val_mae: 20.0530\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 626.9254 - mse: 626.9254 - mae: 22.8968 - val_loss: 440.0832 - val_mse: 440.0832 - val_mae: 20.0492\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 626.7535 - mse: 626.7535 - mae: 22.8930 - val_loss: 439.9344 - val_mse: 439.9344 - val_mae: 20.0455\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 626.5836 - mse: 626.5836 - mae: 22.8893 - val_loss: 439.7834 - val_mse: 439.7834 - val_mae: 20.0417\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 626.4112 - mse: 626.4112 - mae: 22.8855 - val_loss: 439.6335 - val_mse: 439.6335 - val_mae: 20.0380\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 626.2401 - mse: 626.2401 - mae: 22.8818 - val_loss: 439.4835 - val_mse: 439.4835 - val_mae: 20.0342\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 626.0690 - mse: 626.0690 - mae: 22.8780 - val_loss: 439.3350 - val_mse: 439.3350 - val_mae: 20.0305\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 625.8992 - mse: 625.8992 - mae: 22.8744 - val_loss: 439.1848 - val_mse: 439.1848 - val_mae: 20.0268\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 625.7277 - mse: 625.7277 - mae: 22.8706 - val_loss: 439.0346 - val_mse: 439.0346 - val_mae: 20.0230\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 625.5561 - mse: 625.5561 - mae: 22.8669 - val_loss: 438.8840 - val_mse: 438.8840 - val_mae: 20.0193\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 625.3842 - mse: 625.3842 - mae: 22.8631 - val_loss: 438.7339 - val_mse: 438.7339 - val_mae: 20.0155\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 625.2129 - mse: 625.2129 - mae: 22.8594 - val_loss: 438.5851 - val_mse: 438.5851 - val_mae: 20.0118\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 625.0427 - mse: 625.0427 - mae: 22.8556 - val_loss: 438.4339 - val_mse: 438.4339 - val_mae: 20.0080\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 624.8702 - mse: 624.8702 - mae: 22.8518 - val_loss: 438.2856 - val_mse: 438.2856 - val_mae: 20.0043\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 624.7007 - mse: 624.7007 - mae: 22.8481 - val_loss: 438.1361 - val_mse: 438.1361 - val_mae: 20.0006\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 624.5300 - mse: 624.5300 - mae: 22.8444 - val_loss: 437.9861 - val_mse: 437.9861 - val_mae: 19.9968\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 624.3589 - mse: 624.3589 - mae: 22.8406 - val_loss: 437.8379 - val_mse: 437.8379 - val_mae: 19.9931\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 624.1895 - mse: 624.1895 - mae: 22.8370 - val_loss: 437.6887 - val_mse: 437.6887 - val_mae: 19.9894\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 624.0189 - mse: 624.0189 - mae: 22.8332 - val_loss: 437.5383 - val_mse: 437.5383 - val_mae: 19.9856\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 623.8472 - mse: 623.8472 - mae: 22.8294 - val_loss: 437.3895 - val_mse: 437.3895 - val_mae: 19.9819\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 623.6772 - mse: 623.6772 - mae: 22.8257 - val_loss: 437.2403 - val_mse: 437.2403 - val_mae: 19.9782\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 623.5068 - mse: 623.5068 - mae: 22.8220 - val_loss: 437.0916 - val_mse: 437.0916 - val_mae: 19.9745\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 623.3369 - mse: 623.3369 - mae: 22.8183 - val_loss: 436.9416 - val_mse: 436.9416 - val_mae: 19.9707\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 623.1655 - mse: 623.1655 - mae: 22.8146 - val_loss: 436.7912 - val_mse: 436.7912 - val_mae: 19.9669\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 622.9937 - mse: 622.9937 - mae: 22.8107 - val_loss: 436.6416 - val_mse: 436.6416 - val_mae: 19.9632\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 622.8229 - mse: 622.8229 - mae: 22.8070 - val_loss: 436.4934 - val_mse: 436.4934 - val_mae: 19.9595\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 622.6536 - mse: 622.6536 - mae: 22.8033 - val_loss: 436.3440 - val_mse: 436.3440 - val_mae: 19.9557\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 622.4830 - mse: 622.4830 - mae: 22.7995 - val_loss: 436.1952 - val_mse: 436.1952 - val_mae: 19.9520\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 622.3127 - mse: 622.3127 - mae: 22.7959 - val_loss: 436.0449 - val_mse: 436.0449 - val_mae: 19.9482\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 622.1412 - mse: 622.1412 - mae: 22.7921 - val_loss: 435.8963 - val_mse: 435.8963 - val_mae: 19.9445\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 621.9713 - mse: 621.9713 - mae: 22.7883 - val_loss: 435.7478 - val_mse: 435.7478 - val_mae: 19.9408\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 621.8016 - mse: 621.8016 - mae: 22.7846 - val_loss: 435.5991 - val_mse: 435.5991 - val_mae: 19.9371\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 621.6317 - mse: 621.6317 - mae: 22.7809 - val_loss: 435.4490 - val_mse: 435.4490 - val_mae: 19.9333\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 621.4603 - mse: 621.4603 - mae: 22.7771 - val_loss: 435.3006 - val_mse: 435.3006 - val_mae: 19.9296\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 621.2906 - mse: 621.2906 - mae: 22.7734 - val_loss: 435.1515 - val_mse: 435.1515 - val_mae: 19.9258\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 621.1204 - mse: 621.1204 - mae: 22.7697 - val_loss: 435.0038 - val_mse: 435.0038 - val_mae: 19.9221\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.1, model__n_hidden=2, model__n_neurons=25, model__optimizer=nesterov; total time=   5.4s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 9917.9395 - mse: 9917.9395 - mae: 56.8133 - val_loss: 479.3694 - val_mse: 479.3694 - val_mae: 20.2155\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 429.9069 - mse: 429.9069 - mae: 18.4790 - val_loss: 163.9184 - val_mse: 163.9184 - val_mae: 11.2993\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 84.9221 - mse: 84.9221 - mae: 6.8554 - val_loss: 41.4937 - val_mse: 41.4937 - val_mae: 5.2990\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 55.9130 - mse: 55.9130 - mae: 5.3148 - val_loss: 37.3891 - val_mse: 37.3891 - val_mae: 5.0684\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 54.0772 - mse: 54.0772 - mae: 5.2231 - val_loss: 35.8663 - val_mse: 35.8663 - val_mae: 4.7356\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 51.4792 - mse: 51.4792 - mae: 4.9222 - val_loss: 34.2472 - val_mse: 34.2472 - val_mae: 4.6804\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 52.1642 - mse: 52.1642 - mae: 5.0948 - val_loss: 33.8714 - val_mse: 33.8714 - val_mae: 4.5932\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 51.2776 - mse: 51.2776 - mae: 5.1213 - val_loss: 34.9944 - val_mse: 34.9944 - val_mae: 4.5429\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 49.0658 - mse: 49.0658 - mae: 4.7717 - val_loss: 34.0665 - val_mse: 34.0665 - val_mae: 4.5600\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 49.6109 - mse: 49.6109 - mae: 4.9724 - val_loss: 33.8282 - val_mse: 33.8282 - val_mae: 4.5539\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 49.1267 - mse: 49.1267 - mae: 4.9159 - val_loss: 34.5427 - val_mse: 34.5427 - val_mae: 4.5078\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 49.4606 - mse: 49.4606 - mae: 4.8480 - val_loss: 34.7154 - val_mse: 34.7154 - val_mae: 4.5307\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 49.6240 - mse: 49.6240 - mae: 4.8493 - val_loss: 34.1224 - val_mse: 34.1224 - val_mae: 4.5066\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 49.4408 - mse: 49.4408 - mae: 4.8340 - val_loss: 34.1941 - val_mse: 34.1941 - val_mae: 4.4618\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 48.8016 - mse: 48.8016 - mae: 4.8257 - val_loss: 34.8420 - val_mse: 34.8420 - val_mae: 4.4829\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 49.2870 - mse: 49.2870 - mae: 4.7684 - val_loss: 35.4888 - val_mse: 35.4888 - val_mae: 4.5840\n",
      "Epoch 16: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.1, model__n_hidden=2, model__n_neurons=25, model__optimizer=nesterov; total time=   1.3s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 22ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 10: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.5, model__n_hidden=3, model__n_neurons=125, model__optimizer=momentum; total time=   1.5s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 22ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 10: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.5, model__n_hidden=3, model__n_neurons=125, model__optimizer=momentum; total time=   1.1s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 21ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 10: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.5, model__n_hidden=3, model__n_neurons=125, model__optimizer=momentum; total time=   1.1s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 21ms/step - loss: 179.4205 - mse: 179.4205 - mae: 10.3882 - val_loss: 53.5326 - val_mse: 53.5326 - val_mae: 5.7475\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 129.1325 - mse: 129.1325 - mae: 8.4878 - val_loss: 42.2165 - val_mse: 42.2165 - val_mae: 4.8773\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 111.2439 - mse: 111.2439 - mae: 7.6611 - val_loss: 37.7979 - val_mse: 37.7979 - val_mae: 4.3768\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 102.0468 - mse: 102.0468 - mae: 7.2016 - val_loss: 35.3617 - val_mse: 35.3617 - val_mae: 4.2434\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 96.4300 - mse: 96.4300 - mae: 6.9260 - val_loss: 33.7298 - val_mse: 33.7298 - val_mae: 4.2934\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 92.3552 - mse: 92.3552 - mae: 6.7522 - val_loss: 32.1751 - val_mse: 32.1751 - val_mae: 4.2970\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 89.5410 - mse: 89.5410 - mae: 6.6410 - val_loss: 30.8086 - val_mse: 30.8086 - val_mae: 4.2714\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 86.9442 - mse: 86.9442 - mae: 6.5345 - val_loss: 29.5814 - val_mse: 29.5814 - val_mae: 4.2299\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 85.0095 - mse: 85.0095 - mae: 6.4542 - val_loss: 28.6135 - val_mse: 28.6135 - val_mae: 4.1892\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 83.3006 - mse: 83.3006 - mae: 6.4030 - val_loss: 27.9811 - val_mse: 27.9811 - val_mae: 4.1603\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 81.9204 - mse: 81.9204 - mae: 6.3601 - val_loss: 27.5310 - val_mse: 27.5310 - val_mae: 4.1485\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 80.8521 - mse: 80.8521 - mae: 6.3252 - val_loss: 27.1938 - val_mse: 27.1938 - val_mae: 4.1496\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 79.8615 - mse: 79.8615 - mae: 6.3017 - val_loss: 26.8284 - val_mse: 26.8284 - val_mae: 4.1311\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 79.1026 - mse: 79.1026 - mae: 6.2902 - val_loss: 26.6698 - val_mse: 26.6698 - val_mae: 4.1314\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 78.4977 - mse: 78.4977 - mae: 6.2751 - val_loss: 26.4759 - val_mse: 26.4759 - val_mae: 4.1159\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 78.0412 - mse: 78.0412 - mae: 6.2709 - val_loss: 26.4136 - val_mse: 26.4136 - val_mae: 4.1208\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 77.5157 - mse: 77.5157 - mae: 6.2500 - val_loss: 26.3096 - val_mse: 26.3096 - val_mae: 4.1123\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 77.0875 - mse: 77.0875 - mae: 6.2385 - val_loss: 26.3406 - val_mse: 26.3406 - val_mae: 4.1221\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 76.7374 - mse: 76.7374 - mae: 6.2195 - val_loss: 26.2661 - val_mse: 26.2661 - val_mae: 4.1088\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 76.3801 - mse: 76.3801 - mae: 6.2150 - val_loss: 26.2152 - val_mse: 26.2152 - val_mae: 4.0989\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 76.0349 - mse: 76.0349 - mae: 6.2182 - val_loss: 26.1430 - val_mse: 26.1430 - val_mae: 4.0937\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 75.8827 - mse: 75.8827 - mae: 6.2120 - val_loss: 26.1686 - val_mse: 26.1686 - val_mae: 4.0999\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 75.6740 - mse: 75.6740 - mae: 6.1963 - val_loss: 26.1653 - val_mse: 26.1653 - val_mae: 4.0996\n",
      "Epoch 23: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.5, model__n_hidden=2, model__n_neurons=5, model__optimizer=sgd; total time=   1.6s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 6053.2583 - mse: 6053.2583 - mae: 41.8899 - val_loss: 1248.0961 - val_mse: 1248.0961 - val_mae: 23.3459\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 861.0980 - mse: 861.0980 - mae: 20.3401 - val_loss: 628.8769 - val_mse: 628.8769 - val_mae: 17.8586\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 571.4662 - mse: 571.4662 - mae: 18.1203 - val_loss: 416.5993 - val_mse: 416.5993 - val_mae: 15.2131\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 429.7854 - mse: 429.7854 - mae: 16.0561 - val_loss: 288.3078 - val_mse: 288.3078 - val_mae: 13.0084\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 338.2863 - mse: 338.2863 - mae: 14.2782 - val_loss: 206.3475 - val_mse: 206.3475 - val_mae: 11.1197\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 272.4745 - mse: 272.4745 - mae: 12.5980 - val_loss: 150.2476 - val_mse: 150.2476 - val_mae: 9.4849\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 221.5655 - mse: 221.5655 - mae: 11.2378 - val_loss: 112.1993 - val_mse: 112.1993 - val_mae: 8.1571\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 186.3891 - mse: 186.3891 - mae: 10.0992 - val_loss: 89.4015 - val_mse: 89.4015 - val_mae: 7.2743\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 160.1642 - mse: 160.1642 - mae: 9.4323 - val_loss: 67.3567 - val_mse: 67.3567 - val_mae: 6.1956\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 142.3895 - mse: 142.3895 - mae: 8.8457 - val_loss: 54.3059 - val_mse: 54.3059 - val_mae: 5.5409\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 129.7320 - mse: 129.7320 - mae: 8.5053 - val_loss: 45.1517 - val_mse: 45.1517 - val_mae: 5.0693\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 122.1807 - mse: 122.1807 - mae: 8.2049 - val_loss: 43.4277 - val_mse: 43.4277 - val_mae: 5.1855\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 116.2880 - mse: 116.2880 - mae: 8.0975 - val_loss: 41.3764 - val_mse: 41.3764 - val_mae: 5.2050\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 112.6799 - mse: 112.6799 - mae: 8.0305 - val_loss: 37.4088 - val_mse: 37.4088 - val_mae: 5.0140\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 109.3014 - mse: 109.3014 - mae: 7.8113 - val_loss: 36.8942 - val_mse: 36.8942 - val_mae: 5.0525\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 106.7377 - mse: 106.7377 - mae: 7.8255 - val_loss: 33.6844 - val_mse: 33.6844 - val_mae: 4.8318\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 105.6104 - mse: 105.6104 - mae: 7.7025 - val_loss: 34.1859 - val_mse: 34.1859 - val_mae: 4.9245\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 104.0685 - mse: 104.0685 - mae: 7.7027 - val_loss: 31.2339 - val_mse: 31.2339 - val_mae: 4.6747\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 102.7271 - mse: 102.7271 - mae: 7.4859 - val_loss: 33.5997 - val_mse: 33.5997 - val_mae: 4.9184\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 102.3184 - mse: 102.3184 - mae: 7.5923 - val_loss: 33.9835 - val_mse: 33.9835 - val_mae: 4.9569\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 100.8953 - mse: 100.8953 - mae: 7.6529 - val_loss: 29.3434 - val_mse: 29.3434 - val_mae: 4.5467\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 101.0532 - mse: 101.0532 - mae: 7.5456 - val_loss: 27.9795 - val_mse: 27.9795 - val_mae: 4.4322\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 100.1021 - mse: 100.1021 - mae: 7.4401 - val_loss: 29.9336 - val_mse: 29.9336 - val_mae: 4.6065\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 100.0398 - mse: 100.0398 - mae: 7.4636 - val_loss: 33.0468 - val_mse: 33.0468 - val_mae: 4.9057\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 99.2260 - mse: 99.2260 - mae: 7.6193 - val_loss: 28.5114 - val_mse: 28.5114 - val_mae: 4.4902\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 99.1049 - mse: 99.1049 - mae: 7.4644 - val_loss: 30.4129 - val_mse: 30.4129 - val_mae: 4.6577\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 98.7847 - mse: 98.7847 - mae: 7.4905 - val_loss: 27.4016 - val_mse: 27.4016 - val_mae: 4.3903\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 98.5490 - mse: 98.5490 - mae: 7.3851 - val_loss: 29.0304 - val_mse: 29.0304 - val_mae: 4.5424\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 97.9396 - mse: 97.9396 - mae: 7.4151 - val_loss: 28.8582 - val_mse: 28.8582 - val_mae: 4.5227\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 98.0852 - mse: 98.0852 - mae: 7.4622 - val_loss: 28.3074 - val_mse: 28.3074 - val_mae: 4.4705\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 97.5937 - mse: 97.5937 - mae: 7.3785 - val_loss: 28.5125 - val_mse: 28.5125 - val_mae: 4.4774\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 97.8746 - mse: 97.8746 - mae: 7.4095 - val_loss: 30.3201 - val_mse: 30.3201 - val_mae: 4.6540\n",
      "Epoch 32: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.5, model__n_hidden=2, model__n_neurons=5, model__optimizer=sgd; total time=   2.1s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 752.4647 - mse: 752.4647 - mae: 25.2728 - val_loss: 675.4958 - val_mse: 675.4958 - val_mae: 23.7188\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 659.3267 - mse: 659.3267 - mae: 23.7971 - val_loss: 620.6649 - val_mse: 620.6649 - val_mae: 22.9395\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 630.2601 - mse: 630.2601 - mae: 23.3318 - val_loss: 596.0165 - val_mse: 596.0165 - val_mae: 22.5681\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 613.8297 - mse: 613.8297 - mae: 23.0615 - val_loss: 583.2458 - val_mse: 583.2458 - val_mae: 22.3791\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 604.7733 - mse: 604.7733 - mae: 22.9055 - val_loss: 576.2922 - val_mse: 576.2922 - val_mae: 22.2798\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 598.7928 - mse: 598.7928 - mae: 22.8014 - val_loss: 571.1530 - val_mse: 571.1530 - val_mae: 22.2047\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 594.0516 - mse: 594.0516 - mae: 22.7163 - val_loss: 566.2417 - val_mse: 566.2417 - val_mae: 22.1321\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 589.7198 - mse: 589.7198 - mae: 22.6349 - val_loss: 562.5074 - val_mse: 562.5074 - val_mae: 22.0774\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 586.2295 - mse: 586.2295 - mae: 22.5659 - val_loss: 561.1270 - val_mse: 561.1270 - val_mae: 22.0560\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 583.1018 - mse: 583.1018 - mae: 22.4967 - val_loss: 559.6724 - val_mse: 559.6724 - val_mae: 22.0334\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 580.2646 - mse: 580.2646 - mae: 22.4331 - val_loss: 558.5923 - val_mse: 558.5923 - val_mae: 22.0194\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 577.4848 - mse: 577.4848 - mae: 22.3706 - val_loss: 557.4636 - val_mse: 557.4636 - val_mae: 22.0056\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 574.9693 - mse: 574.9693 - mae: 22.3107 - val_loss: 556.2469 - val_mse: 556.2469 - val_mae: 21.9904\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 572.8027 - mse: 572.8027 - mae: 22.2568 - val_loss: 555.2620 - val_mse: 555.2620 - val_mae: 21.9782\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 570.8694 - mse: 570.8694 - mae: 22.2055 - val_loss: 554.2860 - val_mse: 554.2860 - val_mae: 21.9658\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 569.0723 - mse: 569.0723 - mae: 22.1566 - val_loss: 553.4243 - val_mse: 553.4243 - val_mae: 21.9548\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 567.3931 - mse: 567.3931 - mae: 22.1086 - val_loss: 552.6982 - val_mse: 552.6982 - val_mae: 21.9455\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 565.8229 - mse: 565.8229 - mae: 22.0628 - val_loss: 552.2529 - val_mse: 552.2529 - val_mae: 21.9400\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 564.2159 - mse: 564.2159 - mae: 22.0116 - val_loss: 552.2779 - val_mse: 552.2779 - val_mae: 21.9407\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 562.7629 - mse: 562.7629 - mae: 21.9642 - val_loss: 552.1353 - val_mse: 552.1353 - val_mae: 21.9370\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 561.5638 - mse: 561.5638 - mae: 21.9233 - val_loss: 551.7521 - val_mse: 551.7521 - val_mae: 21.9270\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 560.3046 - mse: 560.3046 - mae: 21.8820 - val_loss: 551.3722 - val_mse: 551.3722 - val_mae: 21.9170\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 559.1480 - mse: 559.1480 - mae: 21.8409 - val_loss: 551.0552 - val_mse: 551.0552 - val_mae: 21.9085\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 558.0352 - mse: 558.0352 - mae: 21.8029 - val_loss: 550.7446 - val_mse: 550.7446 - val_mae: 21.9001\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 557.0885 - mse: 557.0885 - mae: 21.7673 - val_loss: 550.4800 - val_mse: 550.4800 - val_mae: 21.8929\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 556.1971 - mse: 556.1970 - mae: 21.7337 - val_loss: 550.1255 - val_mse: 550.1255 - val_mae: 21.8831\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 555.2830 - mse: 555.2830 - mae: 21.6952 - val_loss: 549.7902 - val_mse: 549.7902 - val_mae: 21.8737\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 554.4277 - mse: 554.4277 - mae: 21.6601 - val_loss: 549.5076 - val_mse: 549.5076 - val_mae: 21.8657\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 553.6678 - mse: 553.6678 - mae: 21.6267 - val_loss: 549.3241 - val_mse: 549.3241 - val_mae: 21.8605\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 552.9108 - mse: 552.9108 - mae: 21.5962 - val_loss: 549.0723 - val_mse: 549.0723 - val_mae: 21.8532\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 552.2020 - mse: 552.2020 - mae: 21.5605 - val_loss: 548.8365 - val_mse: 548.8365 - val_mae: 21.8462\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 551.5231 - mse: 551.5231 - mae: 21.5304 - val_loss: 548.6031 - val_mse: 548.6031 - val_mae: 21.8392\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 550.9332 - mse: 550.9332 - mae: 21.5017 - val_loss: 548.4154 - val_mse: 548.4154 - val_mae: 21.8336\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 550.3836 - mse: 550.3836 - mae: 21.4730 - val_loss: 548.2330 - val_mse: 548.2330 - val_mae: 21.8280\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 549.7900 - mse: 549.7900 - mae: 21.4447 - val_loss: 548.0298 - val_mse: 548.0298 - val_mae: 21.8217\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 549.3018 - mse: 549.3018 - mae: 21.4161 - val_loss: 547.8510 - val_mse: 547.8510 - val_mae: 21.8161\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 548.7381 - mse: 548.7381 - mae: 21.3895 - val_loss: 547.6267 - val_mse: 547.6267 - val_mae: 21.8089\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 548.2522 - mse: 548.2522 - mae: 21.3563 - val_loss: 547.4454 - val_mse: 547.4454 - val_mae: 21.8031\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 547.7933 - mse: 547.7933 - mae: 21.3323 - val_loss: 547.2592 - val_mse: 547.2592 - val_mae: 21.7969\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 547.3257 - mse: 547.3257 - mae: 21.3026 - val_loss: 547.0549 - val_mse: 547.0549 - val_mae: 21.7901\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 546.9067 - mse: 546.9067 - mae: 21.2760 - val_loss: 546.8573 - val_mse: 546.8573 - val_mae: 21.7834\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 546.4857 - mse: 546.4857 - mae: 21.2484 - val_loss: 546.6628 - val_mse: 546.6628 - val_mae: 21.7768\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 546.1182 - mse: 546.1182 - mae: 21.2246 - val_loss: 546.4811 - val_mse: 546.4811 - val_mae: 21.7704\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 545.7722 - mse: 545.7722 - mae: 21.2046 - val_loss: 546.3063 - val_mse: 546.3063 - val_mae: 21.7643\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 545.4614 - mse: 545.4614 - mae: 21.1859 - val_loss: 546.1328 - val_mse: 546.1328 - val_mae: 21.7581\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 545.1288 - mse: 545.1288 - mae: 21.1657 - val_loss: 545.9453 - val_mse: 545.9453 - val_mae: 21.7513\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 544.8060 - mse: 544.8060 - mae: 21.1475 - val_loss: 545.7743 - val_mse: 545.7743 - val_mae: 21.7449\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 544.5198 - mse: 544.5198 - mae: 21.1306 - val_loss: 545.6106 - val_mse: 545.6106 - val_mae: 21.7387\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 544.2562 - mse: 544.2562 - mae: 21.1164 - val_loss: 545.4136 - val_mse: 545.4136 - val_mae: 21.7314\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 543.9808 - mse: 543.9808 - mae: 21.1007 - val_loss: 545.0495 - val_mse: 545.0495 - val_mae: 21.7196\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 543.7258 - mse: 543.7258 - mae: 21.0860 - val_loss: 544.5402 - val_mse: 544.5402 - val_mae: 21.7030\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 543.4019 - mse: 543.4019 - mae: 21.0689 - val_loss: 544.0750 - val_mse: 544.0750 - val_mae: 21.6876\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 543.1054 - mse: 543.1054 - mae: 21.0554 - val_loss: 543.6417 - val_mse: 543.6417 - val_mae: 21.6732\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 542.8120 - mse: 542.8120 - mae: 21.0416 - val_loss: 543.1265 - val_mse: 543.1265 - val_mae: 21.6557\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 542.4450 - mse: 542.4450 - mae: 21.0270 - val_loss: 542.6450 - val_mse: 542.6450 - val_mae: 21.6392\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 542.1284 - mse: 542.1284 - mae: 21.0115 - val_loss: 542.2085 - val_mse: 542.2085 - val_mae: 21.6240\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 541.8406 - mse: 541.8406 - mae: 20.9983 - val_loss: 541.7802 - val_mse: 541.7802 - val_mae: 21.6088\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 541.5699 - mse: 541.5700 - mae: 20.9861 - val_loss: 541.3785 - val_mse: 541.3785 - val_mae: 21.5945\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 541.3123 - mse: 541.3123 - mae: 20.9787 - val_loss: 540.9393 - val_mse: 540.9393 - val_mae: 21.5786\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 540.9698 - mse: 540.9698 - mae: 20.9685 - val_loss: 540.4567 - val_mse: 540.4567 - val_mae: 21.5608\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 540.6123 - mse: 540.6123 - mae: 20.9581 - val_loss: 540.0377 - val_mse: 540.0377 - val_mae: 21.5452\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 540.3181 - mse: 540.3181 - mae: 20.9512 - val_loss: 539.6305 - val_mse: 539.6305 - val_mae: 21.5297\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 540.0267 - mse: 540.0267 - mae: 20.9430 - val_loss: 539.2502 - val_mse: 539.2502 - val_mae: 21.5151\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 539.7920 - mse: 539.7920 - mae: 20.9356 - val_loss: 538.9320 - val_mse: 538.9320 - val_mae: 21.5028\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 539.5740 - mse: 539.5740 - mae: 20.9306 - val_loss: 538.5841 - val_mse: 538.5841 - val_mae: 21.4891\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 539.2981 - mse: 539.2981 - mae: 20.9212 - val_loss: 538.2209 - val_mse: 538.2209 - val_mae: 21.4746\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 539.0214 - mse: 539.0214 - mae: 20.9141 - val_loss: 537.8969 - val_mse: 537.8969 - val_mae: 21.4615\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 538.7513 - mse: 538.7513 - mae: 20.9083 - val_loss: 537.5016 - val_mse: 537.5016 - val_mae: 21.4452\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 538.4557 - mse: 538.4557 - mae: 20.9030 - val_loss: 537.2000 - val_mse: 537.2000 - val_mae: 21.4327\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 538.2025 - mse: 538.2025 - mae: 20.8980 - val_loss: 536.8994 - val_mse: 536.8994 - val_mae: 21.4202\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 537.8817 - mse: 537.8817 - mae: 20.8923 - val_loss: 536.4216 - val_mse: 536.4216 - val_mae: 21.4007\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 537.4962 - mse: 537.4962 - mae: 20.8820 - val_loss: 535.9542 - val_mse: 535.9542 - val_mae: 21.3813\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 537.1229 - mse: 537.1229 - mae: 20.8759 - val_loss: 535.5079 - val_mse: 535.5079 - val_mae: 21.3627\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 536.7329 - mse: 536.7329 - mae: 20.8666 - val_loss: 535.0418 - val_mse: 535.0418 - val_mae: 21.3429\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 536.2818 - mse: 536.2818 - mae: 20.8569 - val_loss: 534.6200 - val_mse: 534.6200 - val_mae: 21.3248\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 535.8057 - mse: 535.8057 - mae: 20.8488 - val_loss: 534.0318 - val_mse: 534.0318 - val_mae: 21.3032\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 535.3040 - mse: 535.3040 - mae: 20.8335 - val_loss: 533.3275 - val_mse: 533.3275 - val_mae: 21.2784\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 534.7682 - mse: 534.7682 - mae: 20.8225 - val_loss: 532.5208 - val_mse: 532.5208 - val_mae: 21.2499\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 534.1030 - mse: 534.1030 - mae: 20.8065 - val_loss: 531.4415 - val_mse: 531.4415 - val_mae: 21.2135\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 533.2170 - mse: 533.2170 - mae: 20.7814 - val_loss: 530.0462 - val_mse: 530.0462 - val_mae: 21.1649\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 532.1334 - mse: 532.1334 - mae: 20.7500 - val_loss: 528.4092 - val_mse: 528.4092 - val_mae: 21.1055\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 530.9332 - mse: 530.9332 - mae: 20.7169 - val_loss: 526.8762 - val_mse: 526.8762 - val_mae: 21.0490\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 529.7402 - mse: 529.7402 - mae: 20.6808 - val_loss: 525.3047 - val_mse: 525.3047 - val_mae: 20.9897\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 528.4933 - mse: 528.4933 - mae: 20.6420 - val_loss: 523.6058 - val_mse: 523.6058 - val_mae: 20.9239\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 527.0898 - mse: 527.0898 - mae: 20.6021 - val_loss: 521.9380 - val_mse: 521.9380 - val_mae: 20.8585\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 525.7386 - mse: 525.7386 - mae: 20.5611 - val_loss: 520.0087 - val_mse: 520.0087 - val_mae: 20.7767\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 524.4210 - mse: 524.4210 - mae: 20.5281 - val_loss: 518.2498 - val_mse: 518.2498 - val_mae: 20.7008\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 523.1805 - mse: 523.1805 - mae: 20.4877 - val_loss: 516.5618 - val_mse: 516.5618 - val_mae: 20.6264\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 521.9950 - mse: 521.9950 - mae: 20.4505 - val_loss: 514.9387 - val_mse: 514.9387 - val_mae: 20.5534\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 520.8260 - mse: 520.8260 - mae: 20.4171 - val_loss: 513.3208 - val_mse: 513.3208 - val_mae: 20.4790\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 519.7303 - mse: 519.7303 - mae: 20.3775 - val_loss: 511.7833 - val_mse: 511.7833 - val_mae: 20.4060\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 518.6521 - mse: 518.6521 - mae: 20.3468 - val_loss: 510.2870 - val_mse: 510.2870 - val_mae: 20.3336\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 517.6274 - mse: 517.6274 - mae: 20.3180 - val_loss: 508.8653 - val_mse: 508.8653 - val_mae: 20.2631\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 516.6256 - mse: 516.6256 - mae: 20.2821 - val_loss: 507.4744 - val_mse: 507.4744 - val_mae: 20.1930\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 515.6640 - mse: 515.6640 - mae: 20.2542 - val_loss: 506.0806 - val_mse: 506.0806 - val_mae: 20.1251\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 514.7738 - mse: 514.7738 - mae: 20.2300 - val_loss: 504.8180 - val_mse: 504.8180 - val_mae: 20.0639\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 513.9481 - mse: 513.9481 - mae: 20.1939 - val_loss: 503.4484 - val_mse: 503.4484 - val_mae: 19.9950\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 513.0524 - mse: 513.0524 - mae: 20.1668 - val_loss: 502.0329 - val_mse: 502.0329 - val_mae: 19.9248\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 512.2516 - mse: 512.2516 - mae: 20.1420 - val_loss: 500.5713 - val_mse: 500.5713 - val_mae: 19.8551\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 511.4706 - mse: 511.4706 - mae: 20.1120 - val_loss: 499.2390 - val_mse: 499.2390 - val_mae: 19.7919\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.5, model__n_hidden=2, model__n_neurons=5, model__optimizer=sgd; total time=   5.4s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 5310.4858 - mse: 5310.4858 - mae: 55.4639 - val_loss: 2183.4373 - val_mse: 2183.4373 - val_mae: 38.6204\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1196.0411 - mse: 1196.0411 - mae: 26.2114 - val_loss: 1315.1691 - val_mse: 1315.1691 - val_mae: 28.0421\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 715.4525 - mse: 715.4525 - mae: 19.5163 - val_loss: 834.3122 - val_mse: 834.3122 - val_mae: 23.4791\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 491.4938 - mse: 491.4938 - mae: 16.3491 - val_loss: 559.4034 - val_mse: 559.4034 - val_mae: 17.3667\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 375.0794 - mse: 375.0794 - mae: 14.2944 - val_loss: 404.2122 - val_mse: 404.2122 - val_mae: 15.1687\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 320.4637 - mse: 320.4637 - mae: 13.6614 - val_loss: 314.1229 - val_mse: 314.1229 - val_mae: 13.1555\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 286.8643 - mse: 286.8643 - mae: 13.1028 - val_loss: 249.4003 - val_mse: 249.4003 - val_mae: 11.4218\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 271.9271 - mse: 271.9271 - mae: 13.1137 - val_loss: 222.1109 - val_mse: 222.1109 - val_mae: 11.4376\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 258.7351 - mse: 258.7351 - mae: 12.6396 - val_loss: 228.4779 - val_mse: 228.4779 - val_mae: 12.8064\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 253.5898 - mse: 253.5898 - mae: 12.7109 - val_loss: 220.1793 - val_mse: 220.1793 - val_mae: 12.8507\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 252.7737 - mse: 252.7737 - mae: 12.7708 - val_loss: 169.0711 - val_mse: 169.0711 - val_mae: 10.1963\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 246.5689 - mse: 246.5689 - mae: 12.6701 - val_loss: 152.2130 - val_mse: 152.2130 - val_mae: 9.5041\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 240.0092 - mse: 240.0092 - mae: 12.4569 - val_loss: 188.3962 - val_mse: 188.3962 - val_mae: 11.7940\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 243.5867 - mse: 243.5867 - mae: 12.7068 - val_loss: 145.0096 - val_mse: 145.0096 - val_mae: 9.1630\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 235.6223 - mse: 235.6223 - mae: 12.3223 - val_loss: 146.7172 - val_mse: 146.7172 - val_mae: 9.1541\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 233.9226 - mse: 233.9226 - mae: 12.1204 - val_loss: 152.0377 - val_mse: 152.0377 - val_mae: 9.9358\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 229.4431 - mse: 229.4431 - mae: 12.2801 - val_loss: 136.6003 - val_mse: 136.6003 - val_mae: 9.0525\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 230.0110 - mse: 230.0110 - mae: 12.1198 - val_loss: 151.9371 - val_mse: 151.9371 - val_mae: 10.1086\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 222.3562 - mse: 222.3562 - mae: 11.9085 - val_loss: 159.0095 - val_mse: 159.0095 - val_mae: 10.6427\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 224.0872 - mse: 224.0872 - mae: 12.0168 - val_loss: 141.0719 - val_mse: 141.0719 - val_mae: 9.6245\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 220.5954 - mse: 220.5954 - mae: 11.9947 - val_loss: 129.4469 - val_mse: 129.4469 - val_mae: 8.7183\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 217.7577 - mse: 217.7577 - mae: 11.7453 - val_loss: 137.5103 - val_mse: 137.5103 - val_mae: 9.4408\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 216.6269 - mse: 216.6269 - mae: 11.7690 - val_loss: 179.2376 - val_mse: 179.2376 - val_mae: 11.7634\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 214.5411 - mse: 214.5411 - mae: 11.9185 - val_loss: 138.5173 - val_mse: 138.5173 - val_mae: 8.7919\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 211.7649 - mse: 211.7649 - mae: 11.3894 - val_loss: 139.1892 - val_mse: 139.1892 - val_mae: 9.8190\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 206.1587 - mse: 206.1587 - mae: 11.4874 - val_loss: 136.9188 - val_mse: 136.9188 - val_mae: 9.7207\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 205.9400 - mse: 205.9400 - mae: 11.5299 - val_loss: 119.9935 - val_mse: 119.9935 - val_mae: 8.5066\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 207.7759 - mse: 207.7759 - mae: 11.4856 - val_loss: 120.2075 - val_mse: 120.2075 - val_mae: 8.3035\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 199.0027 - mse: 199.0027 - mae: 11.0892 - val_loss: 133.0802 - val_mse: 133.0802 - val_mae: 9.6116\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 203.7853 - mse: 203.7853 - mae: 11.3021 - val_loss: 124.9187 - val_mse: 124.9187 - val_mae: 9.0816\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 195.1808 - mse: 195.1808 - mae: 11.2261 - val_loss: 119.5894 - val_mse: 119.5894 - val_mae: 8.0536\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 196.4684 - mse: 196.4684 - mae: 11.0318 - val_loss: 125.2730 - val_mse: 125.2730 - val_mae: 9.1701\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 199.3113 - mse: 199.3113 - mae: 11.2066 - val_loss: 147.1387 - val_mse: 147.1387 - val_mae: 10.5135\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 191.1161 - mse: 191.1161 - mae: 11.0570 - val_loss: 139.1832 - val_mse: 139.1832 - val_mae: 10.1396\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 188.4893 - mse: 188.4893 - mae: 10.8948 - val_loss: 122.6696 - val_mse: 122.6696 - val_mae: 9.0470\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 190.8723 - mse: 190.8723 - mae: 10.9933 - val_loss: 112.1679 - val_mse: 112.1679 - val_mae: 7.8481\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 186.3517 - mse: 186.3517 - mae: 10.7070 - val_loss: 111.6005 - val_mse: 111.6005 - val_mae: 8.3913\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 188.2612 - mse: 188.2612 - mae: 10.8510 - val_loss: 109.5126 - val_mse: 109.5126 - val_mae: 7.6165\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 182.7495 - mse: 182.7495 - mae: 10.6577 - val_loss: 126.8365 - val_mse: 126.8365 - val_mae: 9.6015\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 181.8594 - mse: 181.8594 - mae: 10.7407 - val_loss: 118.4768 - val_mse: 118.4768 - val_mae: 9.0984\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 177.3985 - mse: 177.3985 - mae: 10.6795 - val_loss: 104.2403 - val_mse: 104.2403 - val_mae: 7.8820\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 174.0846 - mse: 174.0846 - mae: 10.4092 - val_loss: 108.3214 - val_mse: 108.3214 - val_mae: 8.3889\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 173.5267 - mse: 173.5267 - mae: 10.4582 - val_loss: 103.2519 - val_mse: 103.2519 - val_mae: 7.9609\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 172.8743 - mse: 172.8743 - mae: 10.4367 - val_loss: 112.0173 - val_mse: 112.0173 - val_mae: 8.7860\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 169.5351 - mse: 169.5351 - mae: 10.3651 - val_loss: 100.5880 - val_mse: 100.5880 - val_mae: 7.4346\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 170.9094 - mse: 170.9094 - mae: 10.3398 - val_loss: 98.3364 - val_mse: 98.3364 - val_mae: 7.3628\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 172.3817 - mse: 172.3817 - mae: 10.2391 - val_loss: 107.8810 - val_mse: 107.8810 - val_mae: 8.7747\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 168.2278 - mse: 168.2278 - mae: 10.3102 - val_loss: 103.1798 - val_mse: 103.1798 - val_mae: 8.3917\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 166.1416 - mse: 166.1416 - mae: 10.1894 - val_loss: 97.7856 - val_mse: 97.7856 - val_mae: 8.0356\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 162.6063 - mse: 162.6063 - mae: 10.0549 - val_loss: 98.1741 - val_mse: 98.1741 - val_mae: 8.0981\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 164.7018 - mse: 164.7018 - mae: 10.0378 - val_loss: 99.0340 - val_mse: 99.0340 - val_mae: 8.1314\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 162.0624 - mse: 162.0624 - mae: 10.1243 - val_loss: 92.2237 - val_mse: 92.2237 - val_mae: 7.2677\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 162.1347 - mse: 162.1347 - mae: 9.9851 - val_loss: 94.8537 - val_mse: 94.8537 - val_mae: 7.7434\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 155.9154 - mse: 155.9154 - mae: 9.9181 - val_loss: 90.2207 - val_mse: 90.2207 - val_mae: 7.2203\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 155.5642 - mse: 155.5642 - mae: 9.6651 - val_loss: 107.3054 - val_mse: 107.3054 - val_mae: 8.9327\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 158.5762 - mse: 158.5762 - mae: 9.9079 - val_loss: 88.3134 - val_mse: 88.3134 - val_mae: 7.0099\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 155.2529 - mse: 155.2529 - mae: 9.7105 - val_loss: 97.3619 - val_mse: 97.3619 - val_mae: 8.3429\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 157.2930 - mse: 157.2930 - mae: 9.9632 - val_loss: 101.7751 - val_mse: 101.7751 - val_mae: 8.6262\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 153.1270 - mse: 153.1270 - mae: 9.8024 - val_loss: 88.7397 - val_mse: 88.7397 - val_mae: 7.1902\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 151.3616 - mse: 151.3616 - mae: 9.6207 - val_loss: 87.6187 - val_mse: 87.6187 - val_mae: 7.4052\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 151.5767 - mse: 151.5767 - mae: 9.5872 - val_loss: 85.0076 - val_mse: 85.0076 - val_mae: 6.9232\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 149.6502 - mse: 149.6502 - mae: 9.5317 - val_loss: 85.8739 - val_mse: 85.8739 - val_mae: 6.8564\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 150.7908 - mse: 150.7908 - mae: 9.5758 - val_loss: 87.1726 - val_mse: 87.1726 - val_mae: 7.4587\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 146.0484 - mse: 146.0484 - mae: 9.4673 - val_loss: 93.7575 - val_mse: 93.7575 - val_mae: 8.0453\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 146.2598 - mse: 146.2598 - mae: 9.4702 - val_loss: 83.1670 - val_mse: 83.1670 - val_mae: 6.8671\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 149.5912 - mse: 149.5912 - mae: 9.4552 - val_loss: 83.5455 - val_mse: 83.5455 - val_mae: 6.9726\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 142.8932 - mse: 142.8932 - mae: 9.2755 - val_loss: 82.5136 - val_mse: 82.5136 - val_mae: 7.1122\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 142.6042 - mse: 142.6042 - mae: 9.2902 - val_loss: 83.0554 - val_mse: 83.0554 - val_mae: 7.0799\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 141.5663 - mse: 141.5663 - mae: 9.1892 - val_loss: 89.4743 - val_mse: 89.4743 - val_mae: 7.9748\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 139.5350 - mse: 139.5350 - mae: 9.2644 - val_loss: 79.6479 - val_mse: 79.6479 - val_mae: 6.9916\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 141.5137 - mse: 141.5137 - mae: 9.2593 - val_loss: 90.0422 - val_mse: 90.0422 - val_mae: 6.5464\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 144.7566 - mse: 144.7566 - mae: 9.1142 - val_loss: 84.4633 - val_mse: 84.4633 - val_mae: 7.5957\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 136.3232 - mse: 136.3232 - mae: 9.1051 - val_loss: 78.0749 - val_mse: 78.0749 - val_mae: 6.6328\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 137.2224 - mse: 137.2224 - mae: 9.0276 - val_loss: 82.0274 - val_mse: 82.0274 - val_mae: 7.4000\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 135.9741 - mse: 135.9741 - mae: 8.9957 - val_loss: 80.0102 - val_mse: 80.0102 - val_mae: 7.2354\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 136.0024 - mse: 136.0024 - mae: 9.1397 - val_loss: 75.9322 - val_mse: 75.9322 - val_mae: 6.5373\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 136.3796 - mse: 136.3796 - mae: 8.9302 - val_loss: 76.1695 - val_mse: 76.1695 - val_mae: 6.7594\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 132.4889 - mse: 132.4889 - mae: 8.8564 - val_loss: 78.5542 - val_mse: 78.5542 - val_mae: 7.1743\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 131.8739 - mse: 131.8739 - mae: 8.8767 - val_loss: 79.1177 - val_mse: 79.1177 - val_mae: 7.1718\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 132.2942 - mse: 132.2942 - mae: 8.9189 - val_loss: 79.8444 - val_mse: 79.8444 - val_mae: 7.3367\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 131.4425 - mse: 131.4425 - mae: 8.8217 - val_loss: 73.6025 - val_mse: 73.6025 - val_mae: 6.7880\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 128.2586 - mse: 128.2586 - mae: 8.6591 - val_loss: 77.2516 - val_mse: 77.2516 - val_mae: 7.2977\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 129.1442 - mse: 129.1442 - mae: 8.7820 - val_loss: 85.9085 - val_mse: 85.9085 - val_mae: 7.9949\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 132.9431 - mse: 132.9431 - mae: 8.8881 - val_loss: 94.3306 - val_mse: 94.3306 - val_mae: 8.5418\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 130.7993 - mse: 130.7993 - mae: 8.8489 - val_loss: 76.3261 - val_mse: 76.3261 - val_mae: 7.2398\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 126.0209 - mse: 126.0209 - mae: 8.5873 - val_loss: 81.5576 - val_mse: 81.5576 - val_mae: 7.6589\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 126.7875 - mse: 126.7875 - mae: 8.6899 - val_loss: 71.3372 - val_mse: 71.3372 - val_mae: 6.7016\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 124.5854 - mse: 124.5854 - mae: 8.4840 - val_loss: 87.7460 - val_mse: 87.7460 - val_mae: 8.1555\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 129.1863 - mse: 129.1863 - mae: 8.7527 - val_loss: 72.1077 - val_mse: 72.1077 - val_mae: 6.9098\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 123.7674 - mse: 123.7674 - mae: 8.6166 - val_loss: 68.5071 - val_mse: 68.5071 - val_mae: 6.2636\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 125.0860 - mse: 125.0860 - mae: 8.5276 - val_loss: 79.0792 - val_mse: 79.0792 - val_mae: 7.5679\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 125.1397 - mse: 125.1397 - mae: 8.6692 - val_loss: 72.5114 - val_mse: 72.5114 - val_mae: 5.8881\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 125.0033 - mse: 125.0033 - mae: 8.5763 - val_loss: 69.4670 - val_mse: 69.4670 - val_mae: 5.9433\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 123.4533 - mse: 123.4533 - mae: 8.3834 - val_loss: 72.8102 - val_mse: 72.8102 - val_mae: 7.0445\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 120.2622 - mse: 120.2622 - mae: 8.3913 - val_loss: 72.4792 - val_mse: 72.4792 - val_mae: 7.0761\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 119.6659 - mse: 119.6659 - mae: 8.4093 - val_loss: 71.4559 - val_mse: 71.4559 - val_mae: 6.9602\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 118.9027 - mse: 118.9027 - mae: 8.3541 - val_loss: 70.5497 - val_mse: 70.5497 - val_mae: 6.8133\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 118.7977 - mse: 118.7977 - mae: 8.3885 - val_loss: 66.0857 - val_mse: 66.0857 - val_mae: 6.4279\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 120.0372 - mse: 120.0372 - mae: 8.3191 - val_loss: 67.5605 - val_mse: 67.5605 - val_mae: 6.5103\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 117.0577 - mse: 117.0577 - mae: 8.1265 - val_loss: 76.5806 - val_mse: 76.5806 - val_mae: 7.4043\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.1, model__n_hidden=0, model__n_neurons=125, model__optimizer=nesterov; total time=   5.0s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 5671.3237 - mse: 5671.3237 - mae: 47.9847 - val_loss: 395.0949 - val_mse: 395.0949 - val_mae: 15.3027\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 533.2373 - mse: 533.2373 - mae: 18.7844 - val_loss: 218.0235 - val_mse: 218.0235 - val_mae: 11.7531\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 377.6593 - mse: 377.6593 - mae: 15.6988 - val_loss: 148.2532 - val_mse: 148.2532 - val_mae: 9.2688\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 302.5221 - mse: 302.5221 - mae: 13.9777 - val_loss: 128.2327 - val_mse: 128.2327 - val_mae: 8.9524\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 270.8856 - mse: 270.8856 - mae: 13.0825 - val_loss: 126.4559 - val_mse: 126.4559 - val_mae: 8.5294\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 257.2661 - mse: 257.2661 - mae: 12.8019 - val_loss: 128.3449 - val_mse: 128.3449 - val_mae: 9.3132\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 251.1242 - mse: 251.1242 - mae: 12.6191 - val_loss: 129.2146 - val_mse: 129.2146 - val_mae: 9.3137\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 243.0221 - mse: 243.0221 - mae: 12.3701 - val_loss: 141.5342 - val_mse: 141.5342 - val_mae: 9.0508\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 241.3530 - mse: 241.3530 - mae: 12.5830 - val_loss: 132.2692 - val_mse: 132.2692 - val_mae: 8.9186\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 238.2525 - mse: 238.2525 - mae: 12.4255 - val_loss: 131.5561 - val_mse: 131.5561 - val_mae: 9.0139\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 238.1963 - mse: 238.1963 - mae: 12.4821 - val_loss: 129.8416 - val_mse: 129.8416 - val_mae: 9.2272\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 234.4294 - mse: 234.4294 - mae: 12.2886 - val_loss: 129.9512 - val_mse: 129.9512 - val_mae: 9.0504\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 233.5345 - mse: 233.5345 - mae: 12.3913 - val_loss: 129.0867 - val_mse: 129.0867 - val_mae: 9.1470\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 231.3463 - mse: 231.3463 - mae: 12.2603 - val_loss: 131.3517 - val_mse: 131.3517 - val_mae: 9.0348\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 230.0414 - mse: 230.0414 - mae: 12.3399 - val_loss: 130.5663 - val_mse: 130.5663 - val_mae: 8.9455\n",
      "Epoch 15: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.1, model__n_hidden=0, model__n_neurons=125, model__optimizer=nesterov; total time=   1.2s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 4208.1548 - mse: 4208.1548 - mae: 39.2206 - val_loss: 238.5957 - val_mse: 238.5957 - val_mae: 12.5500\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 175.2073 - mse: 175.2073 - mae: 10.0788 - val_loss: 209.7573 - val_mse: 209.7573 - val_mae: 10.7522\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 168.8116 - mse: 168.8116 - mae: 9.8236 - val_loss: 197.5541 - val_mse: 197.5541 - val_mae: 10.6194\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 160.3959 - mse: 160.3959 - mae: 9.6048 - val_loss: 191.2870 - val_mse: 191.2870 - val_mae: 10.7701\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 158.2737 - mse: 158.2737 - mae: 9.6671 - val_loss: 183.1637 - val_mse: 183.1637 - val_mae: 10.1814\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 156.7649 - mse: 156.7649 - mae: 9.4815 - val_loss: 179.8311 - val_mse: 179.8311 - val_mae: 10.0127\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 161.6055 - mse: 161.6055 - mae: 9.7528 - val_loss: 186.7291 - val_mse: 186.7291 - val_mae: 11.0915\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 153.7741 - mse: 153.7741 - mae: 9.6750 - val_loss: 178.8659 - val_mse: 178.8659 - val_mae: 9.7102\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 154.6926 - mse: 154.6926 - mae: 9.4111 - val_loss: 170.9462 - val_mse: 170.9462 - val_mae: 9.9141\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 150.0873 - mse: 150.0873 - mae: 9.4178 - val_loss: 173.9401 - val_mse: 173.9401 - val_mae: 9.5856\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 148.7779 - mse: 148.7779 - mae: 9.2258 - val_loss: 168.1206 - val_mse: 168.1206 - val_mae: 10.1081\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 148.9619 - mse: 148.9619 - mae: 9.4108 - val_loss: 162.0292 - val_mse: 162.0292 - val_mae: 9.5692\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 143.8497 - mse: 143.8497 - mae: 9.1198 - val_loss: 160.6799 - val_mse: 160.6799 - val_mae: 9.6273\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 144.2426 - mse: 144.2426 - mae: 9.2652 - val_loss: 161.3283 - val_mse: 161.3283 - val_mae: 9.8186\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 141.1628 - mse: 141.1628 - mae: 9.1718 - val_loss: 157.3796 - val_mse: 157.3796 - val_mae: 9.4764\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 141.1308 - mse: 141.1308 - mae: 9.0925 - val_loss: 154.8648 - val_mse: 154.8648 - val_mae: 9.4468\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 135.8026 - mse: 135.8026 - mae: 9.0735 - val_loss: 164.9819 - val_mse: 164.9819 - val_mae: 9.3046\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 143.2346 - mse: 143.2346 - mae: 8.9523 - val_loss: 154.8420 - val_mse: 154.8420 - val_mae: 9.6364\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 138.1155 - mse: 138.1155 - mae: 8.9439 - val_loss: 164.9031 - val_mse: 164.9031 - val_mae: 10.5312\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 135.0370 - mse: 135.0370 - mae: 8.9596 - val_loss: 147.5538 - val_mse: 147.5538 - val_mae: 9.0960\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 135.1427 - mse: 135.1427 - mae: 8.8097 - val_loss: 145.8504 - val_mse: 145.8504 - val_mae: 9.0760\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 132.6687 - mse: 132.6687 - mae: 8.7722 - val_loss: 145.5942 - val_mse: 145.5942 - val_mae: 9.1958\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 131.0665 - mse: 131.0665 - mae: 8.7567 - val_loss: 150.9743 - val_mse: 150.9743 - val_mae: 9.8898\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 130.5980 - mse: 130.5980 - mae: 8.7404 - val_loss: 151.1599 - val_mse: 151.1599 - val_mae: 9.9348\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 131.0879 - mse: 131.0879 - mae: 8.8154 - val_loss: 139.8621 - val_mse: 139.8621 - val_mae: 9.0230\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 128.6445 - mse: 128.6445 - mae: 8.6553 - val_loss: 152.2643 - val_mse: 152.2643 - val_mae: 10.1140\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 128.2428 - mse: 128.2428 - mae: 8.7214 - val_loss: 137.3303 - val_mse: 137.3303 - val_mae: 8.8744\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 126.8280 - mse: 126.8280 - mae: 8.5989 - val_loss: 137.2782 - val_mse: 137.2782 - val_mae: 8.9569\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 126.5412 - mse: 126.5412 - mae: 8.5828 - val_loss: 132.7297 - val_mse: 132.7297 - val_mae: 8.6295\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 127.5156 - mse: 127.5156 - mae: 8.5617 - val_loss: 132.3881 - val_mse: 132.3881 - val_mae: 8.6793\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 125.0150 - mse: 125.0150 - mae: 8.4375 - val_loss: 138.2457 - val_mse: 138.2457 - val_mae: 9.3109\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 121.6141 - mse: 121.6141 - mae: 8.3296 - val_loss: 150.1876 - val_mse: 150.1876 - val_mae: 10.1547\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 126.0943 - mse: 126.0943 - mae: 8.5017 - val_loss: 128.3896 - val_mse: 128.3896 - val_mae: 8.4036\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 122.9601 - mse: 122.9601 - mae: 8.3609 - val_loss: 126.9423 - val_mse: 126.9423 - val_mae: 8.4352\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 119.4052 - mse: 119.4052 - mae: 8.2406 - val_loss: 126.7808 - val_mse: 126.7808 - val_mae: 8.5081\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 125.4469 - mse: 125.4469 - mae: 8.5617 - val_loss: 124.9887 - val_mse: 124.9887 - val_mae: 8.2668\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 120.3080 - mse: 120.3080 - mae: 8.2557 - val_loss: 124.3413 - val_mse: 124.3413 - val_mae: 8.2365\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 122.1053 - mse: 122.1053 - mae: 8.3473 - val_loss: 121.7270 - val_mse: 121.7270 - val_mae: 8.1622\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 121.6975 - mse: 121.6975 - mae: 8.2555 - val_loss: 125.5553 - val_mse: 125.5553 - val_mae: 8.7133\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 116.6717 - mse: 116.6717 - mae: 8.1931 - val_loss: 124.4707 - val_mse: 124.4707 - val_mae: 8.5757\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 120.2043 - mse: 120.2043 - mae: 8.2905 - val_loss: 118.2834 - val_mse: 118.2834 - val_mae: 8.0977\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 114.9359 - mse: 114.9359 - mae: 7.9900 - val_loss: 155.4636 - val_mse: 155.4636 - val_mae: 10.5419\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 119.0271 - mse: 119.0271 - mae: 8.3037 - val_loss: 116.0022 - val_mse: 116.0022 - val_mae: 8.1051\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 115.8092 - mse: 115.8092 - mae: 8.0755 - val_loss: 114.5077 - val_mse: 114.5077 - val_mae: 7.9140\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 114.1704 - mse: 114.1704 - mae: 7.9624 - val_loss: 118.8490 - val_mse: 118.8490 - val_mae: 8.4351\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 111.8544 - mse: 111.8544 - mae: 7.9666 - val_loss: 113.5772 - val_mse: 113.5772 - val_mae: 7.9873\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 109.8461 - mse: 109.8461 - mae: 7.8917 - val_loss: 114.0632 - val_mse: 114.0632 - val_mae: 7.8511\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 111.1292 - mse: 111.1292 - mae: 7.8876 - val_loss: 109.8599 - val_mse: 109.8599 - val_mae: 7.7797\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 109.8857 - mse: 109.8857 - mae: 7.8385 - val_loss: 122.6203 - val_mse: 122.6203 - val_mae: 8.8514\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 112.4154 - mse: 112.4154 - mae: 8.0328 - val_loss: 112.7336 - val_mse: 112.7336 - val_mae: 7.9711\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 108.9111 - mse: 108.9111 - mae: 7.8303 - val_loss: 108.1894 - val_mse: 108.1894 - val_mae: 7.6868\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 110.8962 - mse: 110.8962 - mae: 7.8502 - val_loss: 106.4163 - val_mse: 106.4163 - val_mae: 7.6300\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 106.9641 - mse: 106.9641 - mae: 7.6125 - val_loss: 132.7972 - val_mse: 132.7972 - val_mae: 9.4763\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 108.0681 - mse: 108.0681 - mae: 7.8116 - val_loss: 123.3807 - val_mse: 123.3807 - val_mae: 8.9728\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 110.2453 - mse: 110.2453 - mae: 7.8334 - val_loss: 105.6355 - val_mse: 105.6355 - val_mae: 7.6548\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 105.4139 - mse: 105.4139 - mae: 7.7285 - val_loss: 104.3051 - val_mse: 104.3051 - val_mae: 7.5030\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 106.0908 - mse: 106.0908 - mae: 7.7093 - val_loss: 103.3501 - val_mse: 103.3501 - val_mae: 7.6599\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 107.7759 - mse: 107.7759 - mae: 7.6502 - val_loss: 104.4992 - val_mse: 104.4992 - val_mae: 7.7084\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 105.4640 - mse: 105.4640 - mae: 7.6432 - val_loss: 105.5675 - val_mse: 105.5675 - val_mae: 7.8900\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 104.2942 - mse: 104.2942 - mae: 7.6421 - val_loss: 98.9502 - val_mse: 98.9502 - val_mae: 7.3775\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 104.2454 - mse: 104.2454 - mae: 7.6633 - val_loss: 100.2880 - val_mse: 100.2880 - val_mae: 7.5119\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 103.0215 - mse: 103.0215 - mae: 7.4925 - val_loss: 104.9257 - val_mse: 104.9257 - val_mae: 7.8338\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 105.6477 - mse: 105.6477 - mae: 7.7210 - val_loss: 98.8930 - val_mse: 98.8930 - val_mae: 7.3174\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 101.8375 - mse: 101.8375 - mae: 7.3750 - val_loss: 101.7638 - val_mse: 101.7638 - val_mae: 7.6059\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 100.4610 - mse: 100.4610 - mae: 7.4517 - val_loss: 97.7027 - val_mse: 97.7027 - val_mae: 7.2989\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 102.8793 - mse: 102.8793 - mae: 7.4842 - val_loss: 97.6997 - val_mse: 97.6997 - val_mae: 7.3611\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 102.4001 - mse: 102.4001 - mae: 7.4797 - val_loss: 102.6682 - val_mse: 102.6682 - val_mae: 7.7327\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 101.5419 - mse: 101.5419 - mae: 7.5758 - val_loss: 101.5202 - val_mse: 101.5202 - val_mae: 7.2506\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 99.7020 - mse: 99.7020 - mae: 7.4774 - val_loss: 97.5231 - val_mse: 97.5231 - val_mae: 7.4423\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 100.5878 - mse: 100.5878 - mae: 7.4584 - val_loss: 93.0074 - val_mse: 93.0074 - val_mae: 7.1468\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 99.1696 - mse: 99.1696 - mae: 7.3236 - val_loss: 97.2972 - val_mse: 97.2972 - val_mae: 7.4063\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 98.5960 - mse: 98.5960 - mae: 7.3127 - val_loss: 91.5810 - val_mse: 91.5810 - val_mae: 7.0518\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 97.1422 - mse: 97.1422 - mae: 7.2407 - val_loss: 107.7457 - val_mse: 107.7457 - val_mae: 8.1557\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 99.6789 - mse: 99.6789 - mae: 7.4663 - val_loss: 92.0609 - val_mse: 92.0609 - val_mae: 7.0515\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 97.7446 - mse: 97.7446 - mae: 7.2497 - val_loss: 90.4661 - val_mse: 90.4661 - val_mae: 6.9970\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 98.8575 - mse: 98.8575 - mae: 7.2590 - val_loss: 95.2236 - val_mse: 95.2236 - val_mae: 7.3291\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 97.0529 - mse: 97.0529 - mae: 7.3094 - val_loss: 88.2868 - val_mse: 88.2868 - val_mae: 6.9112\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 99.4247 - mse: 99.4247 - mae: 7.3465 - val_loss: 93.6827 - val_mse: 93.6827 - val_mae: 7.2693\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 96.1331 - mse: 96.1331 - mae: 7.2289 - val_loss: 88.4835 - val_mse: 88.4835 - val_mae: 6.8840\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 96.1460 - mse: 96.1460 - mae: 7.0950 - val_loss: 87.5941 - val_mse: 87.5941 - val_mae: 6.8675\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 96.7483 - mse: 96.7483 - mae: 7.2440 - val_loss: 94.0422 - val_mse: 94.0422 - val_mae: 7.3399\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 96.5662 - mse: 96.5662 - mae: 7.2481 - val_loss: 98.8418 - val_mse: 98.8418 - val_mae: 7.6641\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 95.3369 - mse: 95.3369 - mae: 7.2138 - val_loss: 92.7049 - val_mse: 92.7049 - val_mae: 7.2585\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 97.2694 - mse: 97.2694 - mae: 7.2398 - val_loss: 86.9082 - val_mse: 86.9082 - val_mae: 6.8609\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 94.5750 - mse: 94.5750 - mae: 7.1943 - val_loss: 85.1046 - val_mse: 85.1046 - val_mae: 6.7146\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 94.9011 - mse: 94.9011 - mae: 7.1629 - val_loss: 84.4240 - val_mse: 84.4240 - val_mae: 6.6603\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 94.8650 - mse: 94.8650 - mae: 7.1034 - val_loss: 88.6951 - val_mse: 88.6951 - val_mae: 6.9480\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 93.6150 - mse: 93.6150 - mae: 7.1268 - val_loss: 83.4575 - val_mse: 83.4575 - val_mae: 6.6430\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 94.6213 - mse: 94.6213 - mae: 7.1389 - val_loss: 84.4642 - val_mse: 84.4642 - val_mae: 6.7460\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 95.6649 - mse: 95.6649 - mae: 7.0989 - val_loss: 100.5024 - val_mse: 100.5024 - val_mae: 7.7574\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 94.4570 - mse: 94.4570 - mae: 7.2107 - val_loss: 86.7400 - val_mse: 86.7400 - val_mae: 6.8237\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 95.2918 - mse: 95.2918 - mae: 7.1034 - val_loss: 92.4791 - val_mse: 92.4791 - val_mae: 7.3079\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 93.5256 - mse: 93.5256 - mae: 7.1891 - val_loss: 80.6807 - val_mse: 80.6807 - val_mae: 6.5822\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 92.8480 - mse: 92.8480 - mae: 7.0459 - val_loss: 81.7056 - val_mse: 81.7056 - val_mae: 6.5367\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 92.5420 - mse: 92.5420 - mae: 6.9383 - val_loss: 84.1411 - val_mse: 84.1411 - val_mae: 6.7235\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 92.2016 - mse: 92.2016 - mae: 7.0189 - val_loss: 84.6985 - val_mse: 84.6985 - val_mae: 6.6897\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 91.0823 - mse: 91.0823 - mae: 7.0552 - val_loss: 79.2860 - val_mse: 79.2860 - val_mae: 6.5132\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 91.4481 - mse: 91.4481 - mae: 6.9826 - val_loss: 79.1627 - val_mse: 79.1627 - val_mae: 6.5042\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 91.1938 - mse: 91.1938 - mae: 6.9962 - val_loss: 82.6737 - val_mse: 82.6737 - val_mae: 6.6143\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 89.7479 - mse: 89.7479 - mae: 6.9659 - val_loss: 78.7950 - val_mse: 78.7950 - val_mae: 6.4641\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.1, model__n_hidden=0, model__n_neurons=125, model__optimizer=nesterov; total time=   5.0s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 195.9953 - mse: 195.9953 - mae: 10.1776 - val_loss: 19.8636 - val_mse: 19.8636 - val_mae: 3.7187\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 70.7791 - mse: 70.7791 - mae: 5.6901 - val_loss: 15.5597 - val_mse: 15.5597 - val_mae: 3.3117\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 65.8144 - mse: 65.8144 - mae: 5.5577 - val_loss: 15.2692 - val_mse: 15.2692 - val_mae: 3.2784\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 67.7983 - mse: 67.7983 - mae: 5.7867 - val_loss: 29.2253 - val_mse: 29.2253 - val_mae: 4.7034\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 66.9314 - mse: 66.9314 - mae: 5.7422 - val_loss: 14.8562 - val_mse: 14.8562 - val_mae: 3.2342\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 67.6154 - mse: 67.6154 - mae: 5.6559 - val_loss: 16.0635 - val_mse: 16.0635 - val_mae: 3.2724\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 66.2977 - mse: 66.2977 - mae: 5.6403 - val_loss: 16.9329 - val_mse: 16.9329 - val_mae: 3.5459\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 68.9287 - mse: 68.9287 - mae: 5.7992 - val_loss: 33.4596 - val_mse: 33.4596 - val_mae: 5.0141\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 63.6399 - mse: 63.6399 - mae: 5.5944 - val_loss: 47.9152 - val_mse: 47.9152 - val_mae: 6.1147\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 65.8944 - mse: 65.8944 - mae: 5.7732 - val_loss: 18.9499 - val_mse: 18.9499 - val_mae: 3.7604\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 63.6453 - mse: 63.6453 - mae: 5.6374 - val_loss: 16.2541 - val_mse: 16.2541 - val_mae: 3.4890\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 64.3228 - mse: 64.3228 - mae: 5.5613 - val_loss: 16.4349 - val_mse: 16.4349 - val_mae: 3.5125\n",
      "Epoch 12: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.9, model__n_hidden=3, model__n_neurons=25, model__optimizer=sgd; total time=   1.2s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 169.1717 - mse: 169.1717 - mae: 10.1985 - val_loss: 28.8689 - val_mse: 28.8689 - val_mae: 4.3542\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 86.9895 - mse: 86.9895 - mae: 6.8372 - val_loss: 16.8012 - val_mse: 16.8012 - val_mae: 3.3579\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 97.1434 - mse: 97.1434 - mae: 7.1106 - val_loss: 30.6095 - val_mse: 30.6095 - val_mae: 4.5184\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 84.4825 - mse: 84.4825 - mae: 6.7224 - val_loss: 45.0442 - val_mse: 45.0442 - val_mae: 5.6572\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 89.5969 - mse: 89.5969 - mae: 6.9000 - val_loss: 15.2650 - val_mse: 15.2650 - val_mae: 3.3161\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 96.0076 - mse: 96.0076 - mae: 7.0899 - val_loss: 16.6348 - val_mse: 16.6348 - val_mae: 3.4475\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 91.4361 - mse: 91.4361 - mae: 6.7375 - val_loss: 18.8038 - val_mse: 18.8038 - val_mae: 3.4554\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 121.6153 - mse: 121.6153 - mae: 8.0432 - val_loss: 31.3897 - val_mse: 31.3897 - val_mae: 4.5107\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 80.7935 - mse: 80.7935 - mae: 6.4957 - val_loss: 70.8893 - val_mse: 70.8893 - val_mae: 7.3038\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 83.7557 - mse: 83.7557 - mae: 6.6317 - val_loss: 46.1047 - val_mse: 46.1047 - val_mae: 5.4739\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 86.8159 - mse: 86.8159 - mae: 6.8749 - val_loss: 15.7024 - val_mse: 15.7024 - val_mae: 3.3080\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 83.8316 - mse: 83.8316 - mae: 6.5635 - val_loss: 33.3544 - val_mse: 33.3544 - val_mae: 4.6557\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 80.6232 - mse: 80.6232 - mae: 6.5242 - val_loss: 42.5484 - val_mse: 42.5484 - val_mae: 5.2756\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 95.7830 - mse: 95.7830 - mae: 7.3950 - val_loss: 16.7585 - val_mse: 16.7585 - val_mae: 3.2805\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 78.0006 - mse: 78.0006 - mae: 6.3798 - val_loss: 23.1447 - val_mse: 23.1447 - val_mae: 3.9295\n",
      "Epoch 15: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.9, model__n_hidden=3, model__n_neurons=25, model__optimizer=sgd; total time=   1.3s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 24ms/step - loss: 823.3535 - mse: 823.3535 - mae: 23.3518 - val_loss: 69.9535 - val_mse: 69.9535 - val_mae: 6.3194\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 65.3115 - mse: 65.3115 - mae: 5.8602 - val_loss: 43.8015 - val_mse: 43.8015 - val_mae: 5.1698\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 62.2461 - mse: 62.2461 - mae: 5.6682 - val_loss: 41.8388 - val_mse: 41.8388 - val_mae: 5.2090\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 57.6720 - mse: 57.6720 - mae: 5.3432 - val_loss: 40.8930 - val_mse: 40.8930 - val_mae: 5.0457\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 56.3070 - mse: 56.3070 - mae: 5.2321 - val_loss: 45.3829 - val_mse: 45.3829 - val_mae: 4.9584\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 56.4442 - mse: 56.4442 - mae: 5.1793 - val_loss: 39.4622 - val_mse: 39.4622 - val_mae: 4.7779\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 60.7389 - mse: 60.7389 - mae: 5.6018 - val_loss: 39.7704 - val_mse: 39.7704 - val_mae: 5.0870\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 57.6777 - mse: 57.6777 - mae: 5.3412 - val_loss: 41.5635 - val_mse: 41.5635 - val_mae: 4.7117\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 54.0766 - mse: 54.0766 - mae: 5.0101 - val_loss: 38.6996 - val_mse: 38.6996 - val_mae: 4.6782\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 53.0345 - mse: 53.0345 - mae: 4.9986 - val_loss: 36.8184 - val_mse: 36.8184 - val_mae: 4.7566\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 54.4469 - mse: 54.4469 - mae: 5.1726 - val_loss: 40.7089 - val_mse: 40.7089 - val_mae: 4.8148\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 55.3209 - mse: 55.3209 - mae: 5.1892 - val_loss: 53.3473 - val_mse: 53.3473 - val_mae: 5.5332\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 58.4265 - mse: 58.4265 - mae: 5.2784 - val_loss: 41.1412 - val_mse: 41.1412 - val_mae: 4.7506\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 55.1187 - mse: 55.1187 - mae: 5.1041 - val_loss: 40.2292 - val_mse: 40.2292 - val_mae: 4.8712\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 52.8275 - mse: 52.8275 - mae: 5.0083 - val_loss: 38.5294 - val_mse: 38.5294 - val_mae: 4.5706\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 54.0721 - mse: 54.0721 - mae: 5.0037 - val_loss: 44.7210 - val_mse: 44.7210 - val_mae: 5.2874\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 54.6157 - mse: 54.6157 - mae: 5.2100 - val_loss: 38.9392 - val_mse: 38.9392 - val_mae: 4.6011\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 54.0549 - mse: 54.0549 - mae: 5.0726 - val_loss: 37.2984 - val_mse: 37.2984 - val_mae: 4.5619\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 52.8202 - mse: 52.8202 - mae: 5.0509 - val_loss: 40.2040 - val_mse: 40.2040 - val_mae: 4.5815\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 53.6951 - mse: 53.6951 - mae: 4.9237 - val_loss: 35.3065 - val_mse: 35.3065 - val_mae: 4.6180\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 55.6940 - mse: 55.6940 - mae: 5.2568 - val_loss: 36.8437 - val_mse: 36.8437 - val_mae: 4.5222\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 52.2369 - mse: 52.2369 - mae: 4.8911 - val_loss: 36.5652 - val_mse: 36.5652 - val_mae: 4.4764\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 51.7307 - mse: 51.7307 - mae: 5.0299 - val_loss: 40.8030 - val_mse: 40.8030 - val_mae: 4.6439\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 54.3543 - mse: 54.3543 - mae: 4.9729 - val_loss: 35.6083 - val_mse: 35.6083 - val_mae: 4.7897\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 52.4659 - mse: 52.4659 - mae: 5.0776 - val_loss: 35.5158 - val_mse: 35.5158 - val_mae: 4.4504\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 51.6019 - mse: 51.6019 - mae: 4.8697 - val_loss: 45.1537 - val_mse: 45.1537 - val_mae: 5.4081\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 55.0217 - mse: 55.0217 - mae: 5.2968 - val_loss: 36.7026 - val_mse: 36.7026 - val_mae: 4.6192\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 52.3104 - mse: 52.3104 - mae: 5.0205 - val_loss: 34.3185 - val_mse: 34.3185 - val_mae: 4.4692\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 50.4024 - mse: 50.4024 - mae: 4.8668 - val_loss: 35.1963 - val_mse: 35.1963 - val_mae: 4.4413\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 52.8082 - mse: 52.8082 - mae: 5.0469 - val_loss: 39.2880 - val_mse: 39.2880 - val_mae: 4.7354\n",
      "Epoch 30: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.9, model__n_hidden=3, model__n_neurons=25, model__optimizer=sgd; total time=   2.5s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 23ms/step - loss: 13865491702714133926895270410321920.0000 - mse: 13865491702714133926895270410321920.0000 - mae: 32114134044639232.0000 - val_loss: inf - val_mse: inf - val_mae: inf\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 10: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.9, model__n_hidden=3, model__n_neurons=125, model__optimizer=momentum; total time=   1.2s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 27ms/step - loss: 12247.5020 - mse: 12247.5020 - mae: 76.4060 - val_loss: 3900.1418 - val_mse: 3900.1418 - val_mae: 60.7299\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1121.8871 - mse: 1121.8871 - mae: 25.7450 - val_loss: 272.3838 - val_mse: 272.3838 - val_mae: 15.0138\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 263.6085 - mse: 263.6085 - mae: 13.2248 - val_loss: 62.4693 - val_mse: 62.4693 - val_mae: 6.8849\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 362.3121 - mse: 362.3121 - mae: 15.8617 - val_loss: 42.0278 - val_mse: 42.0278 - val_mae: 5.4791\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 236.4523 - mse: 236.4523 - mae: 12.5953 - val_loss: 36.5018 - val_mse: 36.5018 - val_mae: 4.6218\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 134.8487 - mse: 134.8487 - mae: 8.5451 - val_loss: 41.5330 - val_mse: 41.5330 - val_mae: 5.0969\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 97.4529 - mse: 97.4529 - mae: 7.1849 - val_loss: 22.0018 - val_mse: 22.0018 - val_mae: 3.6796\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 93.5902 - mse: 93.5902 - mae: 7.1039 - val_loss: 18.7670 - val_mse: 18.7670 - val_mae: 3.5876\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 87.2891 - mse: 87.2891 - mae: 6.7357 - val_loss: 28.5231 - val_mse: 28.5231 - val_mae: 4.4726\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 87.9644 - mse: 87.9644 - mae: 6.9192 - val_loss: 22.2990 - val_mse: 22.2990 - val_mae: 3.9338\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 86.5505 - mse: 86.5505 - mae: 6.9776 - val_loss: 18.9613 - val_mse: 18.9613 - val_mae: 3.6752\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 81.4447 - mse: 81.4447 - mae: 6.6109 - val_loss: 30.7997 - val_mse: 30.7997 - val_mae: 4.7118\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 87.3856 - mse: 87.3856 - mae: 6.9302 - val_loss: 38.8403 - val_mse: 38.8403 - val_mae: 5.3587\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 83.9999 - mse: 83.9999 - mae: 6.7245 - val_loss: 34.0066 - val_mse: 34.0066 - val_mae: 4.8422\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 91.3411 - mse: 91.3411 - mae: 6.9078 - val_loss: 39.7916 - val_mse: 39.7916 - val_mae: 5.3192\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 84.4378 - mse: 84.4378 - mae: 6.6922 - val_loss: 22.9364 - val_mse: 22.9364 - val_mae: 4.1002\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 83.3103 - mse: 83.3103 - mae: 6.5573 - val_loss: 30.0063 - val_mse: 30.0063 - val_mae: 4.6085\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 82.4937 - mse: 82.4937 - mae: 6.5714 - val_loss: 35.2348 - val_mse: 35.2348 - val_mae: 4.9209\n",
      "Epoch 18: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.9, model__n_hidden=3, model__n_neurons=125, model__optimizer=momentum; total time=   1.8s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 23ms/step - loss: 3447507.2500 - mse: 3447507.2500 - mae: 722.5128 - val_loss: 151291075947745247232.0000 - val_mse: 151291075947745247232.0000 - val_mae: 12029374464.0000\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan         \n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.9, model__n_hidden=3, model__n_neurons=125, model__optimizer=momentum; total time=   1.2s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 21ms/step - loss: 12590.8184 - mse: 12590.8184 - mae: 61.5717 - val_loss: 24.2209 - val_mse: 24.2209 - val_mae: 4.0536\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 76.6316 - mse: 76.6316 - mae: 6.2993 - val_loss: 17.3980 - val_mse: 17.3980 - val_mae: 3.5309\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 71.3656 - mse: 71.3656 - mae: 5.9890 - val_loss: 17.7285 - val_mse: 17.7285 - val_mae: 3.5842\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 70.5921 - mse: 70.5921 - mae: 5.9681 - val_loss: 25.8573 - val_mse: 25.8573 - val_mae: 4.4818\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 69.5592 - mse: 69.5592 - mae: 5.8708 - val_loss: 18.5883 - val_mse: 18.5883 - val_mae: 3.6874\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 67.1105 - mse: 67.1105 - mae: 5.7176 - val_loss: 17.4208 - val_mse: 17.4208 - val_mae: 3.2899\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 68.4271 - mse: 68.4271 - mae: 5.7292 - val_loss: 18.9130 - val_mse: 18.9130 - val_mae: 3.6873\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 69.8995 - mse: 69.8995 - mae: 5.8248 - val_loss: 31.0851 - val_mse: 31.0851 - val_mae: 4.9526\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 66.8342 - mse: 66.8342 - mae: 5.7689 - val_loss: 36.0905 - val_mse: 36.0905 - val_mae: 5.3680\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 66.2468 - mse: 66.2468 - mae: 5.8047 - val_loss: 26.1706 - val_mse: 26.1706 - val_mae: 4.4653\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 66.3458 - mse: 66.3458 - mae: 5.7930 - val_loss: 19.8030 - val_mse: 19.8030 - val_mae: 3.7974\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 66.1097 - mse: 66.1097 - mae: 5.6870 - val_loss: 19.6402 - val_mse: 19.6402 - val_mae: 3.7455\n",
      "Epoch 12: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.5, model__n_hidden=2, model__n_neurons=125, model__optimizer=nesterov; total time=   1.2s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 12046.1719 - mse: 12046.1719 - mae: 70.4043 - val_loss: 34.5262 - val_mse: 34.5262 - val_mae: 5.0644\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 91.2373 - mse: 91.2373 - mae: 7.0278 - val_loss: 21.4969 - val_mse: 21.4969 - val_mae: 3.9167\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 86.1247 - mse: 86.1247 - mae: 6.7247 - val_loss: 27.4727 - val_mse: 27.4727 - val_mae: 4.2973\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 81.9505 - mse: 81.9505 - mae: 6.6602 - val_loss: 24.2907 - val_mse: 24.2907 - val_mae: 4.0768\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 82.0783 - mse: 82.0783 - mae: 6.6245 - val_loss: 18.4542 - val_mse: 18.4542 - val_mae: 3.5210\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 82.0345 - mse: 82.0345 - mae: 6.5018 - val_loss: 16.2587 - val_mse: 16.2587 - val_mae: 3.2546\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 81.6221 - mse: 81.6221 - mae: 6.4738 - val_loss: 18.7223 - val_mse: 18.7223 - val_mae: 3.5362\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 84.7124 - mse: 84.7124 - mae: 6.4690 - val_loss: 28.1491 - val_mse: 28.1491 - val_mae: 4.4188\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 78.7325 - mse: 78.7325 - mae: 6.4146 - val_loss: 35.7538 - val_mse: 35.7538 - val_mae: 4.9799\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 78.7859 - mse: 78.7859 - mae: 6.5162 - val_loss: 26.7922 - val_mse: 26.7922 - val_mae: 4.3172\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 80.3783 - mse: 80.3783 - mae: 6.5677 - val_loss: 18.1466 - val_mse: 18.1466 - val_mae: 3.4738\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 79.3175 - mse: 79.3175 - mae: 6.4247 - val_loss: 25.1278 - val_mse: 25.1278 - val_mae: 4.1972\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 79.0134 - mse: 79.0134 - mae: 6.4485 - val_loss: 23.6478 - val_mse: 23.6478 - val_mae: 4.0308\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 80.8327 - mse: 80.8327 - mae: 6.6242 - val_loss: 19.2038 - val_mse: 19.2038 - val_mae: 3.5865\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 77.3440 - mse: 77.3440 - mae: 6.3553 - val_loss: 20.7347 - val_mse: 20.7347 - val_mae: 3.7569\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 77.8897 - mse: 77.8897 - mae: 6.3930 - val_loss: 19.1851 - val_mse: 19.1851 - val_mae: 3.5842\n",
      "Epoch 16: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.5, model__n_hidden=2, model__n_neurons=125, model__optimizer=nesterov; total time=   1.4s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 36313.1211 - mse: 36313.1211 - mae: 138.3145 - val_loss: 88.2119 - val_mse: 88.2119 - val_mae: 6.5538\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 88.3904 - mse: 88.3904 - mae: 6.7932 - val_loss: 69.5336 - val_mse: 69.5336 - val_mae: 5.7704\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 74.2577 - mse: 74.2577 - mae: 6.1088 - val_loss: 61.0549 - val_mse: 61.0549 - val_mae: 5.4931\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 64.0655 - mse: 64.0655 - mae: 5.4995 - val_loss: 54.0129 - val_mse: 54.0129 - val_mae: 5.3235\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 61.2155 - mse: 61.2155 - mae: 5.4666 - val_loss: 51.0347 - val_mse: 51.0347 - val_mae: 4.9682\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 60.3740 - mse: 60.3740 - mae: 5.3715 - val_loss: 48.5572 - val_mse: 48.5572 - val_mae: 5.0169\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 62.8914 - mse: 62.8914 - mae: 5.6346 - val_loss: 49.7017 - val_mse: 49.7017 - val_mae: 5.3647\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 59.6995 - mse: 59.6995 - mae: 5.5228 - val_loss: 50.4778 - val_mse: 50.4778 - val_mae: 5.0731\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 57.4889 - mse: 57.4889 - mae: 5.2271 - val_loss: 45.6208 - val_mse: 45.6208 - val_mae: 5.0057\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 56.1561 - mse: 56.1561 - mae: 5.3257 - val_loss: 45.0663 - val_mse: 45.0663 - val_mae: 5.0297\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 55.8314 - mse: 55.8314 - mae: 5.2087 - val_loss: 43.5911 - val_mse: 43.5911 - val_mae: 5.0247\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 56.9560 - mse: 56.9560 - mae: 5.3001 - val_loss: 50.5511 - val_mse: 50.5511 - val_mae: 5.1892\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 57.3832 - mse: 57.3832 - mae: 5.2099 - val_loss: 45.7700 - val_mse: 45.7700 - val_mae: 4.9419\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 56.4279 - mse: 56.4279 - mae: 5.1601 - val_loss: 43.2568 - val_mse: 43.2568 - val_mae: 4.9710\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 54.4817 - mse: 54.4817 - mae: 5.1862 - val_loss: 44.8630 - val_mse: 44.8630 - val_mae: 5.0014\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 55.5041 - mse: 55.5041 - mae: 5.1460 - val_loss: 43.0773 - val_mse: 43.0773 - val_mae: 4.9320\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 54.5480 - mse: 54.5480 - mae: 5.2636 - val_loss: 43.7533 - val_mse: 43.7533 - val_mae: 5.0142\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 54.8074 - mse: 54.8074 - mae: 5.2076 - val_loss: 41.1268 - val_mse: 41.1268 - val_mae: 4.9783\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 54.4935 - mse: 54.4935 - mae: 5.1956 - val_loss: 43.8504 - val_mse: 43.8504 - val_mae: 4.9817\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 54.2190 - mse: 54.2190 - mae: 5.0905 - val_loss: 41.3872 - val_mse: 41.3872 - val_mae: 4.9566\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 56.0574 - mse: 56.0574 - mae: 5.2544 - val_loss: 43.1321 - val_mse: 43.1321 - val_mae: 4.9718\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 53.9739 - mse: 53.9739 - mae: 5.0470 - val_loss: 40.5343 - val_mse: 40.5343 - val_mae: 4.8859\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 53.3388 - mse: 53.3388 - mae: 5.2037 - val_loss: 44.5254 - val_mse: 44.5254 - val_mae: 4.9381\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 53.9869 - mse: 53.9869 - mae: 4.9956 - val_loss: 45.8769 - val_mse: 45.8769 - val_mae: 5.3794\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 55.3805 - mse: 55.3805 - mae: 5.2993 - val_loss: 40.2128 - val_mse: 40.2128 - val_mae: 4.8897\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 52.7014 - mse: 52.7014 - mae: 5.0509 - val_loss: 40.6620 - val_mse: 40.6620 - val_mae: 4.9083\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 54.2817 - mse: 54.2817 - mae: 5.2320 - val_loss: 40.5810 - val_mse: 40.5810 - val_mae: 4.8822\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 54.5980 - mse: 54.5980 - mae: 5.2844 - val_loss: 39.2517 - val_mse: 39.2517 - val_mae: 4.8693\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 53.0712 - mse: 53.0712 - mae: 5.1248 - val_loss: 40.2230 - val_mse: 40.2230 - val_mae: 4.8768\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 52.6468 - mse: 52.6468 - mae: 5.0915 - val_loss: 39.9370 - val_mse: 39.9370 - val_mae: 4.7979\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 51.9789 - mse: 51.9789 - mae: 5.0232 - val_loss: 38.6331 - val_mse: 38.6331 - val_mae: 4.8706\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 52.8777 - mse: 52.8777 - mae: 5.0611 - val_loss: 40.3942 - val_mse: 40.3942 - val_mae: 4.9745\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 53.6673 - mse: 53.6673 - mae: 5.2666 - val_loss: 45.4811 - val_mse: 45.4811 - val_mae: 5.1264\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 54.4387 - mse: 54.4387 - mae: 5.1677 - val_loss: 37.9126 - val_mse: 37.9126 - val_mae: 4.8215\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 52.1205 - mse: 52.1205 - mae: 5.0507 - val_loss: 38.1642 - val_mse: 38.1642 - val_mae: 4.7892\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 53.4755 - mse: 53.4755 - mae: 5.1008 - val_loss: 44.7162 - val_mse: 44.7162 - val_mae: 5.0541\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 55.4627 - mse: 55.4627 - mae: 5.2558 - val_loss: 44.4339 - val_mse: 44.4339 - val_mae: 5.0361\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 54.8148 - mse: 54.8148 - mae: 5.2270 - val_loss: 41.8981 - val_mse: 41.8981 - val_mae: 4.9274\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 54.0632 - mse: 54.0632 - mae: 5.1520 - val_loss: 36.8085 - val_mse: 36.8085 - val_mae: 4.8405\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 51.6098 - mse: 51.6098 - mae: 5.0426 - val_loss: 39.3224 - val_mse: 39.3224 - val_mae: 4.7777\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 52.3821 - mse: 52.3821 - mae: 5.0742 - val_loss: 41.3119 - val_mse: 41.3119 - val_mae: 4.8445\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 51.6358 - mse: 51.6358 - mae: 4.8613 - val_loss: 54.6501 - val_mse: 54.6501 - val_mae: 5.9446\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 57.4428 - mse: 57.4428 - mae: 5.4423 - val_loss: 39.3968 - val_mse: 39.3968 - val_mae: 4.8045\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 53.0805 - mse: 53.0805 - mae: 5.0841 - val_loss: 42.0842 - val_mse: 42.0842 - val_mae: 4.8681\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 52.9189 - mse: 52.9189 - mae: 5.0086 - val_loss: 37.3913 - val_mse: 37.3913 - val_mae: 4.7421\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 50.9260 - mse: 50.9260 - mae: 4.9959 - val_loss: 38.7269 - val_mse: 38.7269 - val_mae: 4.6999\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 51.8291 - mse: 51.8291 - mae: 5.0792 - val_loss: 42.5260 - val_mse: 42.5260 - val_mae: 4.9608\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 51.1446 - mse: 51.1446 - mae: 5.0417 - val_loss: 42.8942 - val_mse: 42.8942 - val_mae: 4.9312\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 51.6739 - mse: 51.6739 - mae: 5.0103 - val_loss: 37.0358 - val_mse: 37.0358 - val_mae: 4.7295\n",
      "Epoch 49: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.5, model__n_hidden=2, model__n_neurons=125, model__optimizer=nesterov; total time=   2.8s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 29ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 10: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.9, model__n_hidden=3, model__n_neurons=25, model__optimizer=momentum; total time=   1.2s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 23ms/step - loss: 109371344815217476929066585030656.0000 - mse: 109371344815217476929066585030656.0000 - mae: 3806303096930304.0000 - val_loss: 390873667988030096895115264.0000 - val_mse: 390873667988030096895115264.0000 - val_mae: 19770525089792.0000\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 943209251883022169424265216.0000 - mse: 943209251883022169424265216.0000 - mae: 30076424945664.0000 - val_loss: 1686955778558406623018090496.0000 - val_mse: 1686955778558406623018090496.0000 - val_mae: 41072566730752.0000\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2064609476725548327490289664.0000 - mse: 2064609476725548327490289664.0000 - mae: 45364900003840.0000 - val_loss: 2486869320524303393563344896.0000 - val_mse: 2486869320524303393563344896.0000 - val_mae: 49868521340928.0000\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2656637805287656780290064384.0000 - mse: 2656637805287656780290064384.0000 - mae: 51533144129536.0000 - val_loss: 2830342382514482016302399488.0000 - val_mse: 2830342382514482016302399488.0000 - val_mae: 53200958783488.0000\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2885847897562511108732878848.0000 - mse: 2885847897562511108732878848.0000 - mae: 53719286677504.0000 - val_loss: 2932353172390001016091705344.0000 - val_mse: 2932353172390001016091705344.0000 - val_mae: 54151207714816.0000\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2934537562036233406355865600.0000 - mse: 2934537562036233406355865600.0000 - mae: 54171361345536.0000 - val_loss: 2923276784009925557990981632.0000 - val_mse: 2923276784009925557990981632.0000 - val_mae: 54067334217728.0000\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2902305935051122181008261120.0000 - mse: 2902305935051122181008261120.0000 - mae: 53872873701376.0000 - val_loss: 2866433658655623638553722880.0000 - val_mse: 2866433658655623638553722880.0000 - val_mae: 53539082600448.0000\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2836026045724520813669908480.0000 - mse: 2836026045724520813669908480.0000 - mae: 53253995757568.0000 - val_loss: 2790411232126957013786689536.0000 - val_mse: 2790411232126957013786689536.0000 - val_mae: 52824343838720.0000\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2756594956038937843117064192.0000 - mse: 2756594956038937843117064192.0000 - mae: 52502846242816.0000 - val_loss: 2707727317378392395740086272.0000 - val_mse: 2707727317378392395740086272.0000 - val_mae: 52035835658240.0000\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2673103516621802515738918912.0000 - mse: 2673103516621802515738918912.0000 - mae: 51701587378176.0000 - val_loss: 2623769544271073690897088512.0000 - val_mse: 2623769544271073690897088512.0000 - val_mae: 51222744662016.0000\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2589440301123852805016125440.0000 - mse: 2589440301123852805016125440.0000 - mae: 50886051102720.0000 - val_loss: 2540812912858123350839394304.0000 - val_mse: 2540812912858123350839394304.0000 - val_mae: 50406482771968.0000\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.9, model__n_hidden=3, model__n_neurons=25, model__optimizer=momentum; total time=   1.2s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 29ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 10: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.9, model__n_hidden=3, model__n_neurons=25, model__optimizer=momentum; total time=   1.4s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 19ms/step - loss: 40282.8633 - mse: 40282.8633 - mae: 195.8599 - val_loss: 39208.7305 - val_mse: 39208.7305 - val_mae: 193.2466\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 40279.9141 - mse: 40279.9141 - mae: 195.8526 - val_loss: 39205.7969 - val_mse: 39205.7969 - val_mae: 193.2393\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 40276.9336 - mse: 40276.9336 - mae: 195.8453 - val_loss: 39202.8594 - val_mse: 39202.8594 - val_mae: 193.2320\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40273.9609 - mse: 40273.9609 - mae: 195.8380 - val_loss: 39199.9219 - val_mse: 39199.9219 - val_mae: 193.2247\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40271.0273 - mse: 40271.0273 - mae: 195.8307 - val_loss: 39196.9766 - val_mse: 39196.9766 - val_mae: 193.2174\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 40268.0195 - mse: 40268.0195 - mae: 195.8234 - val_loss: 39194.0586 - val_mse: 39194.0586 - val_mae: 193.2101\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 40265.0859 - mse: 40265.0859 - mae: 195.8162 - val_loss: 39191.1094 - val_mse: 39191.1094 - val_mae: 193.2028\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40262.0898 - mse: 40262.0898 - mae: 195.8088 - val_loss: 39188.1797 - val_mse: 39188.1797 - val_mae: 193.1955\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 40259.1289 - mse: 40259.1289 - mae: 195.8016 - val_loss: 39185.2344 - val_mse: 39185.2344 - val_mae: 193.1882\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 40256.1562 - mse: 40256.1562 - mae: 195.7943 - val_loss: 39182.3047 - val_mse: 39182.3047 - val_mae: 193.1809\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40253.2109 - mse: 40253.2109 - mae: 195.7870 - val_loss: 39179.3555 - val_mse: 39179.3555 - val_mae: 193.1736\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 40250.2344 - mse: 40250.2344 - mae: 195.7797 - val_loss: 39176.4453 - val_mse: 39176.4453 - val_mae: 193.1663\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40247.2695 - mse: 40247.2695 - mae: 195.7724 - val_loss: 39173.5352 - val_mse: 39173.5352 - val_mae: 193.1591\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 40244.3477 - mse: 40244.3477 - mae: 195.7652 - val_loss: 39170.5898 - val_mse: 39170.5898 - val_mae: 193.1518\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 40241.3398 - mse: 40241.3398 - mae: 195.7578 - val_loss: 39167.6680 - val_mse: 39167.6680 - val_mae: 193.1445\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 40238.3711 - mse: 40238.3711 - mae: 195.7506 - val_loss: 39164.7422 - val_mse: 39164.7422 - val_mae: 193.1372\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 40235.4453 - mse: 40235.4453 - mae: 195.7433 - val_loss: 39161.7812 - val_mse: 39161.7812 - val_mae: 193.1299\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40232.4805 - mse: 40232.4805 - mae: 195.7360 - val_loss: 39158.8398 - val_mse: 39158.8398 - val_mae: 193.1225\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40229.4961 - mse: 40229.4961 - mae: 195.7287 - val_loss: 39155.9297 - val_mse: 39155.9297 - val_mae: 193.1153\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40226.5430 - mse: 40226.5430 - mae: 195.7215 - val_loss: 39153.0234 - val_mse: 39153.0234 - val_mae: 193.1080\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 40223.5859 - mse: 40223.5859 - mae: 195.7142 - val_loss: 39150.0938 - val_mse: 39150.0938 - val_mae: 193.1008\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 40220.6328 - mse: 40220.6328 - mae: 195.7069 - val_loss: 39147.1562 - val_mse: 39147.1562 - val_mae: 193.0935\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 40217.6953 - mse: 40217.6953 - mae: 195.6996 - val_loss: 39144.2227 - val_mse: 39144.2227 - val_mae: 193.0862\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 40214.7305 - mse: 40214.7305 - mae: 195.6923 - val_loss: 39141.2930 - val_mse: 39141.2930 - val_mae: 193.0789\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40211.7695 - mse: 40211.7695 - mae: 195.6851 - val_loss: 39138.3828 - val_mse: 39138.3828 - val_mae: 193.0717\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40208.8125 - mse: 40208.8125 - mae: 195.6778 - val_loss: 39135.4688 - val_mse: 39135.4688 - val_mae: 193.0644\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 40205.8555 - mse: 40205.8555 - mae: 195.6705 - val_loss: 39132.5352 - val_mse: 39132.5352 - val_mae: 193.0571\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 40202.9102 - mse: 40202.9102 - mae: 195.6633 - val_loss: 39129.6094 - val_mse: 39129.6094 - val_mae: 193.0498\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 40199.9805 - mse: 40199.9805 - mae: 195.6560 - val_loss: 39126.6758 - val_mse: 39126.6758 - val_mae: 193.0425\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 40196.9688 - mse: 40196.9688 - mae: 195.6487 - val_loss: 39123.7656 - val_mse: 39123.7656 - val_mae: 193.0353\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 40194.0391 - mse: 40194.0391 - mae: 195.6415 - val_loss: 39120.8398 - val_mse: 39120.8398 - val_mae: 193.0280\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40191.0859 - mse: 40191.0859 - mae: 195.6342 - val_loss: 39117.8945 - val_mse: 39117.8945 - val_mae: 193.0206\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40188.1406 - mse: 40188.1406 - mae: 195.6269 - val_loss: 39114.9453 - val_mse: 39114.9453 - val_mae: 193.0133\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 40185.1328 - mse: 40185.1328 - mae: 195.6196 - val_loss: 39112.0430 - val_mse: 39112.0430 - val_mae: 193.0061\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 40182.1992 - mse: 40182.1992 - mae: 195.6123 - val_loss: 39109.1328 - val_mse: 39109.1328 - val_mae: 192.9988\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40179.2227 - mse: 40179.2227 - mae: 195.6051 - val_loss: 39106.2422 - val_mse: 39106.2422 - val_mae: 192.9916\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 40176.3359 - mse: 40176.3359 - mae: 195.5979 - val_loss: 39103.2734 - val_mse: 39103.2734 - val_mae: 192.9843\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40173.3125 - mse: 40173.3125 - mae: 195.5905 - val_loss: 39100.3320 - val_mse: 39100.3320 - val_mae: 192.9769\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40170.3789 - mse: 40170.3789 - mae: 195.5832 - val_loss: 39097.3828 - val_mse: 39097.3828 - val_mae: 192.9696\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 40167.3867 - mse: 40167.3867 - mae: 195.5759 - val_loss: 39094.4688 - val_mse: 39094.4688 - val_mae: 192.9623\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40164.4336 - mse: 40164.4336 - mae: 195.5686 - val_loss: 39091.5430 - val_mse: 39091.5430 - val_mae: 192.9550\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40161.4531 - mse: 40161.4531 - mae: 195.5613 - val_loss: 39088.6406 - val_mse: 39088.6406 - val_mae: 192.9478\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 40158.5000 - mse: 40158.5000 - mae: 195.5541 - val_loss: 39085.7266 - val_mse: 39085.7266 - val_mae: 192.9406\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40155.5898 - mse: 40155.5898 - mae: 195.5468 - val_loss: 39082.7773 - val_mse: 39082.7773 - val_mae: 192.9332\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 40152.5859 - mse: 40152.5859 - mae: 195.5395 - val_loss: 39079.8594 - val_mse: 39079.8594 - val_mae: 192.9260\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40149.6602 - mse: 40149.6602 - mae: 195.5323 - val_loss: 39076.9453 - val_mse: 39076.9453 - val_mae: 192.9187\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40146.7227 - mse: 40146.7227 - mae: 195.5250 - val_loss: 39074.0195 - val_mse: 39074.0195 - val_mae: 192.9114\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 40143.7461 - mse: 40143.7461 - mae: 195.5177 - val_loss: 39071.1211 - val_mse: 39071.1211 - val_mae: 192.9042\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40140.8086 - mse: 40140.8086 - mae: 195.5105 - val_loss: 39068.2031 - val_mse: 39068.2031 - val_mae: 192.8969\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40137.8711 - mse: 40137.8711 - mae: 195.5032 - val_loss: 39065.2734 - val_mse: 39065.2734 - val_mae: 192.8896\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 40134.8984 - mse: 40134.8984 - mae: 195.4959 - val_loss: 39062.3555 - val_mse: 39062.3555 - val_mae: 192.8824\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 40131.9531 - mse: 40131.9531 - mae: 195.4887 - val_loss: 39059.4219 - val_mse: 39059.4219 - val_mae: 192.8751\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 40128.9883 - mse: 40128.9883 - mae: 195.4814 - val_loss: 39056.5039 - val_mse: 39056.5039 - val_mae: 192.8678\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 40126.0312 - mse: 40126.0312 - mae: 195.4741 - val_loss: 39053.5742 - val_mse: 39053.5742 - val_mae: 192.8605\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 40123.0859 - mse: 40123.0859 - mae: 195.4668 - val_loss: 39050.6289 - val_mse: 39050.6289 - val_mae: 192.8531\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 40120.0820 - mse: 40120.0820 - mae: 195.4595 - val_loss: 39047.7227 - val_mse: 39047.7227 - val_mae: 192.8459\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40117.1562 - mse: 40117.1562 - mae: 195.4523 - val_loss: 39044.7852 - val_mse: 39044.7852 - val_mae: 192.8386\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40114.2031 - mse: 40114.2031 - mae: 195.4450 - val_loss: 39041.8555 - val_mse: 39041.8555 - val_mae: 192.8313\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40111.2109 - mse: 40111.2109 - mae: 195.4377 - val_loss: 39038.9414 - val_mse: 39038.9414 - val_mae: 192.8241\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40108.3047 - mse: 40108.3047 - mae: 195.4304 - val_loss: 39035.9844 - val_mse: 39035.9844 - val_mae: 192.8167\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40105.2930 - mse: 40105.2930 - mae: 195.4230 - val_loss: 39033.0664 - val_mse: 39033.0664 - val_mae: 192.8094\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40102.3438 - mse: 40102.3438 - mae: 195.4158 - val_loss: 39030.1523 - val_mse: 39030.1523 - val_mae: 192.8022\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40099.3867 - mse: 40099.3867 - mae: 195.4085 - val_loss: 39027.2266 - val_mse: 39027.2266 - val_mae: 192.7949\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 40096.4219 - mse: 40096.4219 - mae: 195.4012 - val_loss: 39024.2930 - val_mse: 39024.2930 - val_mae: 192.7876\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 40093.4531 - mse: 40093.4531 - mae: 195.3939 - val_loss: 39021.3594 - val_mse: 39021.3594 - val_mae: 192.7803\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40090.5117 - mse: 40090.5117 - mae: 195.3866 - val_loss: 39018.4023 - val_mse: 39018.4023 - val_mae: 192.7729\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40087.5273 - mse: 40087.5273 - mae: 195.3793 - val_loss: 39015.4570 - val_mse: 39015.4570 - val_mae: 192.7655\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40084.5547 - mse: 40084.5547 - mae: 195.3720 - val_loss: 39012.5273 - val_mse: 39012.5273 - val_mae: 192.7582\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40081.6094 - mse: 40081.6094 - mae: 195.3647 - val_loss: 39009.6016 - val_mse: 39009.6016 - val_mae: 192.7509\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40078.6406 - mse: 40078.6406 - mae: 195.3574 - val_loss: 39006.6836 - val_mse: 39006.6836 - val_mae: 192.7437\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40075.6719 - mse: 40075.6719 - mae: 195.3501 - val_loss: 39003.7930 - val_mse: 39003.7930 - val_mae: 192.7365\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 40072.7617 - mse: 40072.7617 - mae: 195.3429 - val_loss: 39000.8516 - val_mse: 39000.8516 - val_mae: 192.7291\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40069.7891 - mse: 40069.7891 - mae: 195.3356 - val_loss: 38997.9492 - val_mse: 38997.9492 - val_mae: 192.7219\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40066.8359 - mse: 40066.8359 - mae: 195.3283 - val_loss: 38995.0234 - val_mse: 38995.0234 - val_mae: 192.7146\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 40063.9023 - mse: 40063.9023 - mae: 195.3211 - val_loss: 38992.0820 - val_mse: 38992.0820 - val_mae: 192.7073\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 40060.9219 - mse: 40060.9219 - mae: 195.3137 - val_loss: 38989.1562 - val_mse: 38989.1562 - val_mae: 192.7000\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 40057.9492 - mse: 40057.9492 - mae: 195.3064 - val_loss: 38986.2344 - val_mse: 38986.2344 - val_mae: 192.6927\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40054.9922 - mse: 40054.9922 - mae: 195.2991 - val_loss: 38983.3008 - val_mse: 38983.3008 - val_mae: 192.6854\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40052.0312 - mse: 40052.0312 - mae: 195.2918 - val_loss: 38980.3750 - val_mse: 38980.3750 - val_mae: 192.6781\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40049.0664 - mse: 40049.0664 - mae: 195.2845 - val_loss: 38977.4414 - val_mse: 38977.4414 - val_mae: 192.6708\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40046.1133 - mse: 40046.1133 - mae: 195.2773 - val_loss: 38974.5000 - val_mse: 38974.5000 - val_mae: 192.6635\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40043.1328 - mse: 40043.1328 - mae: 195.2699 - val_loss: 38971.5664 - val_mse: 38971.5664 - val_mae: 192.6561\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 40040.1914 - mse: 40040.1914 - mae: 195.2626 - val_loss: 38968.6289 - val_mse: 38968.6289 - val_mae: 192.6488\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 40037.2148 - mse: 40037.2148 - mae: 195.2553 - val_loss: 38965.7070 - val_mse: 38965.7070 - val_mae: 192.6415\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40034.2617 - mse: 40034.2617 - mae: 195.2481 - val_loss: 38962.8008 - val_mse: 38962.8008 - val_mae: 192.6343\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40031.3008 - mse: 40031.3008 - mae: 195.2408 - val_loss: 38959.8906 - val_mse: 38959.8906 - val_mae: 192.6270\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40028.3594 - mse: 40028.3594 - mae: 195.2335 - val_loss: 38956.9766 - val_mse: 38956.9766 - val_mae: 192.6198\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40025.4414 - mse: 40025.4414 - mae: 195.2263 - val_loss: 38954.0352 - val_mse: 38954.0352 - val_mae: 192.6124\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40022.4883 - mse: 40022.4883 - mae: 195.2190 - val_loss: 38951.0938 - val_mse: 38951.0938 - val_mae: 192.6051\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40019.5039 - mse: 40019.5039 - mae: 195.2117 - val_loss: 38948.1914 - val_mse: 38948.1914 - val_mae: 192.5979\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40016.5859 - mse: 40016.5859 - mae: 195.2044 - val_loss: 38945.2578 - val_mse: 38945.2578 - val_mae: 192.5906\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 40013.5938 - mse: 40013.5938 - mae: 195.1971 - val_loss: 38942.3711 - val_mse: 38942.3711 - val_mae: 192.5834\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40010.6602 - mse: 40010.6602 - mae: 195.1899 - val_loss: 38939.4453 - val_mse: 38939.4453 - val_mae: 192.5760\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40007.7070 - mse: 40007.7070 - mae: 195.1826 - val_loss: 38936.5273 - val_mse: 38936.5273 - val_mae: 192.5688\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40004.7578 - mse: 40004.7578 - mae: 195.1753 - val_loss: 38933.5938 - val_mse: 38933.5938 - val_mae: 192.5614\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40001.7852 - mse: 40001.7852 - mae: 195.1680 - val_loss: 38930.6719 - val_mse: 38930.6719 - val_mae: 192.5541\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 39998.8477 - mse: 39998.8477 - mae: 195.1607 - val_loss: 38927.7266 - val_mse: 38927.7266 - val_mae: 192.5468\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 39995.8711 - mse: 39995.8711 - mae: 195.1534 - val_loss: 38924.8008 - val_mse: 38924.8008 - val_mae: 192.5395\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 39992.9102 - mse: 39992.9102 - mae: 195.1461 - val_loss: 38921.8594 - val_mse: 38921.8594 - val_mae: 192.5322\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 39989.9492 - mse: 39989.9492 - mae: 195.1388 - val_loss: 38918.9531 - val_mse: 38918.9531 - val_mae: 192.5249\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.5, model__n_hidden=0, model__n_neurons=25, model__optimizer=adam; total time=   5.1s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 22ms/step - loss: 45184.9102 - mse: 45184.9102 - mae: 206.9130 - val_loss: 47868.0312 - val_mse: 47868.0312 - val_mae: 213.6720\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 45181.7969 - mse: 45181.7969 - mae: 206.9058 - val_loss: 47864.7852 - val_mse: 47864.7852 - val_mae: 213.6648\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 45178.6992 - mse: 45178.6992 - mae: 206.8986 - val_loss: 47861.5195 - val_mse: 47861.5195 - val_mae: 213.6574\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 45175.5859 - mse: 45175.5859 - mae: 206.8914 - val_loss: 47858.2695 - val_mse: 47858.2695 - val_mae: 213.6501\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 45172.4688 - mse: 45172.4688 - mae: 206.8842 - val_loss: 47855.0430 - val_mse: 47855.0430 - val_mae: 213.6429\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 45169.3594 - mse: 45169.3594 - mae: 206.8770 - val_loss: 47851.8164 - val_mse: 47851.8164 - val_mae: 213.6356\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 45166.2617 - mse: 45166.2617 - mae: 206.8698 - val_loss: 47848.5781 - val_mse: 47848.5781 - val_mae: 213.6283\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 45163.1680 - mse: 45163.1680 - mae: 206.8626 - val_loss: 47845.3164 - val_mse: 47845.3164 - val_mae: 213.6210\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 45160.0195 - mse: 45160.0195 - mae: 206.8554 - val_loss: 47842.0703 - val_mse: 47842.0703 - val_mae: 213.6137\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 45156.9336 - mse: 45156.9336 - mae: 206.8482 - val_loss: 47838.8203 - val_mse: 47838.8203 - val_mae: 213.6064\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 45153.8086 - mse: 45153.8086 - mae: 206.8409 - val_loss: 47835.5703 - val_mse: 47835.5703 - val_mae: 213.5992\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 45150.7109 - mse: 45150.7109 - mae: 206.8338 - val_loss: 47832.3594 - val_mse: 47832.3594 - val_mae: 213.5919\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 45147.6016 - mse: 45147.6016 - mae: 206.8266 - val_loss: 47829.1172 - val_mse: 47829.1172 - val_mae: 213.5846\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 45144.5391 - mse: 45144.5391 - mae: 206.8194 - val_loss: 47825.8555 - val_mse: 47825.8555 - val_mae: 213.5773\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 45141.4219 - mse: 45141.4219 - mae: 206.8122 - val_loss: 47822.6211 - val_mse: 47822.6211 - val_mae: 213.5700\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 45138.2812 - mse: 45138.2812 - mae: 206.8050 - val_loss: 47819.4023 - val_mse: 47819.4023 - val_mae: 213.5628\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 45135.2188 - mse: 45135.2188 - mae: 206.7978 - val_loss: 47816.1523 - val_mse: 47816.1523 - val_mae: 213.5555\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 45132.1172 - mse: 45132.1172 - mae: 206.7906 - val_loss: 47812.8828 - val_mse: 47812.8828 - val_mae: 213.5482\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 45128.9922 - mse: 45128.9922 - mae: 206.7834 - val_loss: 47809.6836 - val_mse: 47809.6836 - val_mae: 213.5409\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 45125.8828 - mse: 45125.8828 - mae: 206.7762 - val_loss: 47806.4570 - val_mse: 47806.4570 - val_mae: 213.5337\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 45122.7930 - mse: 45122.7930 - mae: 206.7690 - val_loss: 47803.1992 - val_mse: 47803.1992 - val_mae: 213.5264\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 45119.6914 - mse: 45119.6914 - mae: 206.7618 - val_loss: 47799.9219 - val_mse: 47799.9219 - val_mae: 213.5190\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 45116.5312 - mse: 45116.5312 - mae: 206.7545 - val_loss: 47796.6562 - val_mse: 47796.6562 - val_mae: 213.5117\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 45113.4297 - mse: 45113.4297 - mae: 206.7473 - val_loss: 47793.4219 - val_mse: 47793.4219 - val_mae: 213.5044\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 45110.3477 - mse: 45110.3477 - mae: 206.7401 - val_loss: 47790.1953 - val_mse: 47790.1953 - val_mae: 213.4972\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 45107.2578 - mse: 45107.2578 - mae: 206.7329 - val_loss: 47786.9688 - val_mse: 47786.9688 - val_mae: 213.4899\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 45104.1406 - mse: 45104.1406 - mae: 206.7258 - val_loss: 47783.7461 - val_mse: 47783.7461 - val_mae: 213.4826\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 45101.0742 - mse: 45101.0742 - mae: 206.7186 - val_loss: 47780.4961 - val_mse: 47780.4961 - val_mae: 213.4753\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 45097.9609 - mse: 45097.9609 - mae: 206.7114 - val_loss: 47777.2422 - val_mse: 47777.2422 - val_mae: 213.4680\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 45094.8711 - mse: 45094.8711 - mae: 206.7042 - val_loss: 47773.9922 - val_mse: 47773.9922 - val_mae: 213.4607\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 45091.7305 - mse: 45091.7305 - mae: 206.6969 - val_loss: 47770.7852 - val_mse: 47770.7852 - val_mae: 213.4535\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 45088.6602 - mse: 45088.6602 - mae: 206.6898 - val_loss: 47767.5469 - val_mse: 47767.5469 - val_mae: 213.4462\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 45085.5664 - mse: 45085.5664 - mae: 206.6826 - val_loss: 47764.3086 - val_mse: 47764.3086 - val_mae: 213.4390\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 45082.4531 - mse: 45082.4531 - mae: 206.6754 - val_loss: 47761.1172 - val_mse: 47761.1172 - val_mae: 213.4318\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 45079.4062 - mse: 45079.4062 - mae: 206.6683 - val_loss: 47757.8789 - val_mse: 47757.8789 - val_mae: 213.4245\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 45076.2734 - mse: 45076.2734 - mae: 206.6611 - val_loss: 47754.6562 - val_mse: 47754.6562 - val_mae: 213.4173\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 45073.2031 - mse: 45073.2031 - mae: 206.6539 - val_loss: 47751.3945 - val_mse: 47751.3945 - val_mae: 213.4099\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 45070.0820 - mse: 45070.0820 - mae: 206.6467 - val_loss: 47748.1602 - val_mse: 47748.1602 - val_mae: 213.4026\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 45067.0078 - mse: 45067.0078 - mae: 206.6395 - val_loss: 47744.9180 - val_mse: 47744.9180 - val_mae: 213.3953\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 45063.8906 - mse: 45063.8906 - mae: 206.6323 - val_loss: 47741.6914 - val_mse: 47741.6914 - val_mae: 213.3881\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 45060.7656 - mse: 45060.7656 - mae: 206.6251 - val_loss: 47738.4844 - val_mse: 47738.4844 - val_mae: 213.3809\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 45057.7031 - mse: 45057.7031 - mae: 206.6180 - val_loss: 47735.2695 - val_mse: 47735.2695 - val_mae: 213.3736\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 45054.6250 - mse: 45054.6250 - mae: 206.6108 - val_loss: 47732.0430 - val_mse: 47732.0430 - val_mae: 213.3664\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 45051.5508 - mse: 45051.5508 - mae: 206.6036 - val_loss: 47728.7812 - val_mse: 47728.7812 - val_mae: 213.3590\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 45048.3789 - mse: 45048.3789 - mae: 206.5964 - val_loss: 47725.5664 - val_mse: 47725.5664 - val_mae: 213.3518\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 45045.3398 - mse: 45045.3398 - mae: 206.5892 - val_loss: 47722.3203 - val_mse: 47722.3203 - val_mae: 213.3445\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 45042.2266 - mse: 45042.2266 - mae: 206.5820 - val_loss: 47719.0898 - val_mse: 47719.0898 - val_mae: 213.3372\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 45039.1328 - mse: 45039.1328 - mae: 206.5748 - val_loss: 47715.8672 - val_mse: 47715.8672 - val_mae: 213.3300\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 45036.0664 - mse: 45036.0664 - mae: 206.5676 - val_loss: 47712.6250 - val_mse: 47712.6250 - val_mae: 213.3227\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 45032.9375 - mse: 45032.9375 - mae: 206.5605 - val_loss: 47709.4023 - val_mse: 47709.4023 - val_mae: 213.3154\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 45029.8398 - mse: 45029.8398 - mae: 206.5533 - val_loss: 47706.1719 - val_mse: 47706.1719 - val_mae: 213.3082\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 45026.7344 - mse: 45026.7344 - mae: 206.5460 - val_loss: 47702.9023 - val_mse: 47702.9023 - val_mae: 213.3008\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 45023.6016 - mse: 45023.6016 - mae: 206.5388 - val_loss: 47699.6680 - val_mse: 47699.6680 - val_mae: 213.2935\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 45020.4883 - mse: 45020.4883 - mae: 206.5316 - val_loss: 47696.4414 - val_mse: 47696.4414 - val_mae: 213.2863\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 45017.4023 - mse: 45017.4023 - mae: 206.5244 - val_loss: 47693.1680 - val_mse: 47693.1680 - val_mae: 213.2789\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 45014.3008 - mse: 45014.3008 - mae: 206.5171 - val_loss: 47689.9062 - val_mse: 47689.9062 - val_mae: 213.2716\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 45011.1836 - mse: 45011.1836 - mae: 206.5099 - val_loss: 47686.6914 - val_mse: 47686.6914 - val_mae: 213.2644\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 45008.1094 - mse: 45008.1094 - mae: 206.5027 - val_loss: 47683.4531 - val_mse: 47683.4531 - val_mae: 213.2570\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 45004.9883 - mse: 45004.9883 - mae: 206.4955 - val_loss: 47680.2344 - val_mse: 47680.2344 - val_mae: 213.2498\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 45001.9141 - mse: 45001.9141 - mae: 206.4884 - val_loss: 47677.0000 - val_mse: 47677.0000 - val_mae: 213.2425\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 44998.7969 - mse: 44998.7969 - mae: 206.4812 - val_loss: 47673.7812 - val_mse: 47673.7812 - val_mae: 213.2353\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 44995.7227 - mse: 44995.7227 - mae: 206.4740 - val_loss: 47670.5664 - val_mse: 47670.5664 - val_mae: 213.2280\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 44992.6133 - mse: 44992.6133 - mae: 206.4668 - val_loss: 47667.3320 - val_mse: 47667.3320 - val_mae: 213.2208\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 44989.5117 - mse: 44989.5117 - mae: 206.4596 - val_loss: 47664.0898 - val_mse: 47664.0898 - val_mae: 213.2135\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 44986.4180 - mse: 44986.4180 - mae: 206.4524 - val_loss: 47660.8086 - val_mse: 47660.8086 - val_mae: 213.2061\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 44983.3086 - mse: 44983.3086 - mae: 206.4451 - val_loss: 47657.5547 - val_mse: 47657.5547 - val_mae: 213.1988\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 44980.1719 - mse: 44980.1719 - mae: 206.4379 - val_loss: 47654.3477 - val_mse: 47654.3477 - val_mae: 213.1915\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 44977.1172 - mse: 44977.1172 - mae: 206.4307 - val_loss: 47651.0820 - val_mse: 47651.0820 - val_mae: 213.1842\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 44973.9961 - mse: 44973.9961 - mae: 206.4235 - val_loss: 47647.8516 - val_mse: 47647.8516 - val_mae: 213.1769\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 44970.8984 - mse: 44970.8984 - mae: 206.4163 - val_loss: 47644.6328 - val_mse: 47644.6328 - val_mae: 213.1697\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 44967.8164 - mse: 44967.8164 - mae: 206.4091 - val_loss: 47641.4180 - val_mse: 47641.4180 - val_mae: 213.1624\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 44964.7344 - mse: 44964.7344 - mae: 206.4020 - val_loss: 47638.1797 - val_mse: 47638.1797 - val_mae: 213.1552\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 44961.6172 - mse: 44961.6172 - mae: 206.3948 - val_loss: 47634.9766 - val_mse: 47634.9766 - val_mae: 213.1479\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 44958.5156 - mse: 44958.5156 - mae: 206.3876 - val_loss: 47631.7578 - val_mse: 47631.7578 - val_mae: 213.1407\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 44955.4531 - mse: 44955.4531 - mae: 206.3805 - val_loss: 47628.5000 - val_mse: 47628.5000 - val_mae: 213.1334\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 44952.3359 - mse: 44952.3359 - mae: 206.3732 - val_loss: 47625.2539 - val_mse: 47625.2539 - val_mae: 213.1260\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 44949.2617 - mse: 44949.2617 - mae: 206.3661 - val_loss: 47622.0352 - val_mse: 47622.0352 - val_mae: 213.1188\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 44946.1289 - mse: 44946.1289 - mae: 206.3588 - val_loss: 47618.8203 - val_mse: 47618.8203 - val_mae: 213.1115\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 44943.0781 - mse: 44943.0781 - mae: 206.3517 - val_loss: 47615.5508 - val_mse: 47615.5508 - val_mae: 213.1042\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 44939.9609 - mse: 44939.9609 - mae: 206.3444 - val_loss: 47612.3047 - val_mse: 47612.3047 - val_mae: 213.0969\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 44936.8555 - mse: 44936.8555 - mae: 206.3372 - val_loss: 47609.0898 - val_mse: 47609.0898 - val_mae: 213.0896\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 44933.7656 - mse: 44933.7656 - mae: 206.3300 - val_loss: 47605.8672 - val_mse: 47605.8672 - val_mae: 213.0824\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 44930.6836 - mse: 44930.6836 - mae: 206.3229 - val_loss: 47602.6484 - val_mse: 47602.6484 - val_mae: 213.0751\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 44927.5781 - mse: 44927.5781 - mae: 206.3157 - val_loss: 47599.4492 - val_mse: 47599.4492 - val_mae: 213.0679\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 44924.5273 - mse: 44924.5273 - mae: 206.3086 - val_loss: 47596.2070 - val_mse: 47596.2070 - val_mae: 213.0606\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 44921.4453 - mse: 44921.4453 - mae: 206.3013 - val_loss: 47592.9688 - val_mse: 47592.9688 - val_mae: 213.0533\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 44918.3242 - mse: 44918.3242 - mae: 206.2941 - val_loss: 47589.7461 - val_mse: 47589.7461 - val_mae: 213.0461\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 44915.2383 - mse: 44915.2383 - mae: 206.2870 - val_loss: 47586.5469 - val_mse: 47586.5469 - val_mae: 213.0389\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 44912.1562 - mse: 44912.1562 - mae: 206.2798 - val_loss: 47583.3242 - val_mse: 47583.3242 - val_mae: 213.0316\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 44909.0742 - mse: 44909.0742 - mae: 206.2726 - val_loss: 47580.1055 - val_mse: 47580.1055 - val_mae: 213.0243\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 44906.0156 - mse: 44906.0156 - mae: 206.2655 - val_loss: 47576.8320 - val_mse: 47576.8320 - val_mae: 213.0170\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 44902.8516 - mse: 44902.8516 - mae: 206.2582 - val_loss: 47573.6523 - val_mse: 47573.6523 - val_mae: 213.0098\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 44899.8008 - mse: 44899.8008 - mae: 206.2511 - val_loss: 47570.4258 - val_mse: 47570.4258 - val_mae: 213.0025\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 44896.7227 - mse: 44896.7227 - mae: 206.2439 - val_loss: 47567.1836 - val_mse: 47567.1836 - val_mae: 212.9952\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 44893.6016 - mse: 44893.6016 - mae: 206.2367 - val_loss: 47563.9688 - val_mse: 47563.9688 - val_mae: 212.9880\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 44890.5430 - mse: 44890.5430 - mae: 206.2295 - val_loss: 47560.7500 - val_mse: 47560.7500 - val_mae: 212.9807\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 44887.4219 - mse: 44887.4219 - mae: 206.2223 - val_loss: 47557.5312 - val_mse: 47557.5312 - val_mae: 212.9735\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 44884.3633 - mse: 44884.3633 - mae: 206.2152 - val_loss: 47554.2773 - val_mse: 47554.2773 - val_mae: 212.9661\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 44881.2266 - mse: 44881.2266 - mae: 206.2079 - val_loss: 47551.0508 - val_mse: 47551.0508 - val_mae: 212.9589\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 44878.1523 - mse: 44878.1523 - mae: 206.2007 - val_loss: 47547.7969 - val_mse: 47547.7969 - val_mae: 212.9515\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.5, model__n_hidden=0, model__n_neurons=25, model__optimizer=adam; total time=   5.9s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 33874.9961 - mse: 33874.9961 - mae: 162.1302 - val_loss: 34699.7461 - val_mse: 34699.7461 - val_mae: 166.2674\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33872.4609 - mse: 33872.4609 - mae: 162.1231 - val_loss: 34697.0898 - val_mse: 34697.0898 - val_mae: 166.2600\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 33870.0156 - mse: 33870.0156 - mae: 162.1161 - val_loss: 34694.3711 - val_mse: 34694.3711 - val_mae: 166.2526\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33867.4922 - mse: 33867.4922 - mae: 162.1088 - val_loss: 34691.6680 - val_mse: 34691.6680 - val_mae: 166.2451\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 33864.9922 - mse: 33864.9922 - mae: 162.1017 - val_loss: 34688.9531 - val_mse: 34688.9531 - val_mae: 166.2377\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 33862.4531 - mse: 33862.4531 - mae: 162.0945 - val_loss: 34686.2930 - val_mse: 34686.2930 - val_mae: 166.2303\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33859.9570 - mse: 33859.9570 - mae: 162.0874 - val_loss: 34683.6133 - val_mse: 34683.6133 - val_mae: 166.2230\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33857.4648 - mse: 33857.4648 - mae: 162.0803 - val_loss: 34680.9023 - val_mse: 34680.9023 - val_mae: 166.2155\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33854.9414 - mse: 33854.9414 - mae: 162.0731 - val_loss: 34678.2305 - val_mse: 34678.2305 - val_mae: 166.2082\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33852.4531 - mse: 33852.4531 - mae: 162.0660 - val_loss: 34675.5078 - val_mse: 34675.5078 - val_mae: 166.2007\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33849.9219 - mse: 33849.9219 - mae: 162.0589 - val_loss: 34672.8320 - val_mse: 34672.8320 - val_mae: 166.1933\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33847.4258 - mse: 33847.4258 - mae: 162.0517 - val_loss: 34670.1445 - val_mse: 34670.1445 - val_mae: 166.1859\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33844.9727 - mse: 33844.9727 - mae: 162.0446 - val_loss: 34667.4102 - val_mse: 34667.4102 - val_mae: 166.1784\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33842.4062 - mse: 33842.4062 - mae: 162.0373 - val_loss: 34664.7031 - val_mse: 34664.7031 - val_mae: 166.1710\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33839.8984 - mse: 33839.8984 - mae: 162.0302 - val_loss: 34662.0469 - val_mse: 34662.0469 - val_mae: 166.1637\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33837.4023 - mse: 33837.4023 - mae: 162.0231 - val_loss: 34659.3828 - val_mse: 34659.3828 - val_mae: 166.1563\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 33834.9023 - mse: 33834.9023 - mae: 162.0160 - val_loss: 34656.6992 - val_mse: 34656.6992 - val_mae: 166.1490\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33832.4141 - mse: 33832.4141 - mae: 162.0089 - val_loss: 34653.9805 - val_mse: 34653.9805 - val_mae: 166.1415\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33829.9023 - mse: 33829.9023 - mae: 162.0017 - val_loss: 34651.2656 - val_mse: 34651.2656 - val_mae: 166.1340\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33827.3867 - mse: 33827.3867 - mae: 161.9945 - val_loss: 34648.5547 - val_mse: 34648.5547 - val_mae: 166.1266\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33824.8672 - mse: 33824.8672 - mae: 161.9874 - val_loss: 34645.8750 - val_mse: 34645.8750 - val_mae: 166.1192\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33822.3711 - mse: 33822.3711 - mae: 161.9801 - val_loss: 34643.1562 - val_mse: 34643.1562 - val_mae: 166.1117\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33819.8320 - mse: 33819.8320 - mae: 161.9730 - val_loss: 34640.4844 - val_mse: 34640.4844 - val_mae: 166.1044\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33817.3750 - mse: 33817.3750 - mae: 161.9659 - val_loss: 34637.8086 - val_mse: 34637.8086 - val_mae: 166.0970\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33814.8672 - mse: 33814.8672 - mae: 161.9588 - val_loss: 34635.1445 - val_mse: 34635.1445 - val_mae: 166.0897\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33812.3594 - mse: 33812.3594 - mae: 161.9518 - val_loss: 34632.5156 - val_mse: 34632.5156 - val_mae: 166.0824\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33809.9219 - mse: 33809.9219 - mae: 161.9447 - val_loss: 34629.8203 - val_mse: 34629.8203 - val_mae: 166.0750\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33807.4375 - mse: 33807.4375 - mae: 161.9375 - val_loss: 34627.0938 - val_mse: 34627.0938 - val_mae: 166.0675\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 33804.8945 - mse: 33804.8945 - mae: 161.9304 - val_loss: 34624.4688 - val_mse: 34624.4688 - val_mae: 166.0603\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 33802.4180 - mse: 33802.4180 - mae: 161.9234 - val_loss: 34621.8047 - val_mse: 34621.8047 - val_mae: 166.0529\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 33799.9570 - mse: 33799.9570 - mae: 161.9163 - val_loss: 34619.1250 - val_mse: 34619.1250 - val_mae: 166.0455\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33797.4922 - mse: 33797.4922 - mae: 161.9092 - val_loss: 34616.4297 - val_mse: 34616.4297 - val_mae: 166.0381\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33794.9805 - mse: 33794.9805 - mae: 161.9020 - val_loss: 34613.7656 - val_mse: 34613.7656 - val_mae: 166.0308\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33792.4961 - mse: 33792.4961 - mae: 161.8949 - val_loss: 34611.0859 - val_mse: 34611.0859 - val_mae: 166.0234\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33790.0078 - mse: 33790.0078 - mae: 161.8879 - val_loss: 34608.4219 - val_mse: 34608.4219 - val_mae: 166.0161\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33787.5039 - mse: 33787.5039 - mae: 161.8808 - val_loss: 34605.7773 - val_mse: 34605.7773 - val_mae: 166.0088\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33784.9922 - mse: 33784.9922 - mae: 161.8737 - val_loss: 34603.1172 - val_mse: 34603.1172 - val_mae: 166.0015\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33782.5234 - mse: 33782.5234 - mae: 161.8666 - val_loss: 34600.4102 - val_mse: 34600.4102 - val_mae: 165.9940\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33780.0586 - mse: 33780.0586 - mae: 161.8594 - val_loss: 34597.6602 - val_mse: 34597.6602 - val_mae: 165.9865\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 33777.5117 - mse: 33777.5117 - mae: 161.8521 - val_loss: 34594.9648 - val_mse: 34594.9648 - val_mae: 165.9790\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 33774.9805 - mse: 33774.9805 - mae: 161.8450 - val_loss: 34592.3164 - val_mse: 34592.3164 - val_mae: 165.9717\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33772.4922 - mse: 33772.4922 - mae: 161.8380 - val_loss: 34589.6680 - val_mse: 34589.6680 - val_mae: 165.9644\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33770.0430 - mse: 33770.0430 - mae: 161.8309 - val_loss: 34586.9570 - val_mse: 34586.9570 - val_mae: 165.9570\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33767.5078 - mse: 33767.5078 - mae: 161.8237 - val_loss: 34584.2578 - val_mse: 34584.2578 - val_mae: 165.9495\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33765.0039 - mse: 33765.0039 - mae: 161.8165 - val_loss: 34581.5391 - val_mse: 34581.5391 - val_mae: 165.9421\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33762.4570 - mse: 33762.4570 - mae: 161.8094 - val_loss: 34578.9102 - val_mse: 34578.9102 - val_mae: 165.9348\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33760.0039 - mse: 33760.0039 - mae: 161.8022 - val_loss: 34576.1914 - val_mse: 34576.1914 - val_mae: 165.9273\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 33757.4883 - mse: 33757.4883 - mae: 161.7950 - val_loss: 34573.4844 - val_mse: 34573.4844 - val_mae: 165.9198\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 33755.0117 - mse: 33755.0117 - mae: 161.7879 - val_loss: 34570.7578 - val_mse: 34570.7578 - val_mae: 165.9123\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 33752.4297 - mse: 33752.4297 - mae: 161.7807 - val_loss: 34568.0938 - val_mse: 34568.0938 - val_mae: 165.9050\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 33749.9492 - mse: 33749.9492 - mae: 161.7736 - val_loss: 34565.4062 - val_mse: 34565.4062 - val_mae: 165.8976\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33747.4375 - mse: 33747.4375 - mae: 161.7664 - val_loss: 34562.7227 - val_mse: 34562.7227 - val_mae: 165.8902\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33744.9688 - mse: 33744.9688 - mae: 161.7593 - val_loss: 34559.9844 - val_mse: 34559.9844 - val_mae: 165.8826\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33742.4375 - mse: 33742.4375 - mae: 161.7520 - val_loss: 34557.3008 - val_mse: 34557.3008 - val_mae: 165.8753\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33739.9297 - mse: 33739.9297 - mae: 161.7449 - val_loss: 34554.6719 - val_mse: 34554.6719 - val_mae: 165.8680\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 33737.4805 - mse: 33737.4805 - mae: 161.7379 - val_loss: 34551.9727 - val_mse: 34551.9727 - val_mae: 165.8606\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33734.9844 - mse: 33734.9844 - mae: 161.7308 - val_loss: 34549.2969 - val_mse: 34549.2969 - val_mae: 165.8532\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33732.4883 - mse: 33732.4883 - mae: 161.7236 - val_loss: 34546.6055 - val_mse: 34546.6055 - val_mae: 165.8458\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33729.9883 - mse: 33729.9883 - mae: 161.7165 - val_loss: 34543.9727 - val_mse: 34543.9727 - val_mae: 165.8385\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33727.5273 - mse: 33727.5273 - mae: 161.7095 - val_loss: 34541.3164 - val_mse: 34541.3164 - val_mae: 165.8312\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33725.0977 - mse: 33725.0977 - mae: 161.7024 - val_loss: 34538.6016 - val_mse: 34538.6016 - val_mae: 165.8237\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33722.5352 - mse: 33722.5352 - mae: 161.6952 - val_loss: 34535.9688 - val_mse: 34535.9688 - val_mae: 165.8165\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33720.0703 - mse: 33720.0703 - mae: 161.6882 - val_loss: 34533.3438 - val_mse: 34533.3438 - val_mae: 165.8092\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 33717.6562 - mse: 33717.6562 - mae: 161.6812 - val_loss: 34530.6289 - val_mse: 34530.6289 - val_mae: 165.8017\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33715.0781 - mse: 33715.0781 - mae: 161.6740 - val_loss: 34527.9805 - val_mse: 34527.9805 - val_mae: 165.7944\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33712.6211 - mse: 33712.6211 - mae: 161.6669 - val_loss: 34525.2852 - val_mse: 34525.2852 - val_mae: 165.7870\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33710.1211 - mse: 33710.1211 - mae: 161.6597 - val_loss: 34522.5977 - val_mse: 34522.5977 - val_mae: 165.7796\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33707.6094 - mse: 33707.6094 - mae: 161.6526 - val_loss: 34519.8984 - val_mse: 34519.8984 - val_mae: 165.7722\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 33705.0977 - mse: 33705.0977 - mae: 161.6454 - val_loss: 34517.2031 - val_mse: 34517.2031 - val_mae: 165.7647\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 33702.6094 - mse: 33702.6094 - mae: 161.6382 - val_loss: 34514.5273 - val_mse: 34514.5273 - val_mae: 165.7573\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33700.1406 - mse: 33700.1406 - mae: 161.6311 - val_loss: 34511.8281 - val_mse: 34511.8281 - val_mae: 165.7499\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33697.6211 - mse: 33697.6211 - mae: 161.6240 - val_loss: 34509.2031 - val_mse: 34509.2031 - val_mae: 165.7427\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33695.1797 - mse: 33695.1797 - mae: 161.6170 - val_loss: 34506.5312 - val_mse: 34506.5312 - val_mae: 165.7353\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33692.6602 - mse: 33692.6602 - mae: 161.6098 - val_loss: 34503.8516 - val_mse: 34503.8516 - val_mae: 165.7279\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33690.1758 - mse: 33690.1758 - mae: 161.6027 - val_loss: 34501.1328 - val_mse: 34501.1328 - val_mae: 165.7204\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 33687.6484 - mse: 33687.6484 - mae: 161.5955 - val_loss: 34498.4609 - val_mse: 34498.4609 - val_mae: 165.7130\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33685.1406 - mse: 33685.1406 - mae: 161.5884 - val_loss: 34495.7969 - val_mse: 34495.7969 - val_mae: 165.7057\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33682.6562 - mse: 33682.6562 - mae: 161.5813 - val_loss: 34493.1016 - val_mse: 34493.1016 - val_mae: 165.6982\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 33680.1523 - mse: 33680.1523 - mae: 161.5742 - val_loss: 34490.4570 - val_mse: 34490.4570 - val_mae: 165.6909\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33677.6992 - mse: 33677.6992 - mae: 161.5670 - val_loss: 34487.7305 - val_mse: 34487.7305 - val_mae: 165.6835\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 33675.2148 - mse: 33675.2148 - mae: 161.5598 - val_loss: 34485.0078 - val_mse: 34485.0078 - val_mae: 165.6759\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33672.6641 - mse: 33672.6641 - mae: 161.5527 - val_loss: 34482.3672 - val_mse: 34482.3672 - val_mae: 165.6687\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 33670.1992 - mse: 33670.1992 - mae: 161.5456 - val_loss: 34479.7227 - val_mse: 34479.7227 - val_mae: 165.6613\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33667.7422 - mse: 33667.7422 - mae: 161.5386 - val_loss: 34477.0977 - val_mse: 34477.0977 - val_mae: 165.6541\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33665.2344 - mse: 33665.2344 - mae: 161.5316 - val_loss: 34474.4531 - val_mse: 34474.4531 - val_mae: 165.6468\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 33662.7656 - mse: 33662.7656 - mae: 161.5244 - val_loss: 34471.7578 - val_mse: 34471.7578 - val_mae: 165.6394\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 33660.2891 - mse: 33660.2891 - mae: 161.5173 - val_loss: 34469.0312 - val_mse: 34469.0312 - val_mae: 165.6318\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33657.7500 - mse: 33657.7500 - mae: 161.5101 - val_loss: 34466.3516 - val_mse: 34466.3516 - val_mae: 165.6244\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33655.2500 - mse: 33655.2500 - mae: 161.5029 - val_loss: 34463.6875 - val_mse: 34463.6875 - val_mae: 165.6171\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33652.7539 - mse: 33652.7539 - mae: 161.4958 - val_loss: 34461.0039 - val_mse: 34461.0039 - val_mae: 165.6097\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 33650.2656 - mse: 33650.2656 - mae: 161.4886 - val_loss: 34458.2812 - val_mse: 34458.2812 - val_mae: 165.6022\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 33647.7695 - mse: 33647.7695 - mae: 161.4814 - val_loss: 34455.5820 - val_mse: 34455.5820 - val_mae: 165.5947\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33645.2734 - mse: 33645.2734 - mae: 161.4743 - val_loss: 34452.9180 - val_mse: 34452.9180 - val_mae: 165.5874\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33642.7383 - mse: 33642.7383 - mae: 161.4672 - val_loss: 34450.2930 - val_mse: 34450.2930 - val_mae: 165.5802\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 33640.3164 - mse: 33640.3164 - mae: 161.4601 - val_loss: 34447.5586 - val_mse: 34447.5586 - val_mae: 165.5726\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 33637.7578 - mse: 33637.7578 - mae: 161.4529 - val_loss: 34444.8984 - val_mse: 34444.8984 - val_mae: 165.5652\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33635.3203 - mse: 33635.3203 - mae: 161.4458 - val_loss: 34442.1953 - val_mse: 34442.1953 - val_mae: 165.5578\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 33632.7656 - mse: 33632.7656 - mae: 161.4387 - val_loss: 34439.5547 - val_mse: 34439.5547 - val_mae: 165.5505\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33630.3281 - mse: 33630.3281 - mae: 161.4316 - val_loss: 34436.8750 - val_mse: 34436.8750 - val_mae: 165.5431\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 33627.8047 - mse: 33627.8047 - mae: 161.4245 - val_loss: 34434.2461 - val_mse: 34434.2461 - val_mae: 165.5358\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.5, model__n_hidden=0, model__n_neurons=25, model__optimizer=adam; total time=   5.3s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 821.0828 - mse: 821.0828 - mae: 25.0389 - val_loss: 448.3210 - val_mse: 448.3210 - val_mae: 20.2536\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 596.9398 - mse: 596.9398 - mae: 22.6643 - val_loss: 446.8567 - val_mse: 446.8567 - val_mae: 20.2174\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 595.3027 - mse: 595.3027 - mae: 22.6278 - val_loss: 445.4037 - val_mse: 445.4037 - val_mae: 20.1814\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 593.6760 - mse: 593.6760 - mae: 22.5920 - val_loss: 443.9324 - val_mse: 443.9324 - val_mae: 20.1450\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 592.0309 - mse: 592.0309 - mae: 22.5556 - val_loss: 442.4823 - val_mse: 442.4823 - val_mae: 20.1089\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 590.4092 - mse: 590.4092 - mae: 22.5192 - val_loss: 441.0436 - val_mse: 441.0436 - val_mae: 20.0731\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 588.7981 - mse: 588.7981 - mae: 22.4837 - val_loss: 439.6061 - val_mse: 439.6061 - val_mae: 20.0373\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 587.1885 - mse: 587.1885 - mae: 22.4482 - val_loss: 438.1660 - val_mse: 438.1660 - val_mae: 20.0013\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 585.5751 - mse: 585.5751 - mae: 22.4120 - val_loss: 436.7217 - val_mse: 436.7217 - val_mae: 19.9652\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 583.9587 - mse: 583.9587 - mae: 22.3758 - val_loss: 435.2997 - val_mse: 435.2997 - val_mae: 19.9295\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 582.3654 - mse: 582.3654 - mae: 22.3405 - val_loss: 433.8738 - val_mse: 433.8738 - val_mae: 19.8937\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 580.7681 - mse: 580.7681 - mae: 22.3046 - val_loss: 432.4544 - val_mse: 432.4544 - val_mae: 19.8580\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 579.1769 - mse: 579.1769 - mae: 22.2690 - val_loss: 431.0311 - val_mse: 431.0311 - val_mae: 19.8222\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 577.5834 - mse: 577.5834 - mae: 22.2330 - val_loss: 429.6241 - val_mse: 429.6241 - val_mae: 19.7866\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 576.0051 - mse: 576.0051 - mae: 22.1974 - val_loss: 428.2195 - val_mse: 428.2195 - val_mae: 19.7511\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 574.4318 - mse: 574.4318 - mae: 22.1617 - val_loss: 426.8315 - val_mse: 426.8315 - val_mae: 19.7160\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 572.8738 - mse: 572.8738 - mae: 22.1271 - val_loss: 425.4362 - val_mse: 425.4362 - val_mae: 19.6805\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 571.3085 - mse: 571.3085 - mae: 22.0917 - val_loss: 424.0445 - val_mse: 424.0445 - val_mae: 19.6451\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 569.7476 - mse: 569.7476 - mae: 22.0568 - val_loss: 422.6569 - val_mse: 422.6569 - val_mae: 19.6098\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 568.1901 - mse: 568.1901 - mae: 22.0210 - val_loss: 421.2704 - val_mse: 421.2704 - val_mae: 19.5744\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 566.6338 - mse: 566.6338 - mae: 21.9854 - val_loss: 419.8884 - val_mse: 419.8884 - val_mae: 19.5391\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 565.0838 - mse: 565.0838 - mae: 21.9502 - val_loss: 418.5244 - val_mse: 418.5244 - val_mae: 19.5041\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 563.5512 - mse: 563.5512 - mae: 21.9156 - val_loss: 417.1535 - val_mse: 417.1535 - val_mae: 19.4690\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 562.0114 - mse: 562.0114 - mae: 21.8802 - val_loss: 415.7862 - val_mse: 415.7862 - val_mae: 19.4338\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 560.4758 - mse: 560.4758 - mae: 21.8448 - val_loss: 414.4243 - val_mse: 414.4243 - val_mae: 19.3988\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 558.9455 - mse: 558.9455 - mae: 21.8099 - val_loss: 413.0671 - val_mse: 413.0671 - val_mae: 19.3637\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 557.4214 - mse: 557.4214 - mae: 21.7752 - val_loss: 411.7178 - val_mse: 411.7178 - val_mae: 19.3289\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 555.9052 - mse: 555.9052 - mae: 21.7401 - val_loss: 410.3740 - val_mse: 410.3740 - val_mae: 19.2941\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 554.3934 - mse: 554.3934 - mae: 21.7056 - val_loss: 409.0275 - val_mse: 409.0275 - val_mae: 19.2591\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 552.8809 - mse: 552.8809 - mae: 21.6702 - val_loss: 407.6995 - val_mse: 407.6995 - val_mae: 19.2246\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 551.3876 - mse: 551.3876 - mae: 21.6358 - val_loss: 406.3796 - val_mse: 406.3796 - val_mae: 19.1903\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 549.9014 - mse: 549.9014 - mae: 21.6017 - val_loss: 405.0429 - val_mse: 405.0429 - val_mae: 19.1554\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 548.3978 - mse: 548.3978 - mae: 21.5667 - val_loss: 403.7156 - val_mse: 403.7156 - val_mae: 19.1207\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 546.9040 - mse: 546.9040 - mae: 21.5323 - val_loss: 402.3914 - val_mse: 402.3914 - val_mae: 19.0861\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 545.4142 - mse: 545.4142 - mae: 21.4977 - val_loss: 401.0751 - val_mse: 401.0751 - val_mae: 19.0516\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 543.9332 - mse: 543.9332 - mae: 21.4626 - val_loss: 399.7730 - val_mse: 399.7730 - val_mae: 19.0174\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 542.4667 - mse: 542.4667 - mae: 21.4287 - val_loss: 398.4743 - val_mse: 398.4743 - val_mae: 18.9832\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 541.0051 - mse: 541.0051 - mae: 21.3948 - val_loss: 397.1880 - val_mse: 397.1880 - val_mae: 18.9493\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 539.5546 - mse: 539.5546 - mae: 21.3609 - val_loss: 395.8869 - val_mse: 395.8869 - val_mae: 18.9149\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 538.0893 - mse: 538.0893 - mae: 21.3269 - val_loss: 394.5913 - val_mse: 394.5913 - val_mae: 18.8806\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 536.6292 - mse: 536.6292 - mae: 21.2920 - val_loss: 393.3059 - val_mse: 393.3059 - val_mae: 18.8466\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 535.1806 - mse: 535.1806 - mae: 21.2581 - val_loss: 392.0270 - val_mse: 392.0270 - val_mae: 18.8126\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 533.7395 - mse: 533.7395 - mae: 21.2239 - val_loss: 390.7569 - val_mse: 390.7569 - val_mae: 18.7788\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 532.3065 - mse: 532.3065 - mae: 21.1905 - val_loss: 389.4826 - val_mse: 389.4826 - val_mae: 18.7449\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 530.8698 - mse: 530.8698 - mae: 21.1562 - val_loss: 388.2131 - val_mse: 388.2131 - val_mae: 18.7110\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 529.4378 - mse: 529.4378 - mae: 21.1223 - val_loss: 386.9550 - val_mse: 386.9550 - val_mae: 18.6773\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 528.0193 - mse: 528.0193 - mae: 21.0889 - val_loss: 385.7021 - val_mse: 385.7021 - val_mae: 18.6437\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 526.6034 - mse: 526.6034 - mae: 21.0553 - val_loss: 384.4388 - val_mse: 384.4388 - val_mae: 18.6098\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 525.1779 - mse: 525.1779 - mae: 21.0216 - val_loss: 383.1863 - val_mse: 383.1863 - val_mae: 18.5762\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 523.7642 - mse: 523.7642 - mae: 20.9880 - val_loss: 381.9417 - val_mse: 381.9417 - val_mae: 18.5426\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 522.3582 - mse: 522.3582 - mae: 20.9546 - val_loss: 380.6943 - val_mse: 380.6943 - val_mae: 18.5090\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 520.9501 - mse: 520.9501 - mae: 20.9207 - val_loss: 379.4545 - val_mse: 379.4545 - val_mae: 18.4754\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 519.5491 - mse: 519.5491 - mae: 20.8873 - val_loss: 378.2150 - val_mse: 378.2150 - val_mae: 18.4419\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 518.1505 - mse: 518.1505 - mae: 20.8533 - val_loss: 376.9984 - val_mse: 376.9984 - val_mae: 18.4088\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 516.7738 - mse: 516.7738 - mae: 20.8207 - val_loss: 375.7722 - val_mse: 375.7722 - val_mae: 18.3755\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 515.3875 - mse: 515.3875 - mae: 20.7873 - val_loss: 374.5482 - val_mse: 374.5482 - val_mae: 18.3422\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 514.0045 - mse: 514.0045 - mae: 20.7540 - val_loss: 373.3382 - val_mse: 373.3382 - val_mae: 18.3092\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 512.6352 - mse: 512.6353 - mae: 20.7213 - val_loss: 372.1187 - val_mse: 372.1187 - val_mae: 18.2758\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 511.2568 - mse: 511.2568 - mae: 20.6876 - val_loss: 370.9137 - val_mse: 370.9137 - val_mae: 18.2428\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 509.8932 - mse: 509.8932 - mae: 20.6546 - val_loss: 369.7098 - val_mse: 369.7098 - val_mae: 18.2098\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 508.5318 - mse: 508.5318 - mae: 20.6219 - val_loss: 368.5155 - val_mse: 368.5155 - val_mae: 18.1770\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 507.1791 - mse: 507.1791 - mae: 20.5885 - val_loss: 367.3218 - val_mse: 367.3218 - val_mae: 18.1441\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 505.8279 - mse: 505.8279 - mae: 20.5559 - val_loss: 366.1330 - val_mse: 366.1330 - val_mae: 18.1113\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 504.4810 - mse: 504.4810 - mae: 20.5233 - val_loss: 364.9378 - val_mse: 364.9378 - val_mae: 18.0783\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 503.1294 - mse: 503.1294 - mae: 20.4905 - val_loss: 363.7664 - val_mse: 363.7664 - val_mae: 18.0459\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 501.8012 - mse: 501.8012 - mae: 20.4580 - val_loss: 362.5895 - val_mse: 362.5895 - val_mae: 18.0132\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 500.4676 - mse: 500.4676 - mae: 20.4254 - val_loss: 361.4134 - val_mse: 361.4134 - val_mae: 17.9806\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 499.1341 - mse: 499.1341 - mae: 20.3927 - val_loss: 360.2330 - val_mse: 360.2330 - val_mae: 17.9477\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 497.7973 - mse: 497.7973 - mae: 20.3592 - val_loss: 359.0629 - val_mse: 359.0629 - val_mae: 17.9151\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 496.4701 - mse: 496.4701 - mae: 20.3272 - val_loss: 357.9021 - val_mse: 357.9021 - val_mae: 17.8827\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 495.1551 - mse: 495.1551 - mae: 20.2943 - val_loss: 356.7555 - val_mse: 356.7555 - val_mae: 17.8506\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 493.8528 - mse: 493.8528 - mae: 20.2630 - val_loss: 355.5948 - val_mse: 355.5948 - val_mae: 17.8180\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 492.5370 - mse: 492.5370 - mae: 20.2298 - val_loss: 354.4524 - val_mse: 354.4524 - val_mae: 17.7859\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 491.2393 - mse: 491.2393 - mae: 20.1982 - val_loss: 353.2957 - val_mse: 353.2957 - val_mae: 17.7534\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 489.9274 - mse: 489.9274 - mae: 20.1656 - val_loss: 352.1544 - val_mse: 352.1544 - val_mae: 17.7212\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 488.6332 - mse: 488.6332 - mae: 20.1339 - val_loss: 351.0258 - val_mse: 351.0258 - val_mae: 17.6893\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 487.3496 - mse: 487.3496 - mae: 20.1013 - val_loss: 349.8842 - val_mse: 349.8842 - val_mae: 17.6570\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 486.0532 - mse: 486.0532 - mae: 20.0689 - val_loss: 348.7511 - val_mse: 348.7511 - val_mae: 17.6249\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 484.7660 - mse: 484.7660 - mae: 20.0372 - val_loss: 347.6234 - val_mse: 347.6234 - val_mae: 17.5929\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 483.4849 - mse: 483.4849 - mae: 20.0050 - val_loss: 346.4914 - val_mse: 346.4914 - val_mae: 17.5607\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 482.2002 - mse: 482.2002 - mae: 19.9726 - val_loss: 345.3856 - val_mse: 345.3856 - val_mae: 17.5292\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 480.9409 - mse: 480.9409 - mae: 19.9417 - val_loss: 344.2671 - val_mse: 344.2671 - val_mae: 17.4973\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 479.6693 - mse: 479.6693 - mae: 19.9095 - val_loss: 343.1519 - val_mse: 343.1519 - val_mae: 17.4654\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 478.4010 - mse: 478.4010 - mae: 19.8778 - val_loss: 342.0416 - val_mse: 342.0416 - val_mae: 17.4335\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 477.1390 - mse: 477.1390 - mae: 19.8463 - val_loss: 340.9400 - val_mse: 340.9400 - val_mae: 17.4019\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 475.8843 - mse: 475.8843 - mae: 19.8146 - val_loss: 339.8311 - val_mse: 339.8311 - val_mae: 17.3700\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 474.6232 - mse: 474.6232 - mae: 19.7823 - val_loss: 338.7379 - val_mse: 338.7379 - val_mae: 17.3385\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 473.3778 - mse: 473.3778 - mae: 19.7513 - val_loss: 337.6334 - val_mse: 337.6334 - val_mae: 17.3067\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 472.1221 - mse: 472.1221 - mae: 19.7194 - val_loss: 336.5500 - val_mse: 336.5500 - val_mae: 17.2753\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 470.8875 - mse: 470.8875 - mae: 19.6875 - val_loss: 335.4648 - val_mse: 335.4648 - val_mae: 17.2439\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 469.6512 - mse: 469.6512 - mae: 19.6566 - val_loss: 334.3803 - val_mse: 334.3803 - val_mae: 17.2124\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 468.4168 - mse: 468.4168 - mae: 19.6249 - val_loss: 333.3068 - val_mse: 333.3068 - val_mae: 17.1812\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 467.1931 - mse: 467.1931 - mae: 19.5940 - val_loss: 332.2344 - val_mse: 332.2344 - val_mae: 17.1500\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 465.9701 - mse: 465.9701 - mae: 19.5629 - val_loss: 331.1609 - val_mse: 331.1609 - val_mae: 17.1186\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 464.7462 - mse: 464.7462 - mae: 19.5311 - val_loss: 330.0906 - val_mse: 330.0906 - val_mae: 17.0874\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 463.5264 - mse: 463.5264 - mae: 19.5001 - val_loss: 329.0318 - val_mse: 329.0318 - val_mae: 17.0563\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 462.3174 - mse: 462.3174 - mae: 19.4694 - val_loss: 327.9588 - val_mse: 327.9588 - val_mae: 17.0249\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 461.0943 - mse: 461.0943 - mae: 19.4371 - val_loss: 326.9031 - val_mse: 326.9031 - val_mae: 16.9938\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 459.8888 - mse: 459.8888 - mae: 19.4065 - val_loss: 325.8411 - val_mse: 325.8411 - val_mae: 16.9626\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 458.6774 - mse: 458.6774 - mae: 19.3759 - val_loss: 324.7862 - val_mse: 324.7862 - val_mae: 16.9314\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.1, model__n_hidden=3, model__n_neurons=5, model__optimizer=nesterov; total time=   5.7s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 23ms/step - loss: 538.7467 - mse: 538.7467 - mae: 19.5083 - val_loss: 447.1972 - val_mse: 447.1972 - val_mae: 20.1970\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 630.4819 - mse: 630.4819 - mae: 22.7728 - val_loss: 440.2802 - val_mse: 440.2802 - val_mae: 19.6738\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 626.6893 - mse: 626.6893 - mae: 22.6549 - val_loss: 437.0969 - val_mse: 437.0969 - val_mae: 19.6444\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 625.6112 - mse: 625.6112 - mae: 22.7011 - val_loss: 435.2903 - val_mse: 435.2903 - val_mae: 19.5178\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 622.2675 - mse: 622.2675 - mae: 22.5740 - val_loss: 433.2923 - val_mse: 433.2923 - val_mae: 19.5032\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 619.5059 - mse: 619.5059 - mae: 22.4753 - val_loss: 432.0540 - val_mse: 432.0540 - val_mae: 19.4502\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 617.4269 - mse: 617.4269 - mae: 22.4072 - val_loss: 430.0793 - val_mse: 430.0793 - val_mae: 19.4112\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 616.0862 - mse: 616.0862 - mae: 22.4254 - val_loss: 428.0707 - val_mse: 428.0707 - val_mae: 19.3105\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 613.3177 - mse: 613.3177 - mae: 22.3238 - val_loss: 426.5927 - val_mse: 426.5927 - val_mae: 19.2872\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 611.6340 - mse: 611.6340 - mae: 22.2833 - val_loss: 425.0348 - val_mse: 425.0348 - val_mae: 19.2512\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 609.4117 - mse: 609.4117 - mae: 22.2224 - val_loss: 423.0598 - val_mse: 423.0598 - val_mae: 19.1628\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 607.5021 - mse: 607.5021 - mae: 22.1816 - val_loss: 421.6266 - val_mse: 421.6266 - val_mae: 19.1258\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 605.5800 - mse: 605.5800 - mae: 22.1270 - val_loss: 420.0107 - val_mse: 420.0107 - val_mae: 19.1049\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 603.6127 - mse: 603.6127 - mae: 22.0795 - val_loss: 418.2714 - val_mse: 418.2714 - val_mae: 19.0337\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 601.7004 - mse: 601.7004 - mae: 22.0175 - val_loss: 416.7905 - val_mse: 416.7905 - val_mae: 18.9796\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 600.0262 - mse: 600.0262 - mae: 21.9934 - val_loss: 415.7383 - val_mse: 415.7383 - val_mae: 18.9905\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 598.1190 - mse: 598.1190 - mae: 21.9406 - val_loss: 413.8822 - val_mse: 413.8822 - val_mae: 18.9231\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 596.2122 - mse: 596.2122 - mae: 21.9146 - val_loss: 412.0122 - val_mse: 412.0122 - val_mae: 18.8397\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 594.1122 - mse: 594.1122 - mae: 21.8436 - val_loss: 410.6736 - val_mse: 410.6736 - val_mae: 18.8299\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 592.2367 - mse: 592.2367 - mae: 21.8043 - val_loss: 409.0290 - val_mse: 409.0290 - val_mae: 18.7815\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 590.4626 - mse: 590.4626 - mae: 21.7651 - val_loss: 407.4174 - val_mse: 407.4174 - val_mae: 18.7372\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 588.4796 - mse: 588.4796 - mae: 21.7142 - val_loss: 405.6887 - val_mse: 405.6887 - val_mae: 18.6794\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 586.4851 - mse: 586.4851 - mae: 21.6656 - val_loss: 403.8629 - val_mse: 403.8629 - val_mae: 18.6457\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 584.5295 - mse: 584.5295 - mae: 21.6069 - val_loss: 402.3355 - val_mse: 402.3355 - val_mae: 18.5826\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 582.6183 - mse: 582.6183 - mae: 21.5704 - val_loss: 400.9389 - val_mse: 400.9389 - val_mae: 18.5698\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 580.5071 - mse: 580.5071 - mae: 21.5175 - val_loss: 399.0200 - val_mse: 399.0200 - val_mae: 18.5459\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 577.1866 - mse: 577.1866 - mae: 21.4439 - val_loss: 395.6464 - val_mse: 395.6464 - val_mae: 18.4241\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 573.6555 - mse: 573.6555 - mae: 21.3586 - val_loss: 392.3610 - val_mse: 392.3610 - val_mae: 18.3380\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 570.1476 - mse: 570.1476 - mae: 21.2742 - val_loss: 388.9504 - val_mse: 388.9504 - val_mae: 18.2478\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 566.4344 - mse: 566.4344 - mae: 21.1909 - val_loss: 385.3931 - val_mse: 385.3931 - val_mae: 18.1342\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 562.6538 - mse: 562.6538 - mae: 21.0941 - val_loss: 381.7277 - val_mse: 381.7277 - val_mae: 18.0413\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 558.1910 - mse: 558.1910 - mae: 20.9749 - val_loss: 375.3936 - val_mse: 375.3936 - val_mae: 18.0358\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 514.3829 - mse: 514.3829 - mae: 19.4301 - val_loss: 476.0915 - val_mse: 476.0915 - val_mae: 21.1408\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 579.5074 - mse: 579.5074 - mae: 21.7482 - val_loss: 347.7420 - val_mse: 347.7420 - val_mae: 17.0189\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 317.4214 - mse: 317.4214 - mae: 14.4264 - val_loss: 482.0304 - val_mse: 482.0304 - val_mae: 17.5559\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 293.5487 - mse: 293.5487 - mae: 13.5149 - val_loss: 94.0919 - val_mse: 94.0919 - val_mae: 8.1235\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 139.5465 - mse: 139.5465 - mae: 8.8743 - val_loss: 135.4052 - val_mse: 135.4052 - val_mae: 10.1189\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 179.7844 - mse: 179.7844 - mae: 9.8955 - val_loss: 31.1392 - val_mse: 31.1392 - val_mae: 4.6100\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 157.1305 - mse: 157.1305 - mae: 9.9561 - val_loss: 172.1063 - val_mse: 172.1063 - val_mae: 11.7180\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 176.8826 - mse: 176.8826 - mae: 10.8761 - val_loss: 56.5298 - val_mse: 56.5298 - val_mae: 6.0933\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 214.5469 - mse: 214.5469 - mae: 11.4805 - val_loss: 246.0139 - val_mse: 246.0139 - val_mae: 14.8747\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 215.8262 - mse: 215.8262 - mae: 11.5870 - val_loss: 43.0465 - val_mse: 43.0465 - val_mae: 5.5325\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 190.4366 - mse: 190.4366 - mae: 11.3816 - val_loss: 110.7074 - val_mse: 110.7074 - val_mae: 9.0712\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 177.1844 - mse: 177.1844 - mae: 10.6120 - val_loss: 38.1896 - val_mse: 38.1896 - val_mae: 5.2079\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 268.7490 - mse: 268.7490 - mae: 14.0118 - val_loss: 52.1307 - val_mse: 52.1307 - val_mae: 5.8360\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 99.0657 - mse: 99.0657 - mae: 7.8578 - val_loss: 26.6140 - val_mse: 26.6140 - val_mae: 4.3495\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 102.1553 - mse: 102.1553 - mae: 7.7301 - val_loss: 200.6372 - val_mse: 200.6372 - val_mae: 13.4065\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 161.9913 - mse: 161.9913 - mae: 10.0201 - val_loss: 139.4558 - val_mse: 139.4558 - val_mae: 11.0188\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 180.6398 - mse: 180.6398 - mae: 10.7070 - val_loss: 144.9002 - val_mse: 144.9002 - val_mae: 10.7081\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 138.5348 - mse: 138.5348 - mae: 9.1695 - val_loss: 24.2745 - val_mse: 24.2745 - val_mae: 4.0431\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 195.0536 - mse: 195.0536 - mae: 11.2543 - val_loss: 106.8969 - val_mse: 106.8969 - val_mae: 8.9229\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 277.4268 - mse: 277.4268 - mae: 13.9401 - val_loss: 85.1231 - val_mse: 85.1231 - val_mae: 8.0148\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 134.7883 - mse: 134.7883 - mae: 9.2638 - val_loss: 52.7077 - val_mse: 52.7077 - val_mae: 6.3047\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 144.0919 - mse: 144.0919 - mae: 9.4681 - val_loss: 27.9634 - val_mse: 27.9634 - val_mae: 4.6136\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 116.4999 - mse: 116.4999 - mae: 8.3570 - val_loss: 164.6072 - val_mse: 164.6072 - val_mae: 11.5042\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 176.0713 - mse: 176.0713 - mae: 10.0293 - val_loss: 34.0318 - val_mse: 34.0318 - val_mae: 5.1204\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 105.6223 - mse: 105.6223 - mae: 7.5613 - val_loss: 43.8720 - val_mse: 43.8720 - val_mae: 5.5629\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 134.0435 - mse: 134.0435 - mae: 9.0128 - val_loss: 26.1028 - val_mse: 26.1028 - val_mae: 4.1766\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 114.3136 - mse: 114.3136 - mae: 8.0283 - val_loss: 36.1610 - val_mse: 36.1610 - val_mae: 5.2620\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 124.3647 - mse: 124.3647 - mae: 8.4322 - val_loss: 118.4208 - val_mse: 118.4208 - val_mae: 9.7139\n",
      "Epoch 60: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.1, model__n_hidden=3, model__n_neurons=5, model__optimizer=nesterov; total time=   3.6s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 24ms/step - loss: 556.7738 - mse: 556.7738 - mae: 21.8859 - val_loss: 547.1132 - val_mse: 547.1132 - val_mae: 21.8018\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 555.1776 - mse: 555.1776 - mae: 21.8490 - val_loss: 545.5892 - val_mse: 545.5892 - val_mae: 21.7669\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 553.6185 - mse: 553.6185 - mae: 21.8127 - val_loss: 544.0677 - val_mse: 544.0677 - val_mae: 21.7319\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 552.0719 - mse: 552.0719 - mae: 21.7773 - val_loss: 542.5569 - val_mse: 542.5569 - val_mae: 21.6971\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 550.5493 - mse: 550.5493 - mae: 21.7418 - val_loss: 541.0536 - val_mse: 541.0536 - val_mae: 21.6624\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 549.0362 - mse: 549.0362 - mae: 21.7068 - val_loss: 539.5551 - val_mse: 539.5551 - val_mae: 21.6278\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 547.5301 - mse: 547.5301 - mae: 21.6722 - val_loss: 538.0489 - val_mse: 538.0489 - val_mae: 21.5930\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 546.0197 - mse: 546.0197 - mae: 21.6367 - val_loss: 536.5563 - val_mse: 536.5563 - val_mae: 21.5584\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 544.5247 - mse: 544.5247 - mae: 21.6026 - val_loss: 535.0727 - val_mse: 535.0727 - val_mae: 21.5239\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 543.0381 - mse: 543.0381 - mae: 21.5683 - val_loss: 533.5870 - val_mse: 533.5870 - val_mae: 21.4894\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 541.5504 - mse: 541.5504 - mae: 21.5337 - val_loss: 532.1079 - val_mse: 532.1079 - val_mae: 21.4550\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 540.0707 - mse: 540.0707 - mae: 21.4993 - val_loss: 530.6448 - val_mse: 530.6448 - val_mae: 21.4208\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 538.6043 - mse: 538.6043 - mae: 21.4652 - val_loss: 529.1763 - val_mse: 529.1763 - val_mae: 21.3865\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 537.1339 - mse: 537.1339 - mae: 21.4310 - val_loss: 527.7113 - val_mse: 527.7113 - val_mae: 21.3522\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 535.6669 - mse: 535.6669 - mae: 21.3963 - val_loss: 526.2553 - val_mse: 526.2553 - val_mae: 21.3181\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 534.2079 - mse: 534.2079 - mae: 21.3623 - val_loss: 524.7907 - val_mse: 524.7907 - val_mae: 21.2838\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 532.7424 - mse: 532.7424 - mae: 21.3276 - val_loss: 523.3437 - val_mse: 523.3437 - val_mae: 21.2497\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 531.2932 - mse: 531.2932 - mae: 21.2938 - val_loss: 521.9011 - val_mse: 521.9011 - val_mae: 21.2158\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 529.8489 - mse: 529.8489 - mae: 21.2602 - val_loss: 520.4680 - val_mse: 520.4680 - val_mae: 21.1820\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 528.4131 - mse: 528.4131 - mae: 21.2264 - val_loss: 519.0307 - val_mse: 519.0307 - val_mae: 21.1480\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 526.9745 - mse: 526.9745 - mae: 21.1923 - val_loss: 517.6050 - val_mse: 517.6050 - val_mae: 21.1143\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 525.5460 - mse: 525.5460 - mae: 21.1589 - val_loss: 516.1786 - val_mse: 516.1786 - val_mae: 21.0805\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 524.1176 - mse: 524.1176 - mae: 21.1254 - val_loss: 514.7526 - val_mse: 514.7526 - val_mae: 21.0466\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 522.6886 - mse: 522.6886 - mae: 21.0912 - val_loss: 513.3239 - val_mse: 513.3239 - val_mae: 21.0126\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 521.2582 - mse: 521.2582 - mae: 21.0571 - val_loss: 511.9037 - val_mse: 511.9037 - val_mae: 20.9788\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 519.8358 - mse: 519.8358 - mae: 21.0235 - val_loss: 510.4904 - val_mse: 510.4904 - val_mae: 20.9451\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 518.4208 - mse: 518.4208 - mae: 20.9896 - val_loss: 509.0854 - val_mse: 509.0854 - val_mae: 20.9115\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 517.0136 - mse: 517.0136 - mae: 20.9560 - val_loss: 507.6770 - val_mse: 507.6770 - val_mae: 20.8778\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 515.6036 - mse: 515.6036 - mae: 20.9225 - val_loss: 506.2825 - val_mse: 506.2825 - val_mae: 20.8444\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 514.2066 - mse: 514.2066 - mae: 20.8889 - val_loss: 504.8852 - val_mse: 504.8852 - val_mae: 20.8109\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 512.8076 - mse: 512.8076 - mae: 20.8552 - val_loss: 503.4936 - val_mse: 503.4936 - val_mae: 20.7774\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 511.4133 - mse: 511.4133 - mae: 20.8223 - val_loss: 502.1039 - val_mse: 502.1039 - val_mae: 20.7439\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 510.0229 - mse: 510.0229 - mae: 20.7884 - val_loss: 500.7342 - val_mse: 500.7342 - val_mae: 20.7109\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 508.6507 - mse: 508.6507 - mae: 20.7554 - val_loss: 499.3609 - val_mse: 499.3609 - val_mae: 20.6777\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 507.2744 - mse: 507.2744 - mae: 20.7226 - val_loss: 497.9861 - val_mse: 497.9861 - val_mae: 20.6444\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 505.8993 - mse: 505.8993 - mae: 20.6891 - val_loss: 496.6346 - val_mse: 496.6346 - val_mae: 20.6117\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 504.5462 - mse: 504.5462 - mae: 20.6557 - val_loss: 495.2841 - val_mse: 495.2841 - val_mae: 20.5789\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 503.1936 - mse: 503.1936 - mae: 20.6233 - val_loss: 493.9379 - val_mse: 493.9379 - val_mae: 20.5462\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 501.8446 - mse: 501.8446 - mae: 20.5907 - val_loss: 492.5869 - val_mse: 492.5869 - val_mae: 20.5133\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 500.4908 - mse: 500.4908 - mae: 20.5581 - val_loss: 491.2400 - val_mse: 491.2400 - val_mae: 20.4804\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 499.1425 - mse: 499.1425 - mae: 20.5252 - val_loss: 489.9014 - val_mse: 489.9014 - val_mae: 20.4477\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 497.8001 - mse: 497.8001 - mae: 20.4929 - val_loss: 488.5418 - val_mse: 488.5418 - val_mae: 20.4144\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 496.4397 - mse: 496.4397 - mae: 20.4595 - val_loss: 487.2072 - val_mse: 487.2072 - val_mae: 20.3817\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 495.1035 - mse: 495.1035 - mae: 20.4266 - val_loss: 485.8813 - val_mse: 485.8813 - val_mae: 20.3492\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 493.7742 - mse: 493.7742 - mae: 20.3942 - val_loss: 484.5473 - val_mse: 484.5473 - val_mae: 20.3164\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 492.4391 - mse: 492.4391 - mae: 20.3613 - val_loss: 483.2294 - val_mse: 483.2294 - val_mae: 20.2839\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 491.1196 - mse: 491.1196 - mae: 20.3284 - val_loss: 481.9195 - val_mse: 481.9195 - val_mae: 20.2516\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 489.8072 - mse: 489.8072 - mae: 20.2967 - val_loss: 480.6085 - val_mse: 480.6085 - val_mae: 20.2192\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 488.4939 - mse: 488.4939 - mae: 20.2641 - val_loss: 479.2985 - val_mse: 479.2985 - val_mae: 20.1868\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 487.1821 - mse: 487.1821 - mae: 20.2316 - val_loss: 477.9947 - val_mse: 477.9947 - val_mae: 20.1545\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 485.8765 - mse: 485.8765 - mae: 20.1992 - val_loss: 476.6992 - val_mse: 476.6992 - val_mae: 20.1223\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 484.5791 - mse: 484.5791 - mae: 20.1672 - val_loss: 475.4023 - val_mse: 475.4023 - val_mae: 20.0900\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 483.2790 - mse: 483.2790 - mae: 20.1353 - val_loss: 474.1040 - val_mse: 474.1040 - val_mae: 20.0577\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 481.9788 - mse: 481.9788 - mae: 20.1027 - val_loss: 472.8110 - val_mse: 472.8110 - val_mae: 20.0254\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 480.6843 - mse: 480.6843 - mae: 20.0707 - val_loss: 471.5280 - val_mse: 471.5280 - val_mae: 19.9934\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 479.3997 - mse: 479.3997 - mae: 20.0386 - val_loss: 470.2522 - val_mse: 470.2522 - val_mae: 19.9614\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 478.1208 - mse: 478.1208 - mae: 20.0068 - val_loss: 468.9705 - val_mse: 468.9705 - val_mae: 19.9293\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 476.8371 - mse: 476.8371 - mae: 19.9745 - val_loss: 467.6946 - val_mse: 467.6946 - val_mae: 19.8973\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 475.5591 - mse: 475.5591 - mae: 19.9429 - val_loss: 466.4165 - val_mse: 466.4165 - val_mae: 19.8651\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 474.2802 - mse: 474.2802 - mae: 19.9104 - val_loss: 465.1596 - val_mse: 465.1596 - val_mae: 19.8335\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 473.0198 - mse: 473.0198 - mae: 19.8789 - val_loss: 463.8897 - val_mse: 463.8897 - val_mae: 19.8014\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 471.7485 - mse: 471.7485 - mae: 19.8465 - val_loss: 462.6340 - val_mse: 462.6340 - val_mae: 19.7697\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 470.4913 - mse: 470.4913 - mae: 19.8148 - val_loss: 461.3889 - val_mse: 461.3889 - val_mae: 19.7382\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 469.2440 - mse: 469.2440 - mae: 19.7838 - val_loss: 460.1413 - val_mse: 460.1413 - val_mae: 19.7066\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 467.9947 - mse: 467.9947 - mae: 19.7517 - val_loss: 458.9033 - val_mse: 458.9033 - val_mae: 19.6751\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 466.7536 - mse: 466.7536 - mae: 19.7205 - val_loss: 457.6533 - val_mse: 457.6533 - val_mae: 19.6433\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 465.5020 - mse: 465.5020 - mae: 19.6888 - val_loss: 456.4151 - val_mse: 456.4151 - val_mae: 19.6118\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 464.2627 - mse: 464.2627 - mae: 19.6571 - val_loss: 455.1916 - val_mse: 455.1916 - val_mae: 19.5806\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 463.0354 - mse: 463.0354 - mae: 19.6263 - val_loss: 453.9496 - val_mse: 453.9496 - val_mae: 19.5488\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 461.7929 - mse: 461.7929 - mae: 19.5944 - val_loss: 452.7321 - val_mse: 452.7321 - val_mae: 19.5177\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 460.5723 - mse: 460.5723 - mae: 19.5634 - val_loss: 451.5096 - val_mse: 451.5096 - val_mae: 19.4863\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 459.3481 - mse: 459.3481 - mae: 19.5319 - val_loss: 450.2951 - val_mse: 450.2951 - val_mae: 19.4551\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 458.1324 - mse: 458.1324 - mae: 19.5011 - val_loss: 449.0872 - val_mse: 449.0872 - val_mae: 19.4241\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 456.9218 - mse: 456.9218 - mae: 19.4697 - val_loss: 447.8789 - val_mse: 447.8789 - val_mae: 19.3929\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 455.7114 - mse: 455.7114 - mae: 19.4387 - val_loss: 446.6746 - val_mse: 446.6746 - val_mae: 19.3619\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 454.5047 - mse: 454.5047 - mae: 19.4075 - val_loss: 445.4709 - val_mse: 445.4709 - val_mae: 19.3308\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 453.2995 - mse: 453.2995 - mae: 19.3762 - val_loss: 444.2740 - val_mse: 444.2740 - val_mae: 19.2998\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 452.0999 - mse: 452.0999 - mae: 19.3453 - val_loss: 443.0727 - val_mse: 443.0727 - val_mae: 19.2686\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 450.8984 - mse: 450.8984 - mae: 19.3135 - val_loss: 441.8977 - val_mse: 441.8977 - val_mae: 19.2381\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 449.7205 - mse: 449.7205 - mae: 19.2836 - val_loss: 440.7186 - val_mse: 440.7186 - val_mae: 19.2074\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 448.5380 - mse: 448.5380 - mae: 19.2532 - val_loss: 439.5207 - val_mse: 439.5207 - val_mae: 19.1762\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 447.3385 - mse: 447.3385 - mae: 19.2217 - val_loss: 438.3359 - val_mse: 438.3359 - val_mae: 19.1453\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 446.1513 - mse: 446.1513 - mae: 19.1912 - val_loss: 437.1479 - val_mse: 437.1479 - val_mae: 19.1143\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 444.9617 - mse: 444.9617 - mae: 19.1602 - val_loss: 435.9754 - val_mse: 435.9754 - val_mae: 19.0836\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 443.7880 - mse: 443.7880 - mae: 19.1289 - val_loss: 434.8139 - val_mse: 434.8139 - val_mae: 19.0531\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 442.6243 - mse: 442.6243 - mae: 19.0986 - val_loss: 433.6535 - val_mse: 433.6535 - val_mae: 19.0226\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 441.4621 - mse: 441.4621 - mae: 19.0683 - val_loss: 432.4998 - val_mse: 432.4998 - val_mae: 18.9923\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 440.3057 - mse: 440.3057 - mae: 19.0379 - val_loss: 431.3459 - val_mse: 431.3459 - val_mae: 18.9619\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 439.1499 - mse: 439.1499 - mae: 19.0077 - val_loss: 430.1960 - val_mse: 430.1960 - val_mae: 18.9315\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 437.9988 - mse: 437.9988 - mae: 18.9774 - val_loss: 429.0527 - val_mse: 429.0527 - val_mae: 18.9013\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 436.8520 - mse: 436.8520 - mae: 18.9474 - val_loss: 427.9013 - val_mse: 427.9013 - val_mae: 18.8708\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 435.6979 - mse: 435.6979 - mae: 18.9170 - val_loss: 426.7440 - val_mse: 426.7440 - val_mae: 18.8402\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 434.5396 - mse: 434.5396 - mae: 18.8859 - val_loss: 425.6055 - val_mse: 425.6055 - val_mae: 18.8099\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 433.4001 - mse: 433.4001 - mae: 18.8554 - val_loss: 424.4792 - val_mse: 424.4792 - val_mae: 18.7800\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 432.2706 - mse: 432.2706 - mae: 18.8258 - val_loss: 423.3412 - val_mse: 423.3412 - val_mae: 18.7496\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 431.1319 - mse: 431.1319 - mae: 18.7952 - val_loss: 422.2243 - val_mse: 422.2243 - val_mae: 18.7198\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 430.0118 - mse: 430.0118 - mae: 18.7657 - val_loss: 421.0991 - val_mse: 421.0991 - val_mae: 18.6897\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 428.8848 - mse: 428.8848 - mae: 18.7353 - val_loss: 419.9771 - val_mse: 419.9771 - val_mae: 18.6597\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 427.7609 - mse: 427.7609 - mae: 18.7054 - val_loss: 418.8586 - val_mse: 418.8586 - val_mae: 18.6297\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 426.6411 - mse: 426.6411 - mae: 18.6751 - val_loss: 417.7521 - val_mse: 417.7521 - val_mae: 18.6000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.1, model__n_hidden=3, model__n_neurons=5, model__optimizer=nesterov; total time=   5.8s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 24ms/step - loss: 2109.4700 - mse: 2109.4700 - mae: 41.9512 - val_loss: 862.8511 - val_mse: 862.8511 - val_mae: 26.2851\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 722.4705 - mse: 722.4705 - mae: 23.6341 - val_loss: 481.4942 - val_mse: 481.4942 - val_mae: 18.9026\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 407.7728 - mse: 407.7728 - mae: 16.5168 - val_loss: 285.7981 - val_mse: 285.7981 - val_mae: 13.7511\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 215.8273 - mse: 215.8273 - mae: 11.4634 - val_loss: 68.9367 - val_mse: 68.9367 - val_mae: 7.0380\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 119.8378 - mse: 119.8378 - mae: 7.9681 - val_loss: 35.9208 - val_mse: 35.9208 - val_mae: 5.0574\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 105.1365 - mse: 105.1365 - mae: 7.1170 - val_loss: 29.5971 - val_mse: 29.5971 - val_mae: 4.3123\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 88.2224 - mse: 88.2224 - mae: 6.9471 - val_loss: 28.2031 - val_mse: 28.2031 - val_mae: 4.1027\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 80.6967 - mse: 80.6967 - mae: 6.8729 - val_loss: 25.1279 - val_mse: 25.1279 - val_mae: 4.0176\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 74.7638 - mse: 74.7638 - mae: 6.3495 - val_loss: 20.9776 - val_mse: 20.9776 - val_mae: 3.8397\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 71.4458 - mse: 71.4458 - mae: 5.9879 - val_loss: 22.0802 - val_mse: 22.0802 - val_mae: 3.9924\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 70.1233 - mse: 70.1233 - mae: 5.8882 - val_loss: 22.8546 - val_mse: 22.8546 - val_mae: 4.1006\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 68.9513 - mse: 68.9513 - mae: 5.8692 - val_loss: 22.2672 - val_mse: 22.2672 - val_mae: 4.0777\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 68.0607 - mse: 68.0607 - mae: 5.7900 - val_loss: 21.1791 - val_mse: 21.1791 - val_mae: 3.9722\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 68.0996 - mse: 68.0996 - mae: 5.9793 - val_loss: 25.3180 - val_mse: 25.3180 - val_mae: 4.3874\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 66.6899 - mse: 66.6899 - mae: 5.8234 - val_loss: 20.8926 - val_mse: 20.8926 - val_mae: 3.9425\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 66.3976 - mse: 66.3976 - mae: 5.6613 - val_loss: 21.2916 - val_mse: 21.2916 - val_mae: 3.9805\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 65.6524 - mse: 65.6524 - mae: 5.6670 - val_loss: 21.8152 - val_mse: 21.8152 - val_mae: 4.0486\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 65.1123 - mse: 65.1123 - mae: 5.6537 - val_loss: 23.1069 - val_mse: 23.1069 - val_mae: 4.1754\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 65.4917 - mse: 65.4917 - mae: 5.5926 - val_loss: 21.6620 - val_mse: 21.6620 - val_mae: 4.0111\n",
      "Epoch 19: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.1, model__n_hidden=3, model__n_neurons=125, model__optimizer=adam; total time=   1.7s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 24ms/step - loss: 515.3350 - mse: 515.3350 - mae: 18.8624 - val_loss: 274.1861 - val_mse: 274.1861 - val_mae: 13.8303\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 264.7181 - mse: 264.7181 - mae: 12.6379 - val_loss: 115.9291 - val_mse: 115.9291 - val_mae: 8.3813\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 140.1913 - mse: 140.1913 - mae: 8.7388 - val_loss: 37.7795 - val_mse: 37.7795 - val_mae: 5.1388\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 110.2468 - mse: 110.2468 - mae: 7.3776 - val_loss: 34.4730 - val_mse: 34.4730 - val_mae: 4.9390\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 94.3986 - mse: 94.3986 - mae: 7.9046 - val_loss: 33.5668 - val_mse: 33.5668 - val_mae: 5.0324\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 85.5360 - mse: 85.5360 - mae: 7.2135 - val_loss: 20.2310 - val_mse: 20.2310 - val_mae: 3.8059\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 81.8433 - mse: 81.8433 - mae: 6.3770 - val_loss: 18.6417 - val_mse: 18.6417 - val_mae: 3.6489\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 76.0551 - mse: 76.0551 - mae: 6.3699 - val_loss: 21.7029 - val_mse: 21.7029 - val_mae: 4.1054\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 75.1676 - mse: 75.1676 - mae: 6.3650 - val_loss: 17.2282 - val_mse: 17.2282 - val_mae: 3.5316\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 73.4195 - mse: 73.4195 - mae: 6.1708 - val_loss: 18.2363 - val_mse: 18.2363 - val_mae: 3.6543\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 71.9608 - mse: 71.9608 - mae: 6.0392 - val_loss: 18.1478 - val_mse: 18.1478 - val_mae: 3.6443\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 71.2817 - mse: 71.2817 - mae: 5.9286 - val_loss: 17.5708 - val_mse: 17.5708 - val_mae: 3.5707\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 70.0520 - mse: 70.0520 - mae: 6.0637 - val_loss: 18.5836 - val_mse: 18.5836 - val_mae: 3.7000\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 69.4756 - mse: 69.4756 - mae: 6.1220 - val_loss: 15.4907 - val_mse: 15.4907 - val_mae: 3.2230\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 69.6686 - mse: 69.6686 - mae: 5.6254 - val_loss: 18.7243 - val_mse: 18.7243 - val_mae: 3.7505\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 68.9987 - mse: 68.9987 - mae: 6.3038 - val_loss: 15.7858 - val_mse: 15.7858 - val_mae: 3.3057\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 68.6091 - mse: 68.6091 - mae: 5.5239 - val_loss: 16.2247 - val_mse: 16.2247 - val_mae: 3.4237\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 66.7946 - mse: 66.7946 - mae: 6.0450 - val_loss: 17.4719 - val_mse: 17.4719 - val_mae: 3.5956\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 68.4815 - mse: 68.4815 - mae: 5.6018 - val_loss: 16.4504 - val_mse: 16.4504 - val_mae: 3.3995\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 66.1715 - mse: 66.1715 - mae: 5.9868 - val_loss: 16.8117 - val_mse: 16.8117 - val_mae: 3.4886\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 65.9693 - mse: 65.9693 - mae: 6.0056 - val_loss: 13.2329 - val_mse: 13.2329 - val_mae: 2.8865\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 65.8114 - mse: 65.8114 - mae: 5.3463 - val_loss: 16.1908 - val_mse: 16.1908 - val_mae: 3.4511\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 64.2640 - mse: 64.2640 - mae: 5.8455 - val_loss: 16.2827 - val_mse: 16.2827 - val_mae: 3.4194\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 64.5530 - mse: 64.5530 - mae: 5.7486 - val_loss: 14.2168 - val_mse: 14.2168 - val_mae: 3.0374\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 64.4189 - mse: 64.4189 - mae: 5.6663 - val_loss: 13.6301 - val_mse: 13.6301 - val_mae: 2.9515\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 63.4512 - mse: 63.4512 - mae: 5.4366 - val_loss: 17.2472 - val_mse: 17.2472 - val_mae: 3.6011\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 63.7110 - mse: 63.7110 - mae: 5.8595 - val_loss: 13.7654 - val_mse: 13.7654 - val_mae: 3.0804\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 63.9585 - mse: 63.9585 - mae: 5.3587 - val_loss: 17.1808 - val_mse: 17.1808 - val_mae: 3.5049\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 62.4431 - mse: 62.4431 - mae: 5.7322 - val_loss: 14.1709 - val_mse: 14.1709 - val_mae: 3.0578\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 62.3466 - mse: 62.3466 - mae: 5.4150 - val_loss: 17.0926 - val_mse: 17.0926 - val_mae: 3.5625\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 62.4684 - mse: 62.4684 - mae: 5.7647 - val_loss: 13.1094 - val_mse: 13.1094 - val_mae: 2.9099\n",
      "Epoch 31: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.1, model__n_hidden=3, model__n_neurons=125, model__optimizer=adam; total time=   2.4s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 24ms/step - loss: 2038.5089 - mse: 2038.5089 - mae: 41.5713 - val_loss: 846.4756 - val_mse: 846.4756 - val_mae: 25.9585\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 348.8502 - mse: 348.8502 - mae: 15.5173 - val_loss: 99.2779 - val_mse: 99.2779 - val_mae: 7.5369\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 116.8186 - mse: 116.8186 - mae: 8.0150 - val_loss: 164.9938 - val_mse: 164.9938 - val_mae: 10.5627\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 174.5079 - mse: 174.5079 - mae: 10.5152 - val_loss: 138.2306 - val_mse: 138.2306 - val_mae: 9.4998\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 109.3634 - mse: 109.3634 - mae: 7.5464 - val_loss: 78.0693 - val_mse: 78.0693 - val_mae: 6.2954\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 75.2881 - mse: 75.2881 - mae: 6.2849 - val_loss: 82.1216 - val_mse: 82.1216 - val_mae: 6.9656\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 76.2295 - mse: 76.2295 - mae: 6.6980 - val_loss: 73.7353 - val_mse: 73.7353 - val_mae: 6.5687\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 67.5749 - mse: 67.5749 - mae: 6.1377 - val_loss: 63.7976 - val_mse: 63.7976 - val_mae: 6.0365\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 64.5810 - mse: 64.5810 - mae: 5.6663 - val_loss: 60.4456 - val_mse: 60.4456 - val_mae: 5.7977\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 63.4947 - mse: 63.4947 - mae: 5.4536 - val_loss: 57.1810 - val_mse: 57.1810 - val_mae: 5.7378\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 60.9336 - mse: 60.9336 - mae: 5.4728 - val_loss: 54.6624 - val_mse: 54.6624 - val_mae: 5.7447\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 60.0386 - mse: 60.0386 - mae: 5.5955 - val_loss: 52.8459 - val_mse: 52.8459 - val_mae: 5.7311\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 59.3609 - mse: 59.3609 - mae: 5.5583 - val_loss: 50.8959 - val_mse: 50.8959 - val_mae: 5.6593\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 58.5941 - mse: 58.5941 - mae: 5.4454 - val_loss: 49.6654 - val_mse: 49.6654 - val_mae: 5.6245\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 57.8525 - mse: 57.8525 - mae: 5.4583 - val_loss: 48.6311 - val_mse: 48.6311 - val_mae: 5.6039\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 57.2859 - mse: 57.2859 - mae: 5.5064 - val_loss: 47.4891 - val_mse: 47.4891 - val_mae: 5.5318\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 56.8549 - mse: 56.8549 - mae: 5.4752 - val_loss: 46.7108 - val_mse: 46.7108 - val_mae: 5.4818\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 55.8431 - mse: 55.8431 - mae: 5.3348 - val_loss: 45.9754 - val_mse: 45.9754 - val_mae: 5.4346\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 56.0711 - mse: 56.0711 - mae: 5.2072 - val_loss: 45.0187 - val_mse: 45.0187 - val_mae: 5.3890\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 55.2555 - mse: 55.2555 - mae: 5.3091 - val_loss: 44.2382 - val_mse: 44.2382 - val_mae: 5.3279\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 54.5007 - mse: 54.5007 - mae: 5.2805 - val_loss: 43.2993 - val_mse: 43.2993 - val_mae: 5.2688\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 54.2903 - mse: 54.2903 - mae: 5.1675 - val_loss: 42.6409 - val_mse: 42.6409 - val_mae: 5.2512\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 53.8517 - mse: 53.8517 - mae: 5.0977 - val_loss: 41.7364 - val_mse: 41.7364 - val_mae: 5.2150\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 53.0280 - mse: 53.0280 - mae: 5.2524 - val_loss: 41.5992 - val_mse: 41.5992 - val_mae: 5.1567\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 52.5935 - mse: 52.5935 - mae: 5.2323 - val_loss: 40.9863 - val_mse: 40.9863 - val_mae: 5.0940\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 52.3226 - mse: 52.3226 - mae: 5.0455 - val_loss: 40.3480 - val_mse: 40.3480 - val_mae: 5.0576\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 51.8516 - mse: 51.8516 - mae: 5.1099 - val_loss: 40.3004 - val_mse: 40.3004 - val_mae: 5.0350\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 51.0476 - mse: 51.0476 - mae: 5.0920 - val_loss: 39.5219 - val_mse: 39.5219 - val_mae: 4.9576\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 50.7042 - mse: 50.7042 - mae: 4.9123 - val_loss: 38.8899 - val_mse: 38.8899 - val_mae: 4.9171\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 50.3086 - mse: 50.3086 - mae: 4.9856 - val_loss: 38.4128 - val_mse: 38.4128 - val_mae: 4.8973\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 50.0250 - mse: 50.0250 - mae: 5.0017 - val_loss: 38.3245 - val_mse: 38.3245 - val_mae: 4.8061\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 49.8711 - mse: 49.8711 - mae: 4.8063 - val_loss: 37.8579 - val_mse: 37.8579 - val_mae: 4.7629\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 49.3815 - mse: 49.3815 - mae: 4.9162 - val_loss: 37.8113 - val_mse: 37.8113 - val_mae: 4.7926\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 48.9200 - mse: 48.9200 - mae: 4.9902 - val_loss: 36.9644 - val_mse: 36.9644 - val_mae: 4.7357\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 48.9308 - mse: 48.9308 - mae: 4.7359 - val_loss: 36.5697 - val_mse: 36.5697 - val_mae: 4.7045\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 48.4560 - mse: 48.4560 - mae: 4.8974 - val_loss: 36.5023 - val_mse: 36.5023 - val_mae: 4.6692\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 48.2083 - mse: 48.2083 - mae: 4.9458 - val_loss: 36.1820 - val_mse: 36.1820 - val_mae: 4.6430\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 48.0671 - mse: 48.0671 - mae: 4.6824 - val_loss: 36.1524 - val_mse: 36.1524 - val_mae: 4.6231\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 48.3339 - mse: 48.3339 - mae: 4.7377 - val_loss: 35.8810 - val_mse: 35.8810 - val_mae: 4.5978\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 47.4955 - mse: 47.4955 - mae: 4.8234 - val_loss: 35.2865 - val_mse: 35.2865 - val_mae: 4.6102\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 47.6290 - mse: 47.6290 - mae: 4.7097 - val_loss: 34.9788 - val_mse: 34.9788 - val_mae: 4.5663\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 47.2364 - mse: 47.2364 - mae: 4.7161 - val_loss: 34.9073 - val_mse: 34.9073 - val_mae: 4.5186\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 47.1912 - mse: 47.1912 - mae: 4.8871 - val_loss: 35.9729 - val_mse: 35.9729 - val_mae: 4.5416\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 46.8277 - mse: 46.8277 - mae: 4.7926 - val_loss: 34.7141 - val_mse: 34.7141 - val_mae: 4.5261\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 46.7748 - mse: 46.7748 - mae: 4.5731 - val_loss: 34.5488 - val_mse: 34.5488 - val_mae: 4.4998\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 46.1682 - mse: 46.1682 - mae: 4.7366 - val_loss: 34.5793 - val_mse: 34.5793 - val_mae: 4.4801\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 46.7290 - mse: 46.7290 - mae: 4.9049 - val_loss: 34.5622 - val_mse: 34.5622 - val_mae: 4.4238\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 46.7441 - mse: 46.7441 - mae: 4.5490 - val_loss: 34.0793 - val_mse: 34.0793 - val_mae: 4.4940\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 47.0418 - mse: 47.0418 - mae: 4.7378 - val_loss: 34.0216 - val_mse: 34.0216 - val_mae: 4.4253\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 45.5612 - mse: 45.5612 - mae: 4.6322 - val_loss: 33.7807 - val_mse: 33.7807 - val_mae: 4.4304\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 45.7921 - mse: 45.7921 - mae: 4.6350 - val_loss: 33.9120 - val_mse: 33.9120 - val_mae: 4.3757\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 45.3299 - mse: 45.3299 - mae: 4.5882 - val_loss: 33.3958 - val_mse: 33.3958 - val_mae: 4.3884\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 45.0038 - mse: 45.0038 - mae: 4.5355 - val_loss: 33.3483 - val_mse: 33.3483 - val_mae: 4.3798\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 45.2732 - mse: 45.2732 - mae: 4.7756 - val_loss: 33.2114 - val_mse: 33.2114 - val_mae: 4.3762\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 45.2071 - mse: 45.2071 - mae: 4.5813 - val_loss: 32.8966 - val_mse: 32.8966 - val_mae: 4.3929\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 44.8002 - mse: 44.8002 - mae: 4.6614 - val_loss: 33.3899 - val_mse: 33.3899 - val_mae: 4.3745\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 44.5903 - mse: 44.5903 - mae: 4.5985 - val_loss: 32.6760 - val_mse: 32.6760 - val_mae: 4.3595\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 44.4116 - mse: 44.4116 - mae: 4.5187 - val_loss: 32.4594 - val_mse: 32.4594 - val_mae: 4.3281\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 44.1983 - mse: 44.1983 - mae: 4.5104 - val_loss: 32.6489 - val_mse: 32.6489 - val_mae: 4.3170\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 44.1707 - mse: 44.1707 - mae: 4.7049 - val_loss: 32.3973 - val_mse: 32.3973 - val_mae: 4.3496\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 44.0660 - mse: 44.0660 - mae: 4.4857 - val_loss: 32.3315 - val_mse: 32.3315 - val_mae: 4.3685\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 44.5536 - mse: 44.5536 - mae: 4.6070 - val_loss: 32.5432 - val_mse: 32.5432 - val_mae: 4.3143\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 43.4101 - mse: 43.4101 - mae: 4.5367 - val_loss: 31.8685 - val_mse: 31.8685 - val_mae: 4.2941\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 43.6876 - mse: 43.6876 - mae: 4.4048 - val_loss: 31.6094 - val_mse: 31.6094 - val_mae: 4.2843\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 43.8633 - mse: 43.8633 - mae: 4.6225 - val_loss: 31.7091 - val_mse: 31.7091 - val_mae: 4.3338\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 43.3448 - mse: 43.3448 - mae: 4.4973 - val_loss: 31.5572 - val_mse: 31.5572 - val_mae: 4.3120\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 43.1541 - mse: 43.1541 - mae: 4.4477 - val_loss: 31.8385 - val_mse: 31.8385 - val_mae: 4.2310\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 43.2409 - mse: 43.2409 - mae: 4.6704 - val_loss: 31.2701 - val_mse: 31.2701 - val_mae: 4.2113\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 44.6590 - mse: 44.6590 - mae: 4.4114 - val_loss: 30.9009 - val_mse: 30.9009 - val_mae: 4.2378\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 42.8593 - mse: 42.8593 - mae: 4.6309 - val_loss: 31.1150 - val_mse: 31.1150 - val_mae: 4.2886\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 42.7494 - mse: 42.7494 - mae: 4.4079 - val_loss: 30.7619 - val_mse: 30.7619 - val_mae: 4.2703\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 42.5214 - mse: 42.5214 - mae: 4.4906 - val_loss: 31.3366 - val_mse: 31.3366 - val_mae: 4.2038\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 43.1537 - mse: 43.1537 - mae: 4.4511 - val_loss: 30.3827 - val_mse: 30.3827 - val_mae: 4.2355\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 41.8724 - mse: 41.8724 - mae: 4.4272 - val_loss: 30.4978 - val_mse: 30.4978 - val_mae: 4.2421\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 41.7580 - mse: 41.7580 - mae: 4.4983 - val_loss: 30.1677 - val_mse: 30.1677 - val_mae: 4.1259\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 42.1158 - mse: 42.1158 - mae: 4.3360 - val_loss: 30.0425 - val_mse: 30.0425 - val_mae: 4.1332\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 41.9372 - mse: 41.9372 - mae: 4.5228 - val_loss: 30.4599 - val_mse: 30.4599 - val_mae: 4.2302\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 41.5629 - mse: 41.5629 - mae: 4.4285 - val_loss: 29.7257 - val_mse: 29.7257 - val_mae: 4.2245\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 42.3272 - mse: 42.3272 - mae: 4.5523 - val_loss: 29.4217 - val_mse: 29.4217 - val_mae: 4.1200\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 41.2735 - mse: 41.2735 - mae: 4.2629 - val_loss: 29.3772 - val_mse: 29.3772 - val_mae: 4.1848\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 41.2239 - mse: 41.2239 - mae: 4.3907 - val_loss: 29.0210 - val_mse: 29.0210 - val_mae: 4.1517\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 41.0387 - mse: 41.0387 - mae: 4.4828 - val_loss: 28.8475 - val_mse: 28.8475 - val_mae: 4.1428\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40.8109 - mse: 40.8109 - mae: 4.2077 - val_loss: 28.6752 - val_mse: 28.6752 - val_mae: 4.1693\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40.3017 - mse: 40.3017 - mae: 4.4159 - val_loss: 29.0112 - val_mse: 29.0112 - val_mae: 4.1352\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40.2782 - mse: 40.2782 - mae: 4.4553 - val_loss: 28.6454 - val_mse: 28.6454 - val_mae: 4.0988\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 41.2915 - mse: 41.2915 - mae: 4.1947 - val_loss: 28.3604 - val_mse: 28.3604 - val_mae: 4.0813\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 40.0618 - mse: 40.0618 - mae: 4.3405 - val_loss: 28.1314 - val_mse: 28.1314 - val_mae: 4.1187\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 39.6750 - mse: 39.6750 - mae: 4.2905 - val_loss: 27.9339 - val_mse: 27.9339 - val_mae: 4.0743\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 39.5917 - mse: 39.5917 - mae: 4.1772 - val_loss: 27.8845 - val_mse: 27.8845 - val_mae: 4.0659\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 39.5140 - mse: 39.5140 - mae: 4.2985 - val_loss: 27.6422 - val_mse: 27.6422 - val_mae: 4.0906\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 39.2957 - mse: 39.2957 - mae: 4.2659 - val_loss: 27.6166 - val_mse: 27.6166 - val_mae: 4.0295\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 39.3279 - mse: 39.3279 - mae: 4.3142 - val_loss: 27.2478 - val_mse: 27.2478 - val_mae: 4.0147\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 39.2430 - mse: 39.2430 - mae: 4.3473 - val_loss: 27.2216 - val_mse: 27.2216 - val_mae: 4.0363\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 38.6811 - mse: 38.6811 - mae: 4.2292 - val_loss: 27.1847 - val_mse: 27.1847 - val_mae: 4.0446\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 39.3509 - mse: 39.3509 - mae: 4.2245 - val_loss: 26.9485 - val_mse: 26.9485 - val_mae: 4.0128\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 39.0220 - mse: 39.0220 - mae: 4.2639 - val_loss: 26.7453 - val_mse: 26.7453 - val_mae: 4.0544\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 38.7560 - mse: 38.7560 - mae: 4.2339 - val_loss: 26.7086 - val_mse: 26.7086 - val_mae: 3.9893\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 38.5425 - mse: 38.5425 - mae: 4.1138 - val_loss: 26.3307 - val_mse: 26.3307 - val_mae: 4.0003\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 38.1997 - mse: 38.1997 - mae: 4.2090 - val_loss: 26.1968 - val_mse: 26.1968 - val_mae: 4.0066\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 38.3440 - mse: 38.3440 - mae: 4.2333 - val_loss: 26.1128 - val_mse: 26.1128 - val_mae: 3.9528\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.1, model__n_hidden=3, model__n_neurons=125, model__optimizer=adam; total time=   6.8s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 191115.9062 - mse: 191115.9062 - mae: 235.0137 - val_loss: 450.0601 - val_mse: 450.0601 - val_mae: 20.2965\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 599.4962 - mse: 599.4962 - mae: 22.7205 - val_loss: 449.9128 - val_mse: 449.9128 - val_mae: 20.2929\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 599.3315 - mse: 599.3315 - mae: 22.7169 - val_loss: 449.7663 - val_mse: 449.7663 - val_mae: 20.2892\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 599.1673 - mse: 599.1673 - mae: 22.7133 - val_loss: 449.6175 - val_mse: 449.6175 - val_mae: 20.2856\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 599.0009 - mse: 599.0009 - mae: 22.7096 - val_loss: 449.4703 - val_mse: 449.4703 - val_mae: 20.2819\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 598.8362 - mse: 598.8362 - mae: 22.7059 - val_loss: 449.3239 - val_mse: 449.3239 - val_mae: 20.2783\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 598.6722 - mse: 598.6722 - mae: 22.7024 - val_loss: 449.1772 - val_mse: 449.1772 - val_mae: 20.2747\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 598.5079 - mse: 598.5079 - mae: 22.6988 - val_loss: 449.0296 - val_mse: 449.0296 - val_mae: 20.2711\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 598.3427 - mse: 598.3427 - mae: 22.6951 - val_loss: 448.8813 - val_mse: 448.8813 - val_mae: 20.2674\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 598.1767 - mse: 598.1767 - mae: 22.6914 - val_loss: 448.7348 - val_mse: 448.7348 - val_mae: 20.2638\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 598.0126 - mse: 598.0126 - mae: 22.6879 - val_loss: 448.5873 - val_mse: 448.5873 - val_mae: 20.2602\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 597.8476 - mse: 597.8476 - mae: 22.6842 - val_loss: 448.4403 - val_mse: 448.4403 - val_mae: 20.2565\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 597.6829 - mse: 597.6829 - mae: 22.6806 - val_loss: 448.2923 - val_mse: 448.2923 - val_mae: 20.2529\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 597.5174 - mse: 597.5174 - mae: 22.6769 - val_loss: 448.1455 - val_mse: 448.1455 - val_mae: 20.2493\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 597.3530 - mse: 597.3530 - mae: 22.6733 - val_loss: 447.9986 - val_mse: 447.9986 - val_mae: 20.2456\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 597.1885 - mse: 597.1885 - mae: 22.6696 - val_loss: 447.8529 - val_mse: 447.8529 - val_mae: 20.2420\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 597.0253 - mse: 597.0253 - mae: 22.6661 - val_loss: 447.7060 - val_mse: 447.7060 - val_mae: 20.2384\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 596.8608 - mse: 596.8608 - mae: 22.6625 - val_loss: 447.5591 - val_mse: 447.5591 - val_mae: 20.2348\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 596.6964 - mse: 596.6964 - mae: 22.6589 - val_loss: 447.4121 - val_mse: 447.4121 - val_mae: 20.2311\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 596.5317 - mse: 596.5317 - mae: 22.6552 - val_loss: 447.2649 - val_mse: 447.2649 - val_mae: 20.2275\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 596.3668 - mse: 596.3668 - mae: 22.6515 - val_loss: 447.1176 - val_mse: 447.1176 - val_mae: 20.2239\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 596.2021 - mse: 596.2021 - mae: 22.6479 - val_loss: 446.9718 - val_mse: 446.9718 - val_mae: 20.2203\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 596.0387 - mse: 596.0387 - mae: 22.6443 - val_loss: 446.8249 - val_mse: 446.8249 - val_mae: 20.2166\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 595.8741 - mse: 595.8741 - mae: 22.6407 - val_loss: 446.6779 - val_mse: 446.6779 - val_mae: 20.2130\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 595.7095 - mse: 595.7095 - mae: 22.6370 - val_loss: 446.5310 - val_mse: 446.5310 - val_mae: 20.2094\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 595.5450 - mse: 595.5450 - mae: 22.6334 - val_loss: 446.3841 - val_mse: 446.3841 - val_mae: 20.2057\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 595.3806 - mse: 595.3806 - mae: 22.6298 - val_loss: 446.2377 - val_mse: 446.2377 - val_mae: 20.2021\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 595.2166 - mse: 595.2166 - mae: 22.6261 - val_loss: 446.0914 - val_mse: 446.0914 - val_mae: 20.1985\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 595.0527 - mse: 595.0527 - mae: 22.6225 - val_loss: 445.9444 - val_mse: 445.9444 - val_mae: 20.1948\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 594.8882 - mse: 594.8882 - mae: 22.6189 - val_loss: 445.7989 - val_mse: 445.7989 - val_mae: 20.1912\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 594.7252 - mse: 594.7252 - mae: 22.6153 - val_loss: 445.6538 - val_mse: 445.6538 - val_mae: 20.1876\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 594.5626 - mse: 594.5626 - mae: 22.6117 - val_loss: 445.5066 - val_mse: 445.5066 - val_mae: 20.1840\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 594.3976 - mse: 594.3976 - mae: 22.6080 - val_loss: 445.3598 - val_mse: 445.3598 - val_mae: 20.1804\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 594.2333 - mse: 594.2333 - mae: 22.6044 - val_loss: 445.2130 - val_mse: 445.2130 - val_mae: 20.1767\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 594.0690 - mse: 594.0690 - mae: 22.6008 - val_loss: 445.0667 - val_mse: 445.0667 - val_mae: 20.1731\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 593.9051 - mse: 593.9051 - mae: 22.5971 - val_loss: 444.9213 - val_mse: 444.9213 - val_mae: 20.1695\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 593.7423 - mse: 593.7423 - mae: 22.5935 - val_loss: 444.7760 - val_mse: 444.7760 - val_mae: 20.1659\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 593.5796 - mse: 593.5796 - mae: 22.5899 - val_loss: 444.6316 - val_mse: 444.6316 - val_mae: 20.1623\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 593.4175 - mse: 593.4175 - mae: 22.5864 - val_loss: 444.4850 - val_mse: 444.4850 - val_mae: 20.1587\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 593.2535 - mse: 593.2535 - mae: 22.5828 - val_loss: 444.3387 - val_mse: 444.3387 - val_mae: 20.1550\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 593.0896 - mse: 593.0896 - mae: 22.5791 - val_loss: 444.1931 - val_mse: 444.1931 - val_mae: 20.1514\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 592.9265 - mse: 592.9265 - mae: 22.5755 - val_loss: 444.0477 - val_mse: 444.0477 - val_mae: 20.1478\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 592.7638 - mse: 592.7638 - mae: 22.5718 - val_loss: 443.9029 - val_mse: 443.9029 - val_mae: 20.1442\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 592.6015 - mse: 592.6015 - mae: 22.5683 - val_loss: 443.7572 - val_mse: 443.7572 - val_mae: 20.1406\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 592.4382 - mse: 592.4382 - mae: 22.5646 - val_loss: 443.6116 - val_mse: 443.6116 - val_mae: 20.1370\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 592.2752 - mse: 592.2752 - mae: 22.5610 - val_loss: 443.4668 - val_mse: 443.4668 - val_mae: 20.1334\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 592.1130 - mse: 592.1130 - mae: 22.5574 - val_loss: 443.3222 - val_mse: 443.3222 - val_mae: 20.1298\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 591.9508 - mse: 591.9508 - mae: 22.5538 - val_loss: 443.1760 - val_mse: 443.1760 - val_mae: 20.1262\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 591.7871 - mse: 591.7871 - mae: 22.5502 - val_loss: 443.0307 - val_mse: 443.0307 - val_mae: 20.1226\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 591.6241 - mse: 591.6241 - mae: 22.5466 - val_loss: 442.8856 - val_mse: 442.8856 - val_mae: 20.1190\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 591.4617 - mse: 591.4617 - mae: 22.5430 - val_loss: 442.7400 - val_mse: 442.7400 - val_mae: 20.1153\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 591.2986 - mse: 591.2986 - mae: 22.5394 - val_loss: 442.5947 - val_mse: 442.5947 - val_mae: 20.1117\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 591.1357 - mse: 591.1357 - mae: 22.5358 - val_loss: 442.4491 - val_mse: 442.4491 - val_mae: 20.1081\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 590.9728 - mse: 590.9728 - mae: 22.5321 - val_loss: 442.3055 - val_mse: 442.3055 - val_mae: 20.1045\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 590.8117 - mse: 590.8117 - mae: 22.5286 - val_loss: 442.1604 - val_mse: 442.1604 - val_mae: 20.1009\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 590.6492 - mse: 590.6492 - mae: 22.5250 - val_loss: 442.0153 - val_mse: 442.0153 - val_mae: 20.0973\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 590.4866 - mse: 590.4866 - mae: 22.5214 - val_loss: 441.8712 - val_mse: 441.8712 - val_mae: 20.0937\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 590.3251 - mse: 590.3251 - mae: 22.5178 - val_loss: 441.7257 - val_mse: 441.7257 - val_mae: 20.0901\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 590.1622 - mse: 590.1622 - mae: 22.5141 - val_loss: 441.5814 - val_mse: 441.5814 - val_mae: 20.0865\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 590.0004 - mse: 590.0004 - mae: 22.5106 - val_loss: 441.4368 - val_mse: 441.4368 - val_mae: 20.0829\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 589.8384 - mse: 589.8384 - mae: 22.5070 - val_loss: 441.2929 - val_mse: 441.2929 - val_mae: 20.0793\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 589.6770 - mse: 589.6770 - mae: 22.5033 - val_loss: 441.1486 - val_mse: 441.1486 - val_mae: 20.0757\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 589.5153 - mse: 589.5153 - mae: 22.4998 - val_loss: 441.0044 - val_mse: 441.0044 - val_mae: 20.0722\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 589.3538 - mse: 589.3538 - mae: 22.4962 - val_loss: 440.8592 - val_mse: 440.8592 - val_mae: 20.0685\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 589.1912 - mse: 589.1912 - mae: 22.4926 - val_loss: 440.7162 - val_mse: 440.7162 - val_mae: 20.0650\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 589.0308 - mse: 589.0308 - mae: 22.4890 - val_loss: 440.5722 - val_mse: 440.5722 - val_mae: 20.0614\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 588.8694 - mse: 588.8694 - mae: 22.4854 - val_loss: 440.4278 - val_mse: 440.4278 - val_mae: 20.0578\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 588.7076 - mse: 588.7076 - mae: 22.4818 - val_loss: 440.2827 - val_mse: 440.2827 - val_mae: 20.0542\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 588.5449 - mse: 588.5449 - mae: 22.4782 - val_loss: 440.1382 - val_mse: 440.1382 - val_mae: 20.0506\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 588.3830 - mse: 588.3830 - mae: 22.4746 - val_loss: 439.9944 - val_mse: 439.9944 - val_mae: 20.0470\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 588.2219 - mse: 588.2219 - mae: 22.4710 - val_loss: 439.8518 - val_mse: 439.8518 - val_mae: 20.0434\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 588.0619 - mse: 588.0619 - mae: 22.4675 - val_loss: 439.7072 - val_mse: 439.7072 - val_mae: 20.0398\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 587.8999 - mse: 587.8999 - mae: 22.4638 - val_loss: 439.5643 - val_mse: 439.5643 - val_mae: 20.0363\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 587.7396 - mse: 587.7396 - mae: 22.4603 - val_loss: 439.4194 - val_mse: 439.4194 - val_mae: 20.0326\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 587.5773 - mse: 587.5773 - mae: 22.4567 - val_loss: 439.2759 - val_mse: 439.2759 - val_mae: 20.0291\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 587.4165 - mse: 587.4165 - mae: 22.4531 - val_loss: 439.1333 - val_mse: 439.1333 - val_mae: 20.0255\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 587.2565 - mse: 587.2565 - mae: 22.4495 - val_loss: 438.9889 - val_mse: 438.9889 - val_mae: 20.0219\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 587.0947 - mse: 587.0947 - mae: 22.4459 - val_loss: 438.8451 - val_mse: 438.8451 - val_mae: 20.0183\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 586.9333 - mse: 586.9333 - mae: 22.4423 - val_loss: 438.7014 - val_mse: 438.7014 - val_mae: 20.0147\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 586.7725 - mse: 586.7725 - mae: 22.4387 - val_loss: 438.5570 - val_mse: 438.5570 - val_mae: 20.0111\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 586.6107 - mse: 586.6107 - mae: 22.4351 - val_loss: 438.4152 - val_mse: 438.4152 - val_mae: 20.0076\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 586.4514 - mse: 586.4514 - mae: 22.4316 - val_loss: 438.2715 - val_mse: 438.2715 - val_mae: 20.0040\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 586.2903 - mse: 586.2903 - mae: 22.4280 - val_loss: 438.1277 - val_mse: 438.1277 - val_mae: 20.0004\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 586.1292 - mse: 586.1292 - mae: 22.4244 - val_loss: 437.9841 - val_mse: 437.9841 - val_mae: 19.9968\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 585.9683 - mse: 585.9683 - mae: 22.4209 - val_loss: 437.8412 - val_mse: 437.8412 - val_mae: 19.9932\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 585.8079 - mse: 585.8079 - mae: 22.4173 - val_loss: 437.6971 - val_mse: 437.6971 - val_mae: 19.9896\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 585.6464 - mse: 585.6464 - mae: 22.4136 - val_loss: 437.5543 - val_mse: 437.5543 - val_mae: 19.9860\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 585.4863 - mse: 585.4863 - mae: 22.4101 - val_loss: 437.4098 - val_mse: 437.4098 - val_mae: 19.9824\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 585.3245 - mse: 585.3245 - mae: 22.4065 - val_loss: 437.2675 - val_mse: 437.2675 - val_mae: 19.9789\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 585.1647 - mse: 585.1647 - mae: 22.4029 - val_loss: 437.1245 - val_mse: 437.1245 - val_mae: 19.9753\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 585.0044 - mse: 585.0044 - mae: 22.3993 - val_loss: 436.9812 - val_mse: 436.9812 - val_mae: 19.9717\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 584.8439 - mse: 584.8439 - mae: 22.3957 - val_loss: 436.8389 - val_mse: 436.8389 - val_mae: 19.9681\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 584.6842 - mse: 584.6842 - mae: 22.3922 - val_loss: 436.6962 - val_mse: 436.6962 - val_mae: 19.9646\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 584.5242 - mse: 584.5242 - mae: 22.3886 - val_loss: 436.5531 - val_mse: 436.5531 - val_mae: 19.9610\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 584.3638 - mse: 584.3638 - mae: 22.3850 - val_loss: 436.4100 - val_mse: 436.4100 - val_mae: 19.9574\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 584.2032 - mse: 584.2032 - mae: 22.3814 - val_loss: 436.2678 - val_mse: 436.2678 - val_mae: 19.9538\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 584.0437 - mse: 584.0437 - mae: 22.3779 - val_loss: 436.1237 - val_mse: 436.1237 - val_mae: 19.9502\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 583.8821 - mse: 583.8821 - mae: 22.3742 - val_loss: 435.9810 - val_mse: 435.9810 - val_mae: 19.9466\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 583.7221 - mse: 583.7221 - mae: 22.3707 - val_loss: 435.8374 - val_mse: 435.8374 - val_mae: 19.9430\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 583.5611 - mse: 583.5611 - mae: 22.3671 - val_loss: 435.6942 - val_mse: 435.6942 - val_mae: 19.9394\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.1, model__n_hidden=1, model__n_neurons=5, model__optimizer=nesterov; total time=   5.1s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 977.8614 - mse: 977.8614 - mae: 23.2480 - val_loss: 184.4055 - val_mse: 184.4055 - val_mae: 12.2771\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 270.0834 - mse: 270.0834 - mae: 13.8370 - val_loss: 146.6388 - val_mse: 146.6388 - val_mae: 11.0624\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 236.7252 - mse: 236.7252 - mae: 12.7271 - val_loss: 142.7251 - val_mse: 142.7251 - val_mae: 10.3631\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 218.7345 - mse: 218.7345 - mae: 12.0060 - val_loss: 137.6879 - val_mse: 137.6879 - val_mae: 9.9583\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 202.1005 - mse: 202.1005 - mae: 11.3039 - val_loss: 106.7893 - val_mse: 106.7893 - val_mae: 9.2309\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 206.7620 - mse: 206.7620 - mae: 11.6158 - val_loss: 96.1742 - val_mse: 96.1742 - val_mae: 8.8216\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 181.8339 - mse: 181.8339 - mae: 10.6481 - val_loss: 89.7256 - val_mse: 89.7256 - val_mae: 8.6622\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 176.1321 - mse: 176.1321 - mae: 10.4253 - val_loss: 87.7610 - val_mse: 87.7610 - val_mae: 8.2525\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 161.2821 - mse: 161.2821 - mae: 9.8349 - val_loss: 97.4904 - val_mse: 97.4904 - val_mae: 8.2029\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 157.9490 - mse: 157.9490 - mae: 9.6890 - val_loss: 86.7353 - val_mse: 86.7353 - val_mae: 7.8491\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 148.0782 - mse: 148.0782 - mae: 9.2599 - val_loss: 76.7185 - val_mse: 76.7185 - val_mae: 7.5173\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 144.2858 - mse: 144.2858 - mae: 9.0754 - val_loss: 76.5368 - val_mse: 76.5368 - val_mae: 7.2974\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 137.5069 - mse: 137.5069 - mae: 8.8548 - val_loss: 76.9102 - val_mse: 76.9102 - val_mae: 7.0652\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 139.3999 - mse: 139.3999 - mae: 8.9680 - val_loss: 60.1621 - val_mse: 60.1621 - val_mae: 6.6860\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 126.6752 - mse: 126.6752 - mae: 8.4376 - val_loss: 67.1113 - val_mse: 67.1113 - val_mae: 6.4902\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 120.7622 - mse: 120.7622 - mae: 8.2248 - val_loss: 51.0759 - val_mse: 51.0759 - val_mae: 6.1590\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 119.2803 - mse: 119.2803 - mae: 8.0047 - val_loss: 60.3293 - val_mse: 60.3293 - val_mae: 6.0099\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 117.4815 - mse: 117.4815 - mae: 7.9403 - val_loss: 58.7839 - val_mse: 58.7839 - val_mae: 5.8176\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 112.1819 - mse: 112.1819 - mae: 7.8985 - val_loss: 48.9488 - val_mse: 48.9488 - val_mae: 5.4380\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 111.2596 - mse: 111.2596 - mae: 7.7341 - val_loss: 40.7296 - val_mse: 40.7296 - val_mae: 5.0899\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 104.9531 - mse: 104.9531 - mae: 7.4828 - val_loss: 33.4114 - val_mse: 33.4114 - val_mae: 4.8530\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 104.2413 - mse: 104.2413 - mae: 7.3565 - val_loss: 31.3686 - val_mse: 31.3686 - val_mae: 4.6488\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 100.2290 - mse: 100.2290 - mae: 7.2690 - val_loss: 39.1385 - val_mse: 39.1385 - val_mae: 4.6625\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 98.6703 - mse: 98.6703 - mae: 7.3004 - val_loss: 38.2805 - val_mse: 38.2805 - val_mae: 4.6080\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 96.8877 - mse: 96.8877 - mae: 7.2299 - val_loss: 33.0708 - val_mse: 33.0708 - val_mae: 4.3478\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 95.8910 - mse: 95.8910 - mae: 7.1581 - val_loss: 38.3413 - val_mse: 38.3413 - val_mae: 4.6430\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 94.6483 - mse: 94.6483 - mae: 7.1552 - val_loss: 24.3939 - val_mse: 24.3939 - val_mae: 3.9034\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 98.1258 - mse: 98.1258 - mae: 7.3231 - val_loss: 25.2996 - val_mse: 25.2996 - val_mae: 3.9091\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 91.3928 - mse: 91.3928 - mae: 7.0586 - val_loss: 30.4827 - val_mse: 30.4827 - val_mae: 4.1916\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 90.9854 - mse: 90.9854 - mae: 7.0768 - val_loss: 32.9369 - val_mse: 32.9369 - val_mae: 4.3848\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 92.1261 - mse: 92.1261 - mae: 7.2656 - val_loss: 22.3835 - val_mse: 22.3835 - val_mae: 3.8311\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 92.8500 - mse: 92.8500 - mae: 7.0498 - val_loss: 42.4332 - val_mse: 42.4332 - val_mae: 5.1021\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 91.9956 - mse: 91.9956 - mae: 7.2394 - val_loss: 36.2512 - val_mse: 36.2512 - val_mae: 4.7156\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 89.9441 - mse: 89.9441 - mae: 7.1892 - val_loss: 37.4378 - val_mse: 37.4378 - val_mae: 4.8223\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 89.9704 - mse: 89.9704 - mae: 7.2417 - val_loss: 35.4094 - val_mse: 35.4094 - val_mae: 4.7235\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 89.1350 - mse: 89.1350 - mae: 7.2334 - val_loss: 24.5913 - val_mse: 24.5913 - val_mae: 4.0023\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 90.6078 - mse: 90.6078 - mae: 7.1576 - val_loss: 26.0852 - val_mse: 26.0852 - val_mae: 4.0888\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 91.0378 - mse: 91.0378 - mae: 7.1244 - val_loss: 26.6083 - val_mse: 26.6083 - val_mae: 4.1278\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 89.5848 - mse: 89.5848 - mae: 7.1799 - val_loss: 27.3888 - val_mse: 27.3888 - val_mae: 4.1861\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 88.8834 - mse: 88.8834 - mae: 7.0313 - val_loss: 39.3874 - val_mse: 39.3874 - val_mae: 5.0974\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 89.8009 - mse: 89.8009 - mae: 7.3012 - val_loss: 24.0612 - val_mse: 24.0612 - val_mae: 3.9838\n",
      "Epoch 41: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.1, model__n_hidden=1, model__n_neurons=5, model__optimizer=nesterov; total time=   2.9s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 85901.6562 - mse: 85901.6562 - mae: 141.0453 - val_loss: 548.0331 - val_mse: 548.0331 - val_mae: 21.8229\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 556.5867 - mse: 556.5867 - mae: 21.8799 - val_loss: 547.8802 - val_mse: 547.8802 - val_mae: 21.8194\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 556.4334 - mse: 556.4334 - mae: 21.8764 - val_loss: 547.7271 - val_mse: 547.7271 - val_mae: 21.8159\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 556.2799 - mse: 556.2799 - mae: 21.8729 - val_loss: 547.5746 - val_mse: 547.5746 - val_mae: 21.8124\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 556.1271 - mse: 556.1271 - mae: 21.8694 - val_loss: 547.4225 - val_mse: 547.4225 - val_mae: 21.8089\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 555.9745 - mse: 555.9745 - mae: 21.8659 - val_loss: 547.2704 - val_mse: 547.2704 - val_mae: 21.8055\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 555.8220 - mse: 555.8220 - mae: 21.8625 - val_loss: 547.1172 - val_mse: 547.1172 - val_mae: 21.8019\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 555.6684 - mse: 555.6684 - mae: 21.8589 - val_loss: 546.9648 - val_mse: 546.9648 - val_mae: 21.7984\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 555.5157 - mse: 555.5157 - mae: 21.8554 - val_loss: 546.8129 - val_mse: 546.8129 - val_mae: 21.7950\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 555.3633 - mse: 555.3633 - mae: 21.8520 - val_loss: 546.6604 - val_mse: 546.6604 - val_mae: 21.7915\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 555.2104 - mse: 555.2104 - mae: 21.8485 - val_loss: 546.5081 - val_mse: 546.5081 - val_mae: 21.7880\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 555.0579 - mse: 555.0579 - mae: 21.8450 - val_loss: 546.3570 - val_mse: 546.3570 - val_mae: 21.7845\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 554.9062 - mse: 554.9062 - mae: 21.8415 - val_loss: 546.2049 - val_mse: 546.2049 - val_mae: 21.7810\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 554.7538 - mse: 554.7538 - mae: 21.8380 - val_loss: 546.0528 - val_mse: 546.0528 - val_mae: 21.7775\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 554.6013 - mse: 554.6013 - mae: 21.8345 - val_loss: 545.9011 - val_mse: 545.9011 - val_mae: 21.7740\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 554.4492 - mse: 554.4492 - mae: 21.8310 - val_loss: 545.7481 - val_mse: 545.7481 - val_mae: 21.7705\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 554.2959 - mse: 554.2959 - mae: 21.8275 - val_loss: 545.5966 - val_mse: 545.5966 - val_mae: 21.7670\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 554.1439 - mse: 554.1439 - mae: 21.8240 - val_loss: 545.4449 - val_mse: 545.4449 - val_mae: 21.7636\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 553.9919 - mse: 553.9919 - mae: 21.8206 - val_loss: 545.2939 - val_mse: 545.2939 - val_mae: 21.7601\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 553.8405 - mse: 553.8405 - mae: 21.8171 - val_loss: 545.1420 - val_mse: 545.1420 - val_mae: 21.7566\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 553.6883 - mse: 553.6883 - mae: 21.8136 - val_loss: 544.9910 - val_mse: 544.9910 - val_mae: 21.7531\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 553.5368 - mse: 553.5368 - mae: 21.8101 - val_loss: 544.8393 - val_mse: 544.8393 - val_mae: 21.7496\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 553.3848 - mse: 553.3848 - mae: 21.8067 - val_loss: 544.6874 - val_mse: 544.6874 - val_mae: 21.7461\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 553.2322 - mse: 553.2322 - mae: 21.8031 - val_loss: 544.5347 - val_mse: 544.5347 - val_mae: 21.7426\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 553.0792 - mse: 553.0792 - mae: 21.7996 - val_loss: 544.3824 - val_mse: 544.3824 - val_mae: 21.7391\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 552.9266 - mse: 552.9266 - mae: 21.7961 - val_loss: 544.2305 - val_mse: 544.2305 - val_mae: 21.7356\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 552.7743 - mse: 552.7743 - mae: 21.7926 - val_loss: 544.0790 - val_mse: 544.0790 - val_mae: 21.7321\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 552.6224 - mse: 552.6224 - mae: 21.7891 - val_loss: 543.9267 - val_mse: 543.9267 - val_mae: 21.7286\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 552.4698 - mse: 552.4698 - mae: 21.7856 - val_loss: 543.7755 - val_mse: 543.7755 - val_mae: 21.7252\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 552.3181 - mse: 552.3181 - mae: 21.7821 - val_loss: 543.6235 - val_mse: 543.6235 - val_mae: 21.7217\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 552.1658 - mse: 552.1658 - mae: 21.7786 - val_loss: 543.4717 - val_mse: 543.4717 - val_mae: 21.7182\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 552.0135 - mse: 552.0135 - mae: 21.7752 - val_loss: 543.3198 - val_mse: 543.3198 - val_mae: 21.7147\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 551.8614 - mse: 551.8614 - mae: 21.7716 - val_loss: 543.1694 - val_mse: 543.1694 - val_mae: 21.7112\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 551.7106 - mse: 551.7106 - mae: 21.7682 - val_loss: 543.0183 - val_mse: 543.0183 - val_mae: 21.7077\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 551.5590 - mse: 551.5590 - mae: 21.7647 - val_loss: 542.8666 - val_mse: 542.8666 - val_mae: 21.7042\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 551.4072 - mse: 551.4072 - mae: 21.7612 - val_loss: 542.7170 - val_mse: 542.7170 - val_mae: 21.7008\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 551.2571 - mse: 551.2571 - mae: 21.7577 - val_loss: 542.5671 - val_mse: 542.5671 - val_mae: 21.6973\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 551.1068 - mse: 551.1068 - mae: 21.7543 - val_loss: 542.4172 - val_mse: 542.4172 - val_mae: 21.6939\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 550.9565 - mse: 550.9565 - mae: 21.7509 - val_loss: 542.2664 - val_mse: 542.2664 - val_mae: 21.6904\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 550.8052 - mse: 550.8052 - mae: 21.7474 - val_loss: 542.1156 - val_mse: 542.1156 - val_mae: 21.6869\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 550.6541 - mse: 550.6541 - mae: 21.7439 - val_loss: 541.9653 - val_mse: 541.9653 - val_mae: 21.6835\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 550.5032 - mse: 550.5032 - mae: 21.7405 - val_loss: 541.8124 - val_mse: 541.8124 - val_mae: 21.6799\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 550.3500 - mse: 550.3500 - mae: 21.7369 - val_loss: 541.6616 - val_mse: 541.6616 - val_mae: 21.6765\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 550.1990 - mse: 550.1990 - mae: 21.7335 - val_loss: 541.5114 - val_mse: 541.5114 - val_mae: 21.6730\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 550.0482 - mse: 550.0482 - mae: 21.7300 - val_loss: 541.3600 - val_mse: 541.3600 - val_mae: 21.6695\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 549.8965 - mse: 549.8965 - mae: 21.7265 - val_loss: 541.2099 - val_mse: 541.2099 - val_mae: 21.6660\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 549.7461 - mse: 549.7461 - mae: 21.7230 - val_loss: 541.0601 - val_mse: 541.0601 - val_mae: 21.6626\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 549.5959 - mse: 549.5959 - mae: 21.7196 - val_loss: 540.9099 - val_mse: 540.9099 - val_mae: 21.6591\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 549.4453 - mse: 549.4453 - mae: 21.7161 - val_loss: 540.7595 - val_mse: 540.7595 - val_mae: 21.6556\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 549.2945 - mse: 549.2945 - mae: 21.7126 - val_loss: 540.6093 - val_mse: 540.6093 - val_mae: 21.6522\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 549.1439 - mse: 549.1439 - mae: 21.7091 - val_loss: 540.4595 - val_mse: 540.4595 - val_mae: 21.6487\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 548.9937 - mse: 548.9937 - mae: 21.7057 - val_loss: 540.3092 - val_mse: 540.3092 - val_mae: 21.6452\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 548.8430 - mse: 548.8430 - mae: 21.7023 - val_loss: 540.1583 - val_mse: 540.1583 - val_mae: 21.6418\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 548.6918 - mse: 548.6918 - mae: 21.6987 - val_loss: 540.0077 - val_mse: 540.0077 - val_mae: 21.6383\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 548.5407 - mse: 548.5407 - mae: 21.6953 - val_loss: 539.8577 - val_mse: 539.8577 - val_mae: 21.6348\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 548.3904 - mse: 548.3904 - mae: 21.6918 - val_loss: 539.7081 - val_mse: 539.7081 - val_mae: 21.6314\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 548.2403 - mse: 548.2403 - mae: 21.6884 - val_loss: 539.5575 - val_mse: 539.5575 - val_mae: 21.6279\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 548.0893 - mse: 548.0893 - mae: 21.6849 - val_loss: 539.4070 - val_mse: 539.4070 - val_mae: 21.6244\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 547.9385 - mse: 547.9385 - mae: 21.6814 - val_loss: 539.2560 - val_mse: 539.2560 - val_mae: 21.6209\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 547.7872 - mse: 547.7872 - mae: 21.6779 - val_loss: 539.1069 - val_mse: 539.1069 - val_mae: 21.6175\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 547.6375 - mse: 547.6375 - mae: 21.6745 - val_loss: 538.9560 - val_mse: 538.9560 - val_mae: 21.6140\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 547.4863 - mse: 547.4863 - mae: 21.6709 - val_loss: 538.8062 - val_mse: 538.8062 - val_mae: 21.6105\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 547.3362 - mse: 547.3362 - mae: 21.6675 - val_loss: 538.6571 - val_mse: 538.6571 - val_mae: 21.6070\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 547.1868 - mse: 547.1868 - mae: 21.6641 - val_loss: 538.5074 - val_mse: 538.5074 - val_mae: 21.6036\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 547.0366 - mse: 547.0366 - mae: 21.6606 - val_loss: 538.3585 - val_mse: 538.3585 - val_mae: 21.6001\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 546.8872 - mse: 546.8872 - mae: 21.6571 - val_loss: 538.2077 - val_mse: 538.2077 - val_mae: 21.5966\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 546.7361 - mse: 546.7361 - mae: 21.6537 - val_loss: 538.0579 - val_mse: 538.0579 - val_mae: 21.5932\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 546.5861 - mse: 546.5861 - mae: 21.6502 - val_loss: 537.9093 - val_mse: 537.9093 - val_mae: 21.5897\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 546.4370 - mse: 546.4370 - mae: 21.6468 - val_loss: 537.7584 - val_mse: 537.7584 - val_mae: 21.5862\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 546.2857 - mse: 546.2857 - mae: 21.6433 - val_loss: 537.6097 - val_mse: 537.6097 - val_mae: 21.5828\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 546.1365 - mse: 546.1365 - mae: 21.6398 - val_loss: 537.4600 - val_mse: 537.4600 - val_mae: 21.5793\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 545.9865 - mse: 545.9865 - mae: 21.6363 - val_loss: 537.3109 - val_mse: 537.3109 - val_mae: 21.5759\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 545.8371 - mse: 545.8371 - mae: 21.6329 - val_loss: 537.1622 - val_mse: 537.1622 - val_mae: 21.5724\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 545.6878 - mse: 545.6878 - mae: 21.6294 - val_loss: 537.0129 - val_mse: 537.0129 - val_mae: 21.5690\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 545.5383 - mse: 545.5383 - mae: 21.6260 - val_loss: 536.8638 - val_mse: 536.8638 - val_mae: 21.5655\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 545.3887 - mse: 545.3887 - mae: 21.6225 - val_loss: 536.7143 - val_mse: 536.7143 - val_mae: 21.5620\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 545.2389 - mse: 545.2389 - mae: 21.6190 - val_loss: 536.5652 - val_mse: 536.5652 - val_mae: 21.5586\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 545.0894 - mse: 545.0894 - mae: 21.6156 - val_loss: 536.4153 - val_mse: 536.4153 - val_mae: 21.5551\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 544.9393 - mse: 544.9393 - mae: 21.6120 - val_loss: 536.2679 - val_mse: 536.2679 - val_mae: 21.5517\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 544.7914 - mse: 544.7914 - mae: 21.6087 - val_loss: 536.1196 - val_mse: 536.1196 - val_mae: 21.5482\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 544.6426 - mse: 544.6426 - mae: 21.6053 - val_loss: 535.9690 - val_mse: 535.9690 - val_mae: 21.5448\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 544.4915 - mse: 544.4915 - mae: 21.6017 - val_loss: 535.8193 - val_mse: 535.8193 - val_mae: 21.5413\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 544.3415 - mse: 544.3415 - mae: 21.5983 - val_loss: 535.6690 - val_mse: 535.6690 - val_mae: 21.5378\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 544.1909 - mse: 544.1909 - mae: 21.5948 - val_loss: 535.5200 - val_mse: 535.5200 - val_mae: 21.5343\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 544.0416 - mse: 544.0416 - mae: 21.5913 - val_loss: 535.3719 - val_mse: 535.3719 - val_mae: 21.5309\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 543.8930 - mse: 543.8930 - mae: 21.5879 - val_loss: 535.2235 - val_mse: 535.2235 - val_mae: 21.5274\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 543.7443 - mse: 543.7443 - mae: 21.5844 - val_loss: 535.0754 - val_mse: 535.0754 - val_mae: 21.5240\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 543.5958 - mse: 543.5958 - mae: 21.5810 - val_loss: 534.9271 - val_mse: 534.9271 - val_mae: 21.5206\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 543.4470 - mse: 543.4470 - mae: 21.5776 - val_loss: 534.7787 - val_mse: 534.7787 - val_mae: 21.5171\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 543.2984 - mse: 543.2984 - mae: 21.5741 - val_loss: 534.6309 - val_mse: 534.6309 - val_mae: 21.5137\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 543.1500 - mse: 543.1500 - mae: 21.5707 - val_loss: 534.4817 - val_mse: 534.4817 - val_mae: 21.5102\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 543.0003 - mse: 543.0003 - mae: 21.5672 - val_loss: 534.3315 - val_mse: 534.3315 - val_mae: 21.5067\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 542.8498 - mse: 542.8498 - mae: 21.5637 - val_loss: 534.1829 - val_mse: 534.1829 - val_mae: 21.5033\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 542.7010 - mse: 542.7010 - mae: 21.5602 - val_loss: 534.0355 - val_mse: 534.0355 - val_mae: 21.4998\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 542.5530 - mse: 542.5530 - mae: 21.5568 - val_loss: 533.8863 - val_mse: 533.8863 - val_mae: 21.4964\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 542.4036 - mse: 542.4036 - mae: 21.5533 - val_loss: 533.7391 - val_mse: 533.7391 - val_mae: 21.4929\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 542.2560 - mse: 542.2560 - mae: 21.5499 - val_loss: 533.5907 - val_mse: 533.5907 - val_mae: 21.4895\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 542.1072 - mse: 542.1072 - mae: 21.5464 - val_loss: 533.4423 - val_mse: 533.4423 - val_mae: 21.4860\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 541.9584 - mse: 541.9584 - mae: 21.5430 - val_loss: 533.2939 - val_mse: 533.2939 - val_mae: 21.4826\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 541.8098 - mse: 541.8098 - mae: 21.5395 - val_loss: 533.1465 - val_mse: 533.1465 - val_mae: 21.4791\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.1, model__n_hidden=1, model__n_neurons=5, model__optimizer=nesterov; total time=   5.4s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 71ms/step - loss: 4500.4287 - mse: 4500.4287 - mae: 59.3775 - val_loss: 4925.5083 - val_mse: 4925.5083 - val_mae: 59.1258\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4478.7524 - mse: 4478.7524 - mae: 59.1947 - val_loss: 4907.5947 - val_mse: 4907.5947 - val_mae: 58.9708\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 4457.5903 - mse: 4457.5903 - mae: 59.0215 - val_loss: 4892.2305 - val_mse: 4892.2305 - val_mae: 58.8138\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 4436.8096 - mse: 4436.8096 - mae: 58.8440 - val_loss: 4875.5444 - val_mse: 4875.5444 - val_mae: 58.6594\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4415.6978 - mse: 4415.6978 - mae: 58.6654 - val_loss: 4859.6450 - val_mse: 4859.6450 - val_mae: 58.5046\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 4395.7271 - mse: 4395.7271 - mae: 58.4942 - val_loss: 4844.6938 - val_mse: 4844.6938 - val_mae: 58.3496\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4375.2808 - mse: 4375.2808 - mae: 58.3184 - val_loss: 4829.0420 - val_mse: 4829.0420 - val_mae: 58.1981\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 4356.0410 - mse: 4356.0410 - mae: 58.1550 - val_loss: 4810.6685 - val_mse: 4810.6685 - val_mae: 58.0461\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 4335.8633 - mse: 4335.8633 - mae: 57.9917 - val_loss: 4794.6958 - val_mse: 4794.6958 - val_mae: 57.8965\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 4316.2354 - mse: 4316.2354 - mae: 57.8244 - val_loss: 4779.4912 - val_mse: 4779.4912 - val_mae: 57.7455\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 4296.5752 - mse: 4296.5752 - mae: 57.6575 - val_loss: 4761.4053 - val_mse: 4761.4053 - val_mae: 57.5982\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4277.2358 - mse: 4277.2358 - mae: 57.4973 - val_loss: 4745.3286 - val_mse: 4745.3286 - val_mae: 57.4467\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4258.3091 - mse: 4258.3091 - mae: 57.3324 - val_loss: 4729.3218 - val_mse: 4729.3218 - val_mae: 57.2944\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 4238.7773 - mse: 4238.7773 - mae: 57.1663 - val_loss: 4711.5356 - val_mse: 4711.5356 - val_mae: 57.1498\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 4220.4790 - mse: 4220.4790 - mae: 57.0145 - val_loss: 4696.8130 - val_mse: 4696.8130 - val_mae: 56.9996\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 4202.1943 - mse: 4202.1943 - mae: 56.8493 - val_loss: 4681.4668 - val_mse: 4681.4668 - val_mae: 56.8520\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 4183.3818 - mse: 4183.3818 - mae: 56.6901 - val_loss: 4663.1665 - val_mse: 4663.1665 - val_mae: 56.7123\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 4164.7866 - mse: 4164.7866 - mae: 56.5426 - val_loss: 4645.7622 - val_mse: 4645.7622 - val_mae: 56.5713\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4146.3208 - mse: 4146.3208 - mae: 56.3893 - val_loss: 4629.5103 - val_mse: 4629.5103 - val_mae: 56.4241\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 4128.5786 - mse: 4128.5786 - mae: 56.2351 - val_loss: 4612.0176 - val_mse: 4612.0176 - val_mae: 56.2760\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 4110.1724 - mse: 4110.1724 - mae: 56.0775 - val_loss: 4595.2109 - val_mse: 4595.2109 - val_mae: 56.1308\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4091.7397 - mse: 4091.7397 - mae: 55.9240 - val_loss: 4579.4814 - val_mse: 4579.4814 - val_mae: 55.9874\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 4073.7087 - mse: 4073.7087 - mae: 55.7724 - val_loss: 4561.6396 - val_mse: 4561.6396 - val_mae: 55.8464\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 4056.2903 - mse: 4056.2903 - mae: 55.6236 - val_loss: 4543.8135 - val_mse: 4543.8135 - val_mae: 55.6997\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4037.7144 - mse: 4037.7144 - mae: 55.4696 - val_loss: 4527.1880 - val_mse: 4527.1880 - val_mae: 55.5562\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4019.9075 - mse: 4019.9075 - mae: 55.3170 - val_loss: 4510.4810 - val_mse: 4510.4810 - val_mae: 55.4117\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 4002.3777 - mse: 4002.3777 - mae: 55.1712 - val_loss: 4492.2051 - val_mse: 4492.2051 - val_mae: 55.2710\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3984.8938 - mse: 3984.8938 - mae: 55.0245 - val_loss: 4475.3955 - val_mse: 4475.3955 - val_mae: 55.1258\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3966.2849 - mse: 3966.2849 - mae: 54.8719 - val_loss: 4459.5806 - val_mse: 4459.5806 - val_mae: 54.9872\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3949.7100 - mse: 3949.7100 - mae: 54.7269 - val_loss: 4442.6924 - val_mse: 4442.6924 - val_mae: 54.8422\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3932.3127 - mse: 3932.3127 - mae: 54.5832 - val_loss: 4424.8052 - val_mse: 4424.8052 - val_mae: 54.7017\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3914.8384 - mse: 3914.8384 - mae: 54.4388 - val_loss: 4408.1333 - val_mse: 4408.1333 - val_mae: 54.5619\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3897.3240 - mse: 3897.3240 - mae: 54.2955 - val_loss: 4391.3081 - val_mse: 4391.3081 - val_mae: 54.4242\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3880.6116 - mse: 3880.6116 - mae: 54.1508 - val_loss: 4374.8354 - val_mse: 4374.8354 - val_mae: 54.2805\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3863.9224 - mse: 3863.9224 - mae: 54.0042 - val_loss: 4358.0352 - val_mse: 4358.0352 - val_mae: 54.1356\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3847.0859 - mse: 3847.0859 - mae: 53.8549 - val_loss: 4344.5356 - val_mse: 4344.5356 - val_mae: 53.9896\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3829.3494 - mse: 3829.3494 - mae: 53.7023 - val_loss: 4326.6392 - val_mse: 4326.6392 - val_mae: 53.8586\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 3813.4868 - mse: 3813.4868 - mae: 53.5776 - val_loss: 4307.1665 - val_mse: 4307.1665 - val_mae: 53.7249\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 3796.2029 - mse: 3796.2029 - mae: 53.4458 - val_loss: 4289.4941 - val_mse: 4289.4941 - val_mae: 53.5920\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3779.9143 - mse: 3779.9143 - mae: 53.3130 - val_loss: 4271.4678 - val_mse: 4271.4678 - val_mae: 53.4566\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3763.4714 - mse: 3763.4714 - mae: 53.1740 - val_loss: 4254.3604 - val_mse: 4254.3604 - val_mae: 53.3151\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3746.4192 - mse: 3746.4192 - mae: 53.0352 - val_loss: 4237.1465 - val_mse: 4237.1465 - val_mae: 53.1779\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3730.1948 - mse: 3730.1948 - mae: 52.8975 - val_loss: 4219.9097 - val_mse: 4219.9097 - val_mae: 53.0384\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3713.9209 - mse: 3713.9209 - mae: 52.7704 - val_loss: 4200.1836 - val_mse: 4200.1836 - val_mae: 52.9100\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3697.3330 - mse: 3697.3330 - mae: 52.6365 - val_loss: 4183.0635 - val_mse: 4183.0635 - val_mae: 52.7689\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3680.4502 - mse: 3680.4502 - mae: 52.4947 - val_loss: 4166.2822 - val_mse: 4166.2822 - val_mae: 52.6334\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3664.0645 - mse: 3664.0645 - mae: 52.3553 - val_loss: 4149.9648 - val_mse: 4149.9648 - val_mae: 52.4984\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 3648.2866 - mse: 3648.2866 - mae: 52.2238 - val_loss: 4131.3730 - val_mse: 4131.3730 - val_mae: 52.3628\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3631.7742 - mse: 3631.7742 - mae: 52.0896 - val_loss: 4114.1968 - val_mse: 4114.1968 - val_mae: 52.2267\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3615.8735 - mse: 3615.8735 - mae: 51.9585 - val_loss: 4095.6934 - val_mse: 4095.6934 - val_mae: 52.0949\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3599.4897 - mse: 3599.4897 - mae: 51.8294 - val_loss: 4078.2686 - val_mse: 4078.2686 - val_mae: 51.9623\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3583.7021 - mse: 3583.7021 - mae: 51.6979 - val_loss: 4060.7095 - val_mse: 4060.7095 - val_mae: 51.8265\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3567.4111 - mse: 3567.4111 - mae: 51.5649 - val_loss: 4043.3247 - val_mse: 4043.3247 - val_mae: 51.6928\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3551.4475 - mse: 3551.4475 - mae: 51.4340 - val_loss: 4025.9504 - val_mse: 4025.9504 - val_mae: 51.5600\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3535.4075 - mse: 3535.4075 - mae: 51.3084 - val_loss: 4006.9905 - val_mse: 4006.9905 - val_mae: 51.4303\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3519.5591 - mse: 3519.5591 - mae: 51.1800 - val_loss: 3989.3259 - val_mse: 3989.3259 - val_mae: 51.2967\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3503.6133 - mse: 3503.6133 - mae: 51.0485 - val_loss: 3972.0957 - val_mse: 3972.0957 - val_mae: 51.1641\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3488.8811 - mse: 3488.8811 - mae: 50.9304 - val_loss: 3951.2334 - val_mse: 3951.2334 - val_mae: 51.0348\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3472.1292 - mse: 3472.1292 - mae: 50.7988 - val_loss: 3935.5439 - val_mse: 3935.5439 - val_mae: 50.8977\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3455.9985 - mse: 3455.9985 - mae: 50.6617 - val_loss: 3918.8176 - val_mse: 3918.8176 - val_mae: 50.7680\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3440.9673 - mse: 3440.9673 - mae: 50.5340 - val_loss: 3901.2556 - val_mse: 3901.2556 - val_mae: 50.6354\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3425.3813 - mse: 3425.3813 - mae: 50.4037 - val_loss: 3885.0532 - val_mse: 3885.0532 - val_mae: 50.5029\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3410.1055 - mse: 3410.1055 - mae: 50.2726 - val_loss: 3867.9155 - val_mse: 3867.9155 - val_mae: 50.3710\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 3394.7581 - mse: 3394.7581 - mae: 50.1502 - val_loss: 3849.6719 - val_mse: 3849.6719 - val_mae: 50.2412\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 3379.0063 - mse: 3379.0063 - mae: 50.0247 - val_loss: 3833.2544 - val_mse: 3833.2544 - val_mae: 50.1115\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 3364.2615 - mse: 3364.2615 - mae: 49.9047 - val_loss: 3813.9287 - val_mse: 3813.9287 - val_mae: 49.9858\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3348.6716 - mse: 3348.6716 - mae: 49.7884 - val_loss: 3795.9329 - val_mse: 3795.9329 - val_mae: 49.8581\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3333.9326 - mse: 3333.9326 - mae: 49.6706 - val_loss: 3776.8733 - val_mse: 3776.8733 - val_mae: 49.7323\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3318.0642 - mse: 3318.0642 - mae: 49.5421 - val_loss: 3762.3728 - val_mse: 3762.3728 - val_mae: 49.5994\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3303.5444 - mse: 3303.5444 - mae: 49.4135 - val_loss: 3745.1780 - val_mse: 3745.1780 - val_mae: 49.4670\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3288.4048 - mse: 3288.4048 - mae: 49.2867 - val_loss: 3728.4768 - val_mse: 3728.4768 - val_mae: 49.3328\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3273.5632 - mse: 3273.5632 - mae: 49.1640 - val_loss: 3709.6038 - val_mse: 3709.6038 - val_mae: 49.2079\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 3258.1423 - mse: 3258.1423 - mae: 49.0395 - val_loss: 3694.6443 - val_mse: 3694.6443 - val_mae: 49.0750\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3244.0127 - mse: 3244.0127 - mae: 48.9150 - val_loss: 3676.7073 - val_mse: 3676.7073 - val_mae: 48.9476\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3228.5623 - mse: 3228.5623 - mae: 48.7952 - val_loss: 3659.9983 - val_mse: 3659.9983 - val_mae: 48.8212\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3214.1313 - mse: 3214.1313 - mae: 48.6744 - val_loss: 3643.0156 - val_mse: 3643.0156 - val_mae: 48.6924\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3199.8374 - mse: 3199.8374 - mae: 48.5524 - val_loss: 3625.2068 - val_mse: 3625.2068 - val_mae: 48.5639\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3184.3008 - mse: 3184.3008 - mae: 48.4253 - val_loss: 3610.0530 - val_mse: 3610.0530 - val_mae: 48.4354\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3170.2119 - mse: 3170.2119 - mae: 48.3042 - val_loss: 3593.1035 - val_mse: 3593.1035 - val_mae: 48.3085\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3155.7061 - mse: 3155.7061 - mae: 48.1801 - val_loss: 3577.4241 - val_mse: 3577.4241 - val_mae: 48.1773\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3141.1345 - mse: 3141.1345 - mae: 48.0571 - val_loss: 3560.4817 - val_mse: 3560.4817 - val_mae: 48.0505\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3126.5127 - mse: 3126.5127 - mae: 47.9403 - val_loss: 3541.5266 - val_mse: 3541.5266 - val_mae: 47.9272\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3111.5645 - mse: 3111.5645 - mae: 47.8261 - val_loss: 3524.4424 - val_mse: 3524.4424 - val_mae: 47.8023\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3097.3538 - mse: 3097.3538 - mae: 47.7055 - val_loss: 3507.9578 - val_mse: 3507.9578 - val_mae: 47.6730\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3083.0601 - mse: 3083.0601 - mae: 47.5815 - val_loss: 3491.9629 - val_mse: 3491.9629 - val_mae: 47.5435\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3068.6836 - mse: 3068.6836 - mae: 47.4561 - val_loss: 3476.4363 - val_mse: 3476.4363 - val_mae: 47.4140\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3054.9675 - mse: 3054.9675 - mae: 47.3297 - val_loss: 3461.6384 - val_mse: 3461.6384 - val_mae: 47.2810\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 3040.6741 - mse: 3040.6741 - mae: 47.2077 - val_loss: 3444.0933 - val_mse: 3444.0933 - val_mae: 47.1606\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3027.2278 - mse: 3027.2278 - mae: 47.0970 - val_loss: 3426.7334 - val_mse: 3426.7334 - val_mae: 47.0331\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3012.1985 - mse: 3012.1985 - mae: 46.9691 - val_loss: 3411.6318 - val_mse: 3411.6318 - val_mae: 46.9051\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2998.4377 - mse: 2998.4377 - mae: 46.8495 - val_loss: 3395.5542 - val_mse: 3395.5542 - val_mae: 46.7803\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2984.8333 - mse: 2984.8333 - mae: 46.7313 - val_loss: 3378.9373 - val_mse: 3378.9373 - val_mae: 46.6494\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2970.6658 - mse: 2970.6658 - mae: 46.6079 - val_loss: 3362.4268 - val_mse: 3362.4268 - val_mae: 46.5242\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2956.6445 - mse: 2956.6445 - mae: 46.4926 - val_loss: 3346.0325 - val_mse: 3346.0325 - val_mae: 46.4023\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2942.5330 - mse: 2942.5330 - mae: 46.3705 - val_loss: 3331.5076 - val_mse: 3331.5076 - val_mae: 46.2759\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2929.1887 - mse: 2929.1887 - mae: 46.2512 - val_loss: 3314.7991 - val_mse: 3314.7991 - val_mae: 46.1498\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2915.3635 - mse: 2915.3635 - mae: 46.1385 - val_loss: 3297.7341 - val_mse: 3297.7341 - val_mae: 46.0284\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2901.2666 - mse: 2901.2666 - mae: 46.0146 - val_loss: 3284.5083 - val_mse: 3284.5083 - val_mae: 45.8992\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2888.0344 - mse: 2888.0344 - mae: 45.8914 - val_loss: 3267.1157 - val_mse: 3267.1157 - val_mae: 45.7755\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2874.2354 - mse: 2874.2354 - mae: 45.7792 - val_loss: 3250.7080 - val_mse: 3250.7080 - val_mae: 45.6517\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.9, model__n_hidden=0, model__n_neurons=25, model__optimizer=adam; total time=   6.0s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 5555.7310 - mse: 5555.7310 - mae: 59.3047 - val_loss: 6123.1899 - val_mse: 6123.1899 - val_mae: 59.2406\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 5465.4575 - mse: 5465.4575 - mae: 58.6382 - val_loss: 6029.1987 - val_mse: 6029.1987 - val_mae: 58.5577\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 5376.7241 - mse: 5376.7241 - mae: 57.9843 - val_loss: 5936.6753 - val_mse: 5936.6753 - val_mae: 57.8776\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 5287.8989 - mse: 5287.8989 - mae: 57.3234 - val_loss: 5846.1602 - val_mse: 5846.1602 - val_mae: 57.2555\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 5201.3008 - mse: 5201.3008 - mae: 56.6562 - val_loss: 5757.3013 - val_mse: 5757.3013 - val_mae: 56.6729\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 5116.9453 - mse: 5116.9453 - mae: 56.0105 - val_loss: 5669.7305 - val_mse: 5669.7305 - val_mae: 56.0920\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 5034.2295 - mse: 5034.2295 - mae: 55.3636 - val_loss: 5583.4917 - val_mse: 5583.4917 - val_mae: 55.5135\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 4953.8335 - mse: 4953.8335 - mae: 54.7195 - val_loss: 5497.2788 - val_mse: 5497.2788 - val_mae: 54.9285\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 4870.5112 - mse: 4870.5112 - mae: 54.0832 - val_loss: 5414.7319 - val_mse: 5414.7319 - val_mae: 54.3607\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 4792.5547 - mse: 4792.5547 - mae: 53.4702 - val_loss: 5333.0947 - val_mse: 5333.0947 - val_mae: 53.7924\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4715.5029 - mse: 4715.5029 - mae: 52.8467 - val_loss: 5252.1064 - val_mse: 5252.1064 - val_mae: 53.2630\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4638.7568 - mse: 4638.7568 - mae: 52.2375 - val_loss: 5174.0591 - val_mse: 5174.0591 - val_mae: 52.7481\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4564.3213 - mse: 4564.3213 - mae: 51.6329 - val_loss: 5097.0566 - val_mse: 5097.0566 - val_mae: 52.2338\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4494.1665 - mse: 4494.1665 - mae: 51.0393 - val_loss: 5019.4121 - val_mse: 5019.4121 - val_mae: 51.7097\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 4420.7119 - mse: 4420.7119 - mae: 50.4510 - val_loss: 4944.9502 - val_mse: 4944.9502 - val_mae: 51.2003\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4349.9102 - mse: 4349.9102 - mae: 49.8671 - val_loss: 4873.2817 - val_mse: 4873.2817 - val_mae: 50.7038\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 4282.9590 - mse: 4282.9590 - mae: 49.3013 - val_loss: 4801.9302 - val_mse: 4801.9302 - val_mae: 50.2039\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 4214.5620 - mse: 4214.5620 - mae: 48.7592 - val_loss: 4732.5391 - val_mse: 4732.5391 - val_mae: 49.7107\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4148.6777 - mse: 4148.6777 - mae: 48.1976 - val_loss: 4663.9209 - val_mse: 4663.9209 - val_mae: 49.2228\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 4084.0630 - mse: 4084.0630 - mae: 47.6581 - val_loss: 4596.3770 - val_mse: 4596.3770 - val_mae: 48.7705\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 4021.3840 - mse: 4021.3840 - mae: 47.1155 - val_loss: 4529.3809 - val_mse: 4529.3809 - val_mae: 48.3163\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3957.8923 - mse: 3957.8923 - mae: 46.5875 - val_loss: 4464.5713 - val_mse: 4464.5713 - val_mae: 47.8706\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3896.3286 - mse: 3896.3286 - mae: 46.0549 - val_loss: 4401.3115 - val_mse: 4401.3115 - val_mae: 47.4600\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3838.0137 - mse: 3838.0137 - mae: 45.5389 - val_loss: 4338.2969 - val_mse: 4338.2969 - val_mae: 47.0571\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3778.5933 - mse: 3778.5933 - mae: 45.0162 - val_loss: 4278.3257 - val_mse: 4278.3257 - val_mae: 46.6943\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3722.0791 - mse: 3722.0791 - mae: 44.5239 - val_loss: 4218.4019 - val_mse: 4218.4019 - val_mae: 46.3397\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3665.0571 - mse: 3665.0571 - mae: 44.0312 - val_loss: 4160.2749 - val_mse: 4160.2749 - val_mae: 45.9913\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3612.3464 - mse: 3612.3464 - mae: 43.5519 - val_loss: 4101.4692 - val_mse: 4101.4692 - val_mae: 45.6346\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3556.2544 - mse: 3556.2544 - mae: 43.0701 - val_loss: 4046.0283 - val_mse: 4046.0283 - val_mae: 45.2933\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3504.2688 - mse: 3504.2688 - mae: 42.6198 - val_loss: 3990.8940 - val_mse: 3990.8940 - val_mae: 44.9497\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3452.8186 - mse: 3452.8186 - mae: 42.1492 - val_loss: 3937.5454 - val_mse: 3937.5454 - val_mae: 44.6130\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3402.6379 - mse: 3402.6379 - mae: 41.7049 - val_loss: 3885.5442 - val_mse: 3885.5442 - val_mae: 44.2802\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3353.6772 - mse: 3353.6772 - mae: 41.2688 - val_loss: 3834.4697 - val_mse: 3834.4697 - val_mae: 43.9490\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3304.9707 - mse: 3304.9707 - mae: 40.8292 - val_loss: 3784.3948 - val_mse: 3784.3948 - val_mae: 43.6205\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3260.1260 - mse: 3260.1260 - mae: 40.4151 - val_loss: 3733.0347 - val_mse: 3733.0347 - val_mae: 43.2794\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3211.6528 - mse: 3211.6528 - mae: 39.9980 - val_loss: 3685.2644 - val_mse: 3685.2644 - val_mae: 42.9565\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3167.4272 - mse: 3167.4272 - mae: 39.5897 - val_loss: 3637.8025 - val_mse: 3637.8025 - val_mae: 42.6318\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3122.9521 - mse: 3122.9521 - mae: 39.1980 - val_loss: 3591.6450 - val_mse: 3591.6450 - val_mae: 42.3120\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3080.1746 - mse: 3080.1746 - mae: 38.8253 - val_loss: 3546.3933 - val_mse: 3546.3933 - val_mae: 41.9943\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3037.3889 - mse: 3037.3889 - mae: 38.4414 - val_loss: 3501.9724 - val_mse: 3501.9724 - val_mae: 41.6783\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2996.6169 - mse: 2996.6169 - mae: 38.0744 - val_loss: 3457.6584 - val_mse: 3457.6584 - val_mae: 41.3591\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2955.7603 - mse: 2955.7603 - mae: 37.7168 - val_loss: 3415.2253 - val_mse: 3415.2253 - val_mae: 41.0485\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2916.2402 - mse: 2916.2402 - mae: 37.3542 - val_loss: 3374.1418 - val_mse: 3374.1418 - val_mae: 40.7688\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2879.4116 - mse: 2879.4116 - mae: 37.0138 - val_loss: 3332.8926 - val_mse: 3332.8926 - val_mae: 40.5240\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2840.4272 - mse: 2840.4272 - mae: 36.6736 - val_loss: 3293.7549 - val_mse: 3293.7549 - val_mae: 40.2883\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2803.8430 - mse: 2803.8430 - mae: 36.3447 - val_loss: 3255.7651 - val_mse: 3255.7651 - val_mae: 40.0565\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2767.7839 - mse: 2767.7839 - mae: 36.0172 - val_loss: 3218.7490 - val_mse: 3218.7490 - val_mae: 39.8360\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2734.0605 - mse: 2734.0605 - mae: 35.6899 - val_loss: 3181.1440 - val_mse: 3181.1440 - val_mae: 39.6340\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2699.5305 - mse: 2699.5305 - mae: 35.3746 - val_loss: 3144.5608 - val_mse: 3144.5608 - val_mae: 39.4346\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2665.7991 - mse: 2665.7991 - mae: 35.0499 - val_loss: 3108.1926 - val_mse: 3108.1926 - val_mae: 39.2334\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2632.2832 - mse: 2632.2832 - mae: 34.7338 - val_loss: 3072.6829 - val_mse: 3072.6829 - val_mae: 39.0581\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 2599.0845 - mse: 2599.0845 - mae: 34.4226 - val_loss: 3038.2957 - val_mse: 3038.2957 - val_mae: 38.8918\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2567.7986 - mse: 2567.7986 - mae: 34.1265 - val_loss: 3004.2263 - val_mse: 3004.2263 - val_mae: 38.7241\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2536.5557 - mse: 2536.5557 - mae: 33.8480 - val_loss: 2971.4563 - val_mse: 2971.4563 - val_mae: 38.5608\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2506.0125 - mse: 2506.0125 - mae: 33.5746 - val_loss: 2939.5112 - val_mse: 2939.5112 - val_mae: 38.3991\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2477.3770 - mse: 2477.3770 - mae: 33.3139 - val_loss: 2908.1021 - val_mse: 2908.1021 - val_mae: 38.2376\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2448.6135 - mse: 2448.6135 - mae: 33.0544 - val_loss: 2878.1880 - val_mse: 2878.1880 - val_mae: 38.0815\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2421.2371 - mse: 2421.2371 - mae: 32.8041 - val_loss: 2848.4551 - val_mse: 2848.4551 - val_mae: 37.9237\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2393.8232 - mse: 2393.8232 - mae: 32.5480 - val_loss: 2819.5229 - val_mse: 2819.5229 - val_mae: 37.7677\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2367.5171 - mse: 2367.5171 - mae: 32.3231 - val_loss: 2791.7295 - val_mse: 2791.7295 - val_mae: 37.6157\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2342.1682 - mse: 2342.1682 - mae: 32.0886 - val_loss: 2764.6548 - val_mse: 2764.6548 - val_mae: 37.4653\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 2318.0417 - mse: 2318.0417 - mae: 31.8653 - val_loss: 2737.7786 - val_mse: 2737.7786 - val_mae: 37.3136\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 2293.4390 - mse: 2293.4390 - mae: 31.6460 - val_loss: 2711.5872 - val_mse: 2711.5872 - val_mae: 37.1634\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 2269.1770 - mse: 2269.1770 - mae: 31.4363 - val_loss: 2686.6001 - val_mse: 2686.6001 - val_mae: 37.0181\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2246.1924 - mse: 2246.1924 - mae: 31.2405 - val_loss: 2661.5767 - val_mse: 2661.5767 - val_mae: 36.8705\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2224.1985 - mse: 2224.1985 - mae: 31.0485 - val_loss: 2636.4001 - val_mse: 2636.4001 - val_mae: 36.7193\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2200.7957 - mse: 2200.7957 - mae: 30.8394 - val_loss: 2612.8286 - val_mse: 2612.8286 - val_mae: 36.5756\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2180.2507 - mse: 2180.2507 - mae: 30.6512 - val_loss: 2589.0732 - val_mse: 2589.0732 - val_mae: 36.4287\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2158.6609 - mse: 2158.6609 - mae: 30.4725 - val_loss: 2566.5872 - val_mse: 2566.5872 - val_mae: 36.2876\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 2138.2815 - mse: 2138.2815 - mae: 30.2964 - val_loss: 2544.6016 - val_mse: 2544.6016 - val_mae: 36.1474\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2118.9954 - mse: 2118.9954 - mae: 30.1322 - val_loss: 2522.8174 - val_mse: 2522.8174 - val_mae: 36.0064\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2099.8738 - mse: 2099.8738 - mae: 29.9790 - val_loss: 2501.2942 - val_mse: 2501.2942 - val_mae: 35.8649\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2079.8181 - mse: 2079.8181 - mae: 29.8251 - val_loss: 2481.5212 - val_mse: 2481.5212 - val_mae: 35.7329\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2061.8442 - mse: 2061.8442 - mae: 29.6778 - val_loss: 2461.7578 - val_mse: 2461.7578 - val_mae: 35.6152\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2044.0050 - mse: 2044.0050 - mae: 29.5441 - val_loss: 2442.1372 - val_mse: 2442.1372 - val_mae: 35.5011\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2026.6736 - mse: 2026.6736 - mae: 29.4140 - val_loss: 2422.6482 - val_mse: 2422.6482 - val_mae: 35.3852\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2010.1820 - mse: 2010.1820 - mae: 29.2876 - val_loss: 2403.1990 - val_mse: 2403.1990 - val_mae: 35.2678\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1992.3232 - mse: 1992.3232 - mae: 29.1622 - val_loss: 2385.1208 - val_mse: 2385.1208 - val_mae: 35.1572\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1976.4871 - mse: 1976.4871 - mae: 29.0462 - val_loss: 2366.8125 - val_mse: 2366.8125 - val_mae: 35.0434\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1960.1212 - mse: 1960.1212 - mae: 28.9219 - val_loss: 2349.4968 - val_mse: 2349.4968 - val_mae: 34.9338\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1945.4348 - mse: 1945.4348 - mae: 28.8086 - val_loss: 2332.0691 - val_mse: 2332.0691 - val_mae: 34.8220\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1929.6338 - mse: 1929.6338 - mae: 28.6921 - val_loss: 2315.3645 - val_mse: 2315.3645 - val_mae: 34.7128\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1914.6100 - mse: 1914.6100 - mae: 28.5787 - val_loss: 2298.7573 - val_mse: 2298.7573 - val_mae: 34.6026\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1899.4443 - mse: 1899.4443 - mae: 28.4613 - val_loss: 2283.0378 - val_mse: 2283.0378 - val_mae: 34.4966\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1885.4879 - mse: 1885.4879 - mae: 28.3597 - val_loss: 2267.1541 - val_mse: 2267.1541 - val_mae: 34.3881\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1871.8383 - mse: 1871.8383 - mae: 28.2721 - val_loss: 2251.4048 - val_mse: 2251.4048 - val_mae: 34.2787\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1858.8647 - mse: 1858.8647 - mae: 28.1713 - val_loss: 2235.6760 - val_mse: 2235.6760 - val_mae: 34.1677\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1845.0979 - mse: 1845.0979 - mae: 28.0683 - val_loss: 2221.1018 - val_mse: 2221.1018 - val_mae: 34.0704\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1833.2772 - mse: 1833.2772 - mae: 27.9907 - val_loss: 2206.1340 - val_mse: 2206.1340 - val_mae: 33.9835\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1819.7792 - mse: 1819.7792 - mae: 27.8958 - val_loss: 2192.6555 - val_mse: 2192.6555 - val_mae: 33.9038\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1808.3551 - mse: 1808.3551 - mae: 27.8241 - val_loss: 2178.8562 - val_mse: 2178.8562 - val_mae: 33.8210\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1795.4893 - mse: 1795.4893 - mae: 27.7336 - val_loss: 2165.9172 - val_mse: 2165.9172 - val_mae: 33.7409\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1785.0941 - mse: 1785.0941 - mae: 27.6568 - val_loss: 2152.5720 - val_mse: 2152.5720 - val_mae: 33.6574\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1772.3905 - mse: 1772.3905 - mae: 27.5887 - val_loss: 2140.7622 - val_mse: 2140.7622 - val_mae: 33.5830\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1761.7169 - mse: 1761.7169 - mae: 27.5240 - val_loss: 2128.2249 - val_mse: 2128.2249 - val_mae: 33.5029\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1751.8684 - mse: 1751.8684 - mae: 27.4581 - val_loss: 2114.9451 - val_mse: 2114.9451 - val_mae: 33.4161\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1740.3440 - mse: 1740.3440 - mae: 27.3843 - val_loss: 2102.8584 - val_mse: 2102.8584 - val_mae: 33.3360\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1729.7019 - mse: 1729.7019 - mae: 27.3307 - val_loss: 2091.1189 - val_mse: 2091.1189 - val_mae: 33.2575\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1719.5398 - mse: 1719.5398 - mae: 27.2660 - val_loss: 2079.1628 - val_mse: 2079.1628 - val_mae: 33.1759\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1709.6162 - mse: 1709.6162 - mae: 27.2159 - val_loss: 2067.6956 - val_mse: 2067.6956 - val_mae: 33.0966\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.9, model__n_hidden=0, model__n_neurons=25, model__optimizer=adam; total time=   5.5s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 22ms/step - loss: 4657.6577 - mse: 4657.6577 - mae: 64.9408 - val_loss: 4963.5220 - val_mse: 4963.5220 - val_mae: 67.7371\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 4561.1562 - mse: 4561.1562 - mae: 64.2224 - val_loss: 4862.5469 - val_mse: 4862.5469 - val_mae: 66.9970\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 4468.9878 - mse: 4468.9878 - mae: 63.5103 - val_loss: 4762.1196 - val_mse: 4762.1196 - val_mae: 66.2526\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4375.4175 - mse: 4375.4175 - mae: 62.7955 - val_loss: 4663.9375 - val_mse: 4663.9375 - val_mae: 65.5164\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 4285.6411 - mse: 4285.6411 - mae: 62.0853 - val_loss: 4566.7212 - val_mse: 4566.7212 - val_mae: 64.7791\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 4195.0991 - mse: 4195.0991 - mae: 61.3764 - val_loss: 4471.9683 - val_mse: 4471.9683 - val_mae: 64.0521\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4106.7173 - mse: 4106.7173 - mae: 60.6782 - val_loss: 4379.1440 - val_mse: 4379.1440 - val_mae: 63.3315\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4021.8162 - mse: 4021.8162 - mae: 59.9794 - val_loss: 4286.6748 - val_mse: 4286.6748 - val_mae: 62.6054\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3935.5068 - mse: 3935.5068 - mae: 59.2836 - val_loss: 4196.8726 - val_mse: 4196.8726 - val_mae: 61.8918\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3852.8398 - mse: 3852.8398 - mae: 58.5958 - val_loss: 4108.0220 - val_mse: 4108.0220 - val_mae: 61.1773\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3769.9990 - mse: 3769.9990 - mae: 57.9120 - val_loss: 4021.5857 - val_mse: 4021.5857 - val_mae: 60.4737\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3690.2634 - mse: 3690.2634 - mae: 57.2313 - val_loss: 3935.9111 - val_mse: 3935.9111 - val_mae: 59.7683\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 3611.6587 - mse: 3611.6587 - mae: 56.5561 - val_loss: 3851.5115 - val_mse: 3851.5115 - val_mae: 59.0647\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3533.0376 - mse: 3533.0376 - mae: 55.8805 - val_loss: 3769.2327 - val_mse: 3769.2327 - val_mae: 58.3705\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3456.9993 - mse: 3456.9993 - mae: 55.2111 - val_loss: 3688.1272 - val_mse: 3688.1272 - val_mae: 57.6780\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3382.1306 - mse: 3382.1306 - mae: 54.5438 - val_loss: 3608.9341 - val_mse: 3608.9341 - val_mae: 56.9936\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3308.9043 - mse: 3308.9043 - mae: 53.8809 - val_loss: 3530.7974 - val_mse: 3530.7974 - val_mae: 56.3098\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3236.6450 - mse: 3236.6450 - mae: 53.2282 - val_loss: 3454.3662 - val_mse: 3454.3662 - val_mae: 55.6322\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3165.9390 - mse: 3165.9390 - mae: 52.5777 - val_loss: 3379.1201 - val_mse: 3379.1201 - val_mae: 54.9569\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3096.6111 - mse: 3096.6111 - mae: 51.9247 - val_loss: 3305.0857 - val_mse: 3305.0857 - val_mae: 54.2842\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3028.1980 - mse: 3028.1980 - mae: 51.2821 - val_loss: 3232.8982 - val_mse: 3232.8982 - val_mae: 53.6199\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2961.4604 - mse: 2961.4604 - mae: 50.6427 - val_loss: 3162.2725 - val_mse: 3162.2725 - val_mae: 52.9616\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2895.2109 - mse: 2895.2109 - mae: 50.0117 - val_loss: 3093.6318 - val_mse: 3093.6318 - val_mae: 52.3135\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2831.9990 - mse: 2831.9990 - mae: 49.3855 - val_loss: 3025.3611 - val_mse: 3025.3611 - val_mae: 51.6609\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2768.7639 - mse: 2768.7639 - mae: 48.7565 - val_loss: 2958.6177 - val_mse: 2958.6177 - val_mae: 51.0146\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2706.4492 - mse: 2706.4492 - mae: 48.1356 - val_loss: 2893.6121 - val_mse: 2893.6121 - val_mae: 50.3771\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2646.6516 - mse: 2646.6516 - mae: 47.5191 - val_loss: 2829.1230 - val_mse: 2829.1230 - val_mae: 49.7360\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2587.7649 - mse: 2587.7649 - mae: 46.9055 - val_loss: 2765.5269 - val_mse: 2765.5269 - val_mae: 49.0954\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2528.7109 - mse: 2528.7109 - mae: 46.2921 - val_loss: 2704.5017 - val_mse: 2704.5017 - val_mae: 48.4724\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2472.0007 - mse: 2472.0007 - mae: 45.6902 - val_loss: 2644.5735 - val_mse: 2644.5735 - val_mae: 47.8525\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2416.8394 - mse: 2416.8394 - mae: 45.0941 - val_loss: 2585.1074 - val_mse: 2585.1074 - val_mae: 47.2292\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2361.5693 - mse: 2361.5693 - mae: 44.4987 - val_loss: 2527.2971 - val_mse: 2527.2971 - val_mae: 46.6148\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 2308.6816 - mse: 2308.6816 - mae: 43.9030 - val_loss: 2470.1365 - val_mse: 2470.1365 - val_mae: 45.9994\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 2255.8044 - mse: 2255.8044 - mae: 43.3135 - val_loss: 2414.7651 - val_mse: 2414.7651 - val_mae: 45.3948\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 2204.2866 - mse: 2204.2866 - mae: 42.7364 - val_loss: 2361.0012 - val_mse: 2361.0012 - val_mae: 44.7995\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 2154.2554 - mse: 2154.2554 - mae: 42.1611 - val_loss: 2308.1423 - val_mse: 2308.1423 - val_mae: 44.2063\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 2105.3416 - mse: 2105.3416 - mae: 41.5858 - val_loss: 2256.2952 - val_mse: 2256.2952 - val_mae: 43.6163\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 2057.4241 - mse: 2057.4241 - mae: 41.0249 - val_loss: 2205.2583 - val_mse: 2205.2583 - val_mae: 43.0272\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 2011.1077 - mse: 2011.1077 - mae: 40.4637 - val_loss: 2154.8069 - val_mse: 2154.8069 - val_mae: 42.4370\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1964.0170 - mse: 1964.0170 - mae: 39.9023 - val_loss: 2106.3127 - val_mse: 2106.3127 - val_mae: 41.8613\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1919.0592 - mse: 1919.0592 - mae: 39.3502 - val_loss: 2058.9314 - val_mse: 2058.9314 - val_mae: 41.2910\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1874.2386 - mse: 1874.2386 - mae: 38.8092 - val_loss: 2013.2333 - val_mse: 2013.2333 - val_mae: 40.7326\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1832.2457 - mse: 1832.2457 - mae: 38.2691 - val_loss: 1967.0663 - val_mse: 1967.0663 - val_mae: 40.1607\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1789.7795 - mse: 1789.7795 - mae: 37.7260 - val_loss: 1922.1978 - val_mse: 1922.1978 - val_mae: 39.5969\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1748.2244 - mse: 1748.2244 - mae: 37.1892 - val_loss: 1878.8304 - val_mse: 1878.8304 - val_mae: 39.0435\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1707.8000 - mse: 1707.8000 - mae: 36.6581 - val_loss: 1836.3235 - val_mse: 1836.3235 - val_mae: 38.4938\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1669.4141 - mse: 1669.4141 - mae: 36.1321 - val_loss: 1794.0526 - val_mse: 1794.0526 - val_mae: 37.9386\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1629.8292 - mse: 1629.8292 - mae: 35.6153 - val_loss: 1753.9274 - val_mse: 1753.9274 - val_mae: 37.4032\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1593.2148 - mse: 1593.2148 - mae: 35.1065 - val_loss: 1714.1354 - val_mse: 1714.1354 - val_mae: 36.8647\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1555.9041 - mse: 1555.9041 - mae: 34.5961 - val_loss: 1675.5670 - val_mse: 1675.5670 - val_mae: 36.3352\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1520.8007 - mse: 1520.8007 - mae: 34.0919 - val_loss: 1637.4923 - val_mse: 1637.4923 - val_mae: 35.8043\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1485.2430 - mse: 1485.2430 - mae: 33.5965 - val_loss: 1600.6771 - val_mse: 1600.6771 - val_mae: 35.2830\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1451.2672 - mse: 1451.2672 - mae: 33.1094 - val_loss: 1564.4797 - val_mse: 1564.4797 - val_mae: 34.7623\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1417.7505 - mse: 1417.7505 - mae: 32.6195 - val_loss: 1529.4037 - val_mse: 1529.4037 - val_mae: 34.2499\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1385.1161 - mse: 1385.1161 - mae: 32.1411 - val_loss: 1495.4509 - val_mse: 1495.4509 - val_mae: 33.7463\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1354.1456 - mse: 1354.1456 - mae: 31.6653 - val_loss: 1461.8475 - val_mse: 1461.8475 - val_mae: 33.2398\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1322.8298 - mse: 1322.8298 - mae: 31.1976 - val_loss: 1429.4761 - val_mse: 1429.4761 - val_mae: 32.7446\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1293.0371 - mse: 1293.0371 - mae: 30.7313 - val_loss: 1397.5304 - val_mse: 1397.5304 - val_mae: 32.2486\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1262.9926 - mse: 1262.9926 - mae: 30.2753 - val_loss: 1367.1382 - val_mse: 1367.1382 - val_mae: 31.7685\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1235.1250 - mse: 1235.1250 - mae: 29.8188 - val_loss: 1336.7850 - val_mse: 1336.7850 - val_mae: 31.2820\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1207.5515 - mse: 1207.5515 - mae: 29.3656 - val_loss: 1307.0471 - val_mse: 1307.0471 - val_mae: 30.7973\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1179.8955 - mse: 1179.8955 - mae: 28.9170 - val_loss: 1278.5787 - val_mse: 1278.5787 - val_mae: 30.3259\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1153.4622 - mse: 1153.4622 - mae: 28.4778 - val_loss: 1250.7640 - val_mse: 1250.7640 - val_mae: 29.8579\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1127.8093 - mse: 1127.8093 - mae: 28.0498 - val_loss: 1223.5085 - val_mse: 1223.5085 - val_mae: 29.3914\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1102.5837 - mse: 1102.5837 - mae: 27.6108 - val_loss: 1197.0475 - val_mse: 1197.0475 - val_mae: 28.9315\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1078.1631 - mse: 1078.1631 - mae: 27.1882 - val_loss: 1171.3201 - val_mse: 1171.3201 - val_mae: 28.4808\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1054.2661 - mse: 1054.2661 - mae: 26.7712 - val_loss: 1146.3708 - val_mse: 1146.3708 - val_mae: 28.0614\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1031.3459 - mse: 1031.3459 - mae: 26.3557 - val_loss: 1121.8602 - val_mse: 1121.8602 - val_mae: 27.6423\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1008.4274 - mse: 1008.4274 - mae: 25.9520 - val_loss: 1098.4806 - val_mse: 1098.4806 - val_mae: 27.2360\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 986.9122 - mse: 986.9122 - mae: 25.5555 - val_loss: 1075.2916 - val_mse: 1075.2916 - val_mae: 26.8266\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 965.3466 - mse: 965.3466 - mae: 25.1698 - val_loss: 1052.9681 - val_mse: 1052.9681 - val_mae: 26.4255\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 944.7082 - mse: 944.7082 - mae: 24.7816 - val_loss: 1031.2712 - val_mse: 1031.2712 - val_mae: 26.0291\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 924.4357 - mse: 924.4357 - mae: 24.4072 - val_loss: 1010.2573 - val_mse: 1010.2573 - val_mae: 25.6387\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 905.1063 - mse: 905.1063 - mae: 24.0376 - val_loss: 989.3981 - val_mse: 989.3981 - val_mae: 25.2446\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 885.9834 - mse: 885.9834 - mae: 23.6721 - val_loss: 969.1901 - val_mse: 969.1901 - val_mae: 24.8555\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 867.3871 - mse: 867.3871 - mae: 23.3186 - val_loss: 949.6872 - val_mse: 949.6872 - val_mae: 24.4736\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 849.2631 - mse: 849.2631 - mae: 22.9612 - val_loss: 930.7382 - val_mse: 930.7382 - val_mae: 24.0964\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 831.8834 - mse: 831.8834 - mae: 22.6156 - val_loss: 912.2485 - val_mse: 912.2485 - val_mae: 23.7217\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 815.0695 - mse: 815.0695 - mae: 22.2707 - val_loss: 894.2077 - val_mse: 894.2077 - val_mae: 23.3493\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 798.3913 - mse: 798.3913 - mae: 21.9425 - val_loss: 876.9534 - val_mse: 876.9534 - val_mae: 23.0252\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 782.3527 - mse: 782.3527 - mae: 21.6218 - val_loss: 860.2884 - val_mse: 860.2884 - val_mae: 22.7071\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 766.9412 - mse: 766.9412 - mae: 21.3088 - val_loss: 843.7595 - val_mse: 843.7595 - val_mae: 22.3869\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 751.2161 - mse: 751.2161 - mae: 20.9914 - val_loss: 828.0226 - val_mse: 828.0226 - val_mae: 22.0762\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 736.7192 - mse: 736.7192 - mae: 20.7040 - val_loss: 812.5798 - val_mse: 812.5798 - val_mae: 21.7655\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 722.6058 - mse: 722.6058 - mae: 20.4033 - val_loss: 797.4958 - val_mse: 797.4958 - val_mae: 21.4744\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 708.8578 - mse: 708.8578 - mae: 20.1170 - val_loss: 782.9492 - val_mse: 782.9492 - val_mae: 21.1916\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 695.5135 - mse: 695.5135 - mae: 19.8372 - val_loss: 768.9339 - val_mse: 768.9339 - val_mae: 20.9143\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 682.5361 - mse: 682.5361 - mae: 19.5646 - val_loss: 755.5683 - val_mse: 755.5683 - val_mae: 20.6444\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 670.1674 - mse: 670.1674 - mae: 19.2987 - val_loss: 742.5997 - val_mse: 742.5997 - val_mae: 20.3781\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 658.0458 - mse: 658.0458 - mae: 19.0442 - val_loss: 729.9218 - val_mse: 729.9218 - val_mae: 20.1127\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 646.3278 - mse: 646.3278 - mae: 18.7878 - val_loss: 717.5112 - val_mse: 717.5112 - val_mae: 19.8477\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 634.8159 - mse: 634.8159 - mae: 18.5404 - val_loss: 705.5481 - val_mse: 705.5481 - val_mae: 19.5872\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 623.9106 - mse: 623.9106 - mae: 18.3050 - val_loss: 693.6889 - val_mse: 693.6889 - val_mae: 19.3248\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 613.0734 - mse: 613.0734 - mae: 18.0633 - val_loss: 682.3519 - val_mse: 682.3519 - val_mae: 19.0697\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 602.6298 - mse: 602.6298 - mae: 17.8383 - val_loss: 671.5075 - val_mse: 671.5075 - val_mae: 18.8193\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 592.5988 - mse: 592.5988 - mae: 17.6166 - val_loss: 660.9719 - val_mse: 660.9719 - val_mae: 18.5724\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 582.9984 - mse: 582.9984 - mae: 17.4220 - val_loss: 650.7521 - val_mse: 650.7521 - val_mae: 18.3268\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 573.6032 - mse: 573.6032 - mae: 17.2122 - val_loss: 640.8488 - val_mse: 640.8488 - val_mae: 18.0928\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 564.3404 - mse: 564.3404 - mae: 17.0194 - val_loss: 631.3212 - val_mse: 631.3212 - val_mae: 17.8765\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 555.7565 - mse: 555.7565 - mae: 16.8264 - val_loss: 621.9943 - val_mse: 621.9943 - val_mae: 17.6611\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.9, model__n_hidden=0, model__n_neurons=25, model__optimizer=adam; total time=   5.9s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 25ms/step - loss: 63310618624.0000 - mse: 63310614528.0000 - mae: 94662.0625 - val_loss: 15793726160896.0000 - val_mse: 15793726160896.0000 - val_mae: 3890212.5000\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 539946762793354526720.0000 - mse: 539946762793354526720.0000 - mae: 9319663616.0000 - val_loss: 123501942365403469578240.0000 - val_mse: 123501942365403469578240.0000 - val_mae: 344288952320.0000\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4199966062160681462036554579968.0000 - mse: 4199966062160681462036554579968.0000 - mae: 790760991490048.0000 - val_loss: 1098942274764358849563038207442944.0000 - val_mse: 1098942274764358849563038207442944.0000 - val_mae: 32375860761722880.0000\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: inf - mse: inf - mae: 79339914635386028032.0000 - val_loss: inf - val_mse: inf - val_mae: 2636512212029001433088.0000\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: inf - mse: inf - mae: 6609735879399201132511232.0000 - val_loss: inf - val_mse: inf - val_mae: 234631535017885344534102016.0000\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: inf - mse: inf - mae: 517199584054049753992697741312.0000 - val_loss: inf - val_mse: inf - val_mae: 20273994271021332990262121594880.0000\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: inf - mse: inf - mae: 52421293920522370402683956353826816.0000 - val_loss: inf - val_mse: inf - val_mae: inf\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.9, model__n_hidden=0, model__n_neurons=125, model__optimizer=momentum; total time=   1.2s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 22ms/step - loss: 1242312736768.0000 - mse: 1242312736768.0000 - mae: 446428.9375 - val_loss: 229694934876160.0000 - val_mse: 229694934876160.0000 - val_mae: 14892006.0000\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 4591528187970254274560.0000 - mse: 4591528187970254274560.0000 - mae: 26746167296.0000 - val_loss: 1029279392206641227104256.0000 - val_mse: 1029279392206641227104256.0000 - val_mae: 997138300928.0000\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 20877513009763528552229764071424.0000 - mse: 20877513009763528552229764071424.0000 - mae: 1815372885917696.0000 - val_loss: 3766569052415100699555893079965696.0000 - val_mse: 3766569052415100699555893079965696.0000 - val_mae: 60348731504984064.0000\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: inf - mse: inf - mae: 114843417775156756480.0000 - val_loss: inf - val_mse: inf - val_mae: 3544678558011981037568.0000\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: inf - mse: inf - mae: 6580414203233287496794112.0000 - val_loss: inf - val_mse: inf - val_mae: 221720382139534923714789376.0000\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: inf - mse: inf - mae: 355068011197097017243017936896.0000 - val_loss: inf - val_mse: inf - val_mae: 12897896587628321055730974064640.0000\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: inf - mse: inf - mae: 23517453935434111856566837866135552.0000 - val_loss: inf - val_mse: inf - val_mae: 874104351522924842056496830981603328.0000\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.9, model__n_hidden=0, model__n_neurons=125, model__optimizer=momentum; total time=   1.1s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 20ms/step - loss: 1674487168.0000 - mse: 1674487168.0000 - mae: 16734.7402 - val_loss: 256257933312.0000 - val_mse: 256257933312.0000 - val_mae: 495483.2500\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3309616311268540416.0000 - mse: 3309616311268540416.0000 - mae: 699456320.0000 - val_loss: 707226004446358011904.0000 - val_mse: 707226004446358011904.0000 - val_mae: 26009335808.0000\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 11228145293319598399046025216.0000 - mse: 11228145293319598399046025216.0000 - mae: 44248653103104.0000 - val_loss: 1928021191257987226005959344128.0000 - val_mse: 1928021191257987226005959344128.0000 - val_mae: 1364126576148480.0000\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: inf - mse: inf - mae: 2293815726395359232.0000 - val_loss: inf - val_mse: inf - val_mae: 75049154070873899008.0000\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: inf - mse: inf - mae: 139548006628946159337472.0000 - val_loss: inf - val_mse: inf - val_mae: 4101774201212880837148672.0000\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: inf - mse: inf - mae: 7054141187032397104975708160.0000 - val_loss: inf - val_mse: inf - val_mae: 210657760808954294429481435136.0000\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: inf - mse: inf - mae: 354090774801885712135015024820224.0000 - val_loss: inf - val_mse: inf - val_mae: 11001245075018763877255668710768640.0000\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.9, model__n_hidden=0, model__n_neurons=125, model__optimizer=momentum; total time=   1.0s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 22ms/step - loss: 17105431822336.0000 - mse: 17105431822336.0000 - mae: 2061589.8750 - val_loss: 10711966.0000 - val_mse: 10711966.0000 - val_mae: 3272.9080\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 10681749.0000 - mse: 10681749.0000 - mae: 3268.2810 - val_loss: 10675460.0000 - val_mse: 10675460.0000 - val_mae: 3267.3262\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 10644051.0000 - mse: 10644051.0000 - mae: 3262.5090 - val_loss: 10637589.0000 - val_mse: 10637589.0000 - val_mae: 3261.5261\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 10606265.0000 - mse: 10606265.0000 - mae: 3256.7134 - val_loss: 10599859.0000 - val_mse: 10599859.0000 - val_mae: 3255.7366\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 10568621.0000 - mse: 10568621.0000 - mae: 3250.9282 - val_loss: 10562263.0000 - val_mse: 10562263.0000 - val_mae: 3249.9578\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 10531105.0000 - mse: 10531105.0000 - mae: 3245.1536 - val_loss: 10524794.0000 - val_mse: 10524794.0000 - val_mae: 3244.1885\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 10493719.0000 - mse: 10493719.0000 - mae: 3239.3884 - val_loss: 10487460.0000 - val_mse: 10487460.0000 - val_mae: 3238.4287\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 10456468.0000 - mse: 10456468.0000 - mae: 3233.6328 - val_loss: 10450263.0000 - val_mse: 10450263.0000 - val_mae: 3232.6809\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 10419354.0000 - mse: 10419354.0000 - mae: 3227.8894 - val_loss: 10413199.0000 - val_mse: 10413199.0000 - val_mae: 3226.9426\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 10382372.0000 - mse: 10382372.0000 - mae: 3222.1558 - val_loss: 10376263.0000 - val_mse: 10376263.0000 - val_mae: 3221.2146\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 10345517.0000 - mse: 10345517.0000 - mae: 3216.4314 - val_loss: 10339460.0000 - val_mse: 10339460.0000 - val_mae: 3215.4971\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 10308796.0000 - mse: 10308796.0000 - mae: 3210.7180 - val_loss: 10302785.0000 - val_mse: 10302785.0000 - val_mae: 3209.7896\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 10272202.0000 - mse: 10272202.0000 - mae: 3205.0139 - val_loss: 10266241.0000 - val_mse: 10266241.0000 - val_mae: 3204.0918\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 10235740.0000 - mse: 10235740.0000 - mae: 3199.3208 - val_loss: 10229825.0000 - val_mse: 10229825.0000 - val_mae: 3198.4041\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 10199404.0000 - mse: 10199404.0000 - mae: 3193.6375 - val_loss: 10193538.0000 - val_mse: 10193538.0000 - val_mae: 3192.7261\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 10163199.0000 - mse: 10163199.0000 - mae: 3187.9641 - val_loss: 10157381.0000 - val_mse: 10157381.0000 - val_mae: 3187.0583\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 10127121.0000 - mse: 10127121.0000 - mae: 3182.3003 - val_loss: 10121353.0000 - val_mse: 10121353.0000 - val_mae: 3181.4016\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 10091173.0000 - mse: 10091173.0000 - mae: 3176.6470 - val_loss: 10085449.0000 - val_mse: 10085449.0000 - val_mae: 3175.7537\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 10055350.0000 - mse: 10055350.0000 - mae: 3171.0032 - val_loss: 10049677.0000 - val_mse: 10049677.0000 - val_mae: 3170.1162\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 10019658.0000 - mse: 10019658.0000 - mae: 3165.3704 - val_loss: 10014031.0000 - val_mse: 10014031.0000 - val_mae: 3164.4893\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 9984091.0000 - mse: 9984091.0000 - mae: 3159.7476 - val_loss: 9978513.0000 - val_mse: 9978513.0000 - val_mae: 3158.8721\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 9948652.0000 - mse: 9948652.0000 - mae: 3154.1345 - val_loss: 9943119.0000 - val_mse: 9943119.0000 - val_mae: 3153.2651\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 9913338.0000 - mse: 9913338.0000 - mae: 3148.5312 - val_loss: 9907853.0000 - val_mse: 9907853.0000 - val_mae: 3147.6685\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 9878152.0000 - mse: 9878152.0000 - mae: 3142.9390 - val_loss: 9872713.0000 - val_mse: 9872713.0000 - val_mae: 3142.0813\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 9843088.0000 - mse: 9843088.0000 - mae: 3137.3562 - val_loss: 9837697.0000 - val_mse: 9837697.0000 - val_mae: 3136.5044\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 9808150.0000 - mse: 9808150.0000 - mae: 3131.7827 - val_loss: 9802803.0000 - val_mse: 9802803.0000 - val_mae: 3130.9365\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 9773335.0000 - mse: 9773335.0000 - mae: 3126.2195 - val_loss: 9768033.0000 - val_mse: 9768033.0000 - val_mae: 3125.3794\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 9738641.0000 - mse: 9738641.0000 - mae: 3120.6658 - val_loss: 9733385.0000 - val_mse: 9733385.0000 - val_mae: 3119.8313\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 9704074.0000 - mse: 9704074.0000 - mae: 3115.1218 - val_loss: 9698862.0000 - val_mse: 9698862.0000 - val_mae: 3114.2937\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 9669627.0000 - mse: 9669627.0000 - mae: 3109.5889 - val_loss: 9664459.0000 - val_mse: 9664459.0000 - val_mae: 3108.7651\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 9635302.0000 - mse: 9635302.0000 - mae: 3104.0642 - val_loss: 9630179.0000 - val_mse: 9630179.0000 - val_mae: 3103.2468\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 9601097.0000 - mse: 9601097.0000 - mae: 3098.5496 - val_loss: 9596023.0000 - val_mse: 9596023.0000 - val_mae: 3097.7388\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 9567018.0000 - mse: 9567018.0000 - mae: 3093.0457 - val_loss: 9561987.0000 - val_mse: 9561987.0000 - val_mae: 3092.2402\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 9533059.0000 - mse: 9533059.0000 - mae: 3087.5518 - val_loss: 9528074.0000 - val_mse: 9528074.0000 - val_mae: 3086.7517\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 9499223.0000 - mse: 9499223.0000 - mae: 3082.0667 - val_loss: 9494280.0000 - val_mse: 9494280.0000 - val_mae: 3081.2725\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 9465505.0000 - mse: 9465505.0000 - mae: 3076.5930 - val_loss: 9460607.0000 - val_mse: 9460607.0000 - val_mae: 3075.8037\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 9431905.0000 - mse: 9431905.0000 - mae: 3071.1265 - val_loss: 9427049.0000 - val_mse: 9427049.0000 - val_mae: 3070.3438\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 9398424.0000 - mse: 9398424.0000 - mae: 3065.6707 - val_loss: 9393609.0000 - val_mse: 9393609.0000 - val_mae: 3064.8936\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 9365060.0000 - mse: 9365060.0000 - mae: 3060.2244 - val_loss: 9360292.0000 - val_mse: 9360292.0000 - val_mae: 3059.4531\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 9331818.0000 - mse: 9331818.0000 - mae: 3054.7881 - val_loss: 9327092.0000 - val_mse: 9327092.0000 - val_mae: 3054.0225\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 9298692.0000 - mse: 9298692.0000 - mae: 3049.3618 - val_loss: 9294010.0000 - val_mse: 9294010.0000 - val_mae: 3048.6013\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 9265686.0000 - mse: 9265686.0000 - mae: 3043.9448 - val_loss: 9261045.0000 - val_mse: 9261045.0000 - val_mae: 3043.1904\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 9232794.0000 - mse: 9232794.0000 - mae: 3038.5376 - val_loss: 9228196.0000 - val_mse: 9228196.0000 - val_mae: 3037.7881\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 9200020.0000 - mse: 9200020.0000 - mae: 3033.1394 - val_loss: 9195465.0000 - val_mse: 9195465.0000 - val_mae: 3032.3965\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 9167363.0000 - mse: 9167363.0000 - mae: 3027.7515 - val_loss: 9162849.0000 - val_mse: 9162849.0000 - val_mae: 3027.0137\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 9134820.0000 - mse: 9134820.0000 - mae: 3022.3730 - val_loss: 9130350.0000 - val_mse: 9130350.0000 - val_mae: 3021.6406\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 9102395.0000 - mse: 9102395.0000 - mae: 3017.0037 - val_loss: 9097964.0000 - val_mse: 9097964.0000 - val_mae: 3016.2769\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 9070085.0000 - mse: 9070085.0000 - mae: 3011.6438 - val_loss: 9065700.0000 - val_mse: 9065700.0000 - val_mae: 3010.9236\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 9037891.0000 - mse: 9037891.0000 - mae: 3006.2942 - val_loss: 9033544.0000 - val_mse: 9033544.0000 - val_mae: 3005.5793\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 9005810.0000 - mse: 9005810.0000 - mae: 3000.9536 - val_loss: 9001504.0000 - val_mse: 9001504.0000 - val_mae: 3000.2439\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 8973841.0000 - mse: 8973841.0000 - mae: 2995.6223 - val_loss: 8969577.0000 - val_mse: 8969577.0000 - val_mae: 2994.9189\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8941988.0000 - mse: 8941988.0000 - mae: 2990.3015 - val_loss: 8937765.0000 - val_mse: 8937765.0000 - val_mae: 2989.6030\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8910248.0000 - mse: 8910248.0000 - mae: 2984.9895 - val_loss: 8906066.0000 - val_mse: 8906066.0000 - val_mae: 2984.2969\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8878622.0000 - mse: 8878622.0000 - mae: 2979.6875 - val_loss: 8874476.0000 - val_mse: 8874476.0000 - val_mae: 2978.9995\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8847102.0000 - mse: 8847102.0000 - mae: 2974.3933 - val_loss: 8843000.0000 - val_mse: 8843000.0000 - val_mae: 2973.7119\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8815696.0000 - mse: 8815696.0000 - mae: 2969.1096 - val_loss: 8811632.0000 - val_mse: 8811632.0000 - val_mae: 2968.4329\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8784401.0000 - mse: 8784401.0000 - mae: 2963.8350 - val_loss: 8780377.0000 - val_mse: 8780377.0000 - val_mae: 2963.1638\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8753220.0000 - mse: 8753220.0000 - mae: 2958.5696 - val_loss: 8749238.0000 - val_mse: 8749238.0000 - val_mae: 2957.9048\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8722151.0000 - mse: 8722151.0000 - mae: 2953.3142 - val_loss: 8718206.0000 - val_mse: 8718206.0000 - val_mae: 2952.6543\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8691190.0000 - mse: 8691190.0000 - mae: 2948.0681 - val_loss: 8687284.0000 - val_mse: 8687284.0000 - val_mae: 2947.4131\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8660337.0000 - mse: 8660337.0000 - mae: 2942.8303 - val_loss: 8656468.0000 - val_mse: 8656468.0000 - val_mae: 2942.1812\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8629592.0000 - mse: 8629592.0000 - mae: 2937.6025 - val_loss: 8625764.0000 - val_mse: 8625764.0000 - val_mae: 2936.9583\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8598957.0000 - mse: 8598957.0000 - mae: 2932.3838 - val_loss: 8595168.0000 - val_mse: 8595168.0000 - val_mae: 2931.7454\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8568432.0000 - mse: 8568432.0000 - mae: 2927.1738 - val_loss: 8564683.0000 - val_mse: 8564683.0000 - val_mae: 2926.5413\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8538017.0000 - mse: 8538017.0000 - mae: 2921.9734 - val_loss: 8534302.0000 - val_mse: 8534302.0000 - val_mae: 2921.3467\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8507705.0000 - mse: 8507705.0000 - mae: 2916.7822 - val_loss: 8504031.0000 - val_mse: 8504031.0000 - val_mae: 2916.1609\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8477503.0000 - mse: 8477503.0000 - mae: 2911.6006 - val_loss: 8473868.0000 - val_mse: 8473868.0000 - val_mae: 2910.9844\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8447411.0000 - mse: 8447411.0000 - mae: 2906.4285 - val_loss: 8443816.0000 - val_mse: 8443816.0000 - val_mae: 2905.8176\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8417428.0000 - mse: 8417428.0000 - mae: 2901.2664 - val_loss: 8413868.0000 - val_mse: 8413868.0000 - val_mae: 2900.6599\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8387549.0000 - mse: 8387549.0000 - mae: 2896.1116 - val_loss: 8384026.5000 - val_mse: 8384026.5000 - val_mae: 2895.5115\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8357776.0000 - mse: 8357776.0000 - mae: 2890.9678 - val_loss: 8354291.0000 - val_mse: 8354291.0000 - val_mae: 2890.3721\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8328108.0000 - mse: 8328108.0000 - mae: 2885.8313 - val_loss: 8324660.5000 - val_mse: 8324660.5000 - val_mae: 2885.2419\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8298547.0000 - mse: 8298547.0000 - mae: 2880.7053 - val_loss: 8295135.0000 - val_mse: 8295135.0000 - val_mae: 2880.1208\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8269089.5000 - mse: 8269089.5000 - mae: 2875.5879 - val_loss: 8265716.5000 - val_mse: 8265716.5000 - val_mae: 2875.0093\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8239738.5000 - mse: 8239738.5000 - mae: 2870.4795 - val_loss: 8236400.5000 - val_mse: 8236400.5000 - val_mae: 2869.9062\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8210490.0000 - mse: 8210490.0000 - mae: 2865.3801 - val_loss: 8207187.5000 - val_mse: 8207187.5000 - val_mae: 2864.8123\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 8181344.5000 - mse: 8181344.5000 - mae: 2860.2903 - val_loss: 8178080.0000 - val_mse: 8178080.0000 - val_mae: 2859.7278\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8152304.5000 - mse: 8152304.5000 - mae: 2855.2097 - val_loss: 8149075.5000 - val_mse: 8149075.5000 - val_mae: 2854.6519\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8123367.0000 - mse: 8123367.0000 - mae: 2850.1370 - val_loss: 8120173.5000 - val_mse: 8120173.5000 - val_mae: 2849.5850\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8094530.5000 - mse: 8094530.5000 - mae: 2845.0742 - val_loss: 8091373.0000 - val_mse: 8091373.0000 - val_mae: 2844.5271\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8065798.5000 - mse: 8065798.5000 - mae: 2840.0205 - val_loss: 8062674.0000 - val_mse: 8062674.0000 - val_mae: 2839.4780\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8037166.5000 - mse: 8037166.5000 - mae: 2834.9744 - val_loss: 8034080.0000 - val_mse: 8034080.0000 - val_mae: 2834.4387\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8008637.0000 - mse: 8008637.0000 - mae: 2829.9385 - val_loss: 8005584.5000 - val_mse: 8005584.5000 - val_mae: 2829.4075\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 7980209.0000 - mse: 7980209.0000 - mae: 2824.9114 - val_loss: 7977194.0000 - val_mse: 7977194.0000 - val_mae: 2824.3860\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 7951883.0000 - mse: 7951883.0000 - mae: 2819.8931 - val_loss: 7948900.0000 - val_mse: 7948900.0000 - val_mae: 2819.3728\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 7923654.5000 - mse: 7923654.5000 - mae: 2814.8833 - val_loss: 7920706.5000 - val_mse: 7920706.5000 - val_mae: 2814.3684\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 7895526.0000 - mse: 7895526.0000 - mae: 2809.8833 - val_loss: 7892612.5000 - val_mse: 7892612.5000 - val_mae: 2809.3728\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 7867499.5000 - mse: 7867499.5000 - mae: 2804.8904 - val_loss: 7864621.5000 - val_mse: 7864621.5000 - val_mae: 2804.3865\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 7839574.0000 - mse: 7839574.0000 - mae: 2799.9087 - val_loss: 7836730.0000 - val_mse: 7836730.0000 - val_mae: 2799.4094\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 7811746.5000 - mse: 7811746.5000 - mae: 2794.9351 - val_loss: 7808935.5000 - val_mse: 7808935.5000 - val_mae: 2794.4407\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 7784016.5000 - mse: 7784016.5000 - mae: 2789.9697 - val_loss: 7781239.5000 - val_mse: 7781239.5000 - val_mae: 2789.4802\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 7756385.5000 - mse: 7756385.5000 - mae: 2785.0134 - val_loss: 7753641.5000 - val_mse: 7753641.5000 - val_mae: 2784.5293\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 7728851.5000 - mse: 7728851.5000 - mae: 2780.0657 - val_loss: 7726141.5000 - val_mse: 7726141.5000 - val_mae: 2779.5872\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 7701415.0000 - mse: 7701415.0000 - mae: 2775.1265 - val_loss: 7698740.0000 - val_mse: 7698740.0000 - val_mae: 2774.6533\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 7674078.5000 - mse: 7674078.5000 - mae: 2770.1978 - val_loss: 7671437.0000 - val_mse: 7671437.0000 - val_mae: 2769.7292\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 7646839.5000 - mse: 7646839.5000 - mae: 2765.2764 - val_loss: 7644229.5000 - val_mse: 7644229.5000 - val_mae: 2764.8135\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 7619694.5000 - mse: 7619694.5000 - mae: 2760.3633 - val_loss: 7617120.0000 - val_mse: 7617120.0000 - val_mae: 2759.9060\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 7592649.0000 - mse: 7592649.0000 - mae: 2755.4612 - val_loss: 7590104.5000 - val_mse: 7590104.5000 - val_mae: 2755.0073\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 7565696.0000 - mse: 7565696.0000 - mae: 2750.5657 - val_loss: 7563186.0000 - val_mse: 7563186.0000 - val_mae: 2750.1182\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 7538842.5000 - mse: 7538842.5000 - mae: 2745.6792 - val_loss: 7536363.5000 - val_mse: 7536363.5000 - val_mae: 2745.2373\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.1, model__n_hidden=2, model__n_neurons=25, model__optimizer=momentum; total time=   5.4s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 23ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 10: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.1, model__n_hidden=2, model__n_neurons=25, model__optimizer=momentum; total time=   1.2s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 21ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n",
      "Epoch 10: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.1, model__n_hidden=2, model__n_neurons=25, model__optimizer=momentum; total time=   1.1s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/kamilbernacik/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [            nan -7.37309438e+01 -1.85126615e+00  1.45163753e-01\n",
      "             nan  1.41684735e-01 -5.48433914e+13             nan\n",
      " -7.57775935e+02             nan             nan  1.91538873e-01\n",
      " -3.95002929e+02             nan -3.43542130e+00 -2.36309617e+00\n",
      "             nan -1.59744704e+00 -7.62859782e-01  2.45315315e-01\n",
      "             nan  2.42644581e-01             nan -5.12627218e+02\n",
      " -3.13799408e+00  3.56656287e-01 -3.49198833e+00 -2.13936669e+01\n",
      "             nan             nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 1s 15ms/step - loss: 1156.2904 - mse: 1156.2904 - mae: 31.1206 - val_loss: 191.0138 - val_mse: 191.0138 - val_mae: 12.3603\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 157.8184 - mse: 157.8184 - mae: 9.8982 - val_loss: 131.9712 - val_mse: 131.9712 - val_mae: 8.8789\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 141.0938 - mse: 141.0938 - mae: 9.7339 - val_loss: 50.9592 - val_mse: 50.9592 - val_mae: 6.0747\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 76.8893 - mse: 76.8893 - mae: 6.3340 - val_loss: 26.7590 - val_mse: 26.7590 - val_mae: 4.0263\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 75.2245 - mse: 75.2245 - mae: 5.9670 - val_loss: 25.7716 - val_mse: 25.7716 - val_mae: 4.1436\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 71.2877 - mse: 71.2877 - mae: 6.1335 - val_loss: 29.7247 - val_mse: 29.7247 - val_mae: 4.6513\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 68.8995 - mse: 68.8995 - mae: 6.0129 - val_loss: 24.4968 - val_mse: 24.4968 - val_mae: 4.1644\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 67.4380 - mse: 67.4380 - mae: 5.7497 - val_loss: 24.2103 - val_mse: 24.2103 - val_mae: 4.1675\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 66.5409 - mse: 66.5409 - mae: 5.8179 - val_loss: 24.8964 - val_mse: 24.8964 - val_mae: 4.2694\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 65.8332 - mse: 65.8332 - mae: 5.7493 - val_loss: 23.4170 - val_mse: 23.4170 - val_mae: 4.0756\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 65.5539 - mse: 65.5539 - mae: 5.7757 - val_loss: 24.3236 - val_mse: 24.3236 - val_mae: 4.1940\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 64.8307 - mse: 64.8307 - mae: 5.7030 - val_loss: 23.1998 - val_mse: 23.1998 - val_mae: 4.0475\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 64.2358 - mse: 64.2358 - mae: 5.6486 - val_loss: 23.8018 - val_mse: 23.8018 - val_mae: 4.1311\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 63.8986 - mse: 63.8986 - mae: 5.7638 - val_loss: 23.8878 - val_mse: 23.8878 - val_mae: 4.1560\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 63.2328 - mse: 63.2328 - mae: 5.5972 - val_loss: 22.6416 - val_mse: 22.6416 - val_mae: 3.9641\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 62.8763 - mse: 62.8763 - mae: 5.5237 - val_loss: 22.8413 - val_mse: 22.8413 - val_mae: 4.0268\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 62.5878 - mse: 62.5878 - mae: 5.4919 - val_loss: 22.9078 - val_mse: 22.9078 - val_mae: 4.0484\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 62.5280 - mse: 62.5280 - mae: 5.7505 - val_loss: 23.5262 - val_mse: 23.5262 - val_mae: 4.1359\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 61.5553 - mse: 61.5553 - mae: 5.4325 - val_loss: 21.7946 - val_mse: 21.7946 - val_mae: 3.8611\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 61.2872 - mse: 61.2872 - mae: 5.4900 - val_loss: 22.8508 - val_mse: 22.8508 - val_mae: 4.0111\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 61.7337 - mse: 61.7337 - mae: 5.4284 - val_loss: 22.2464 - val_mse: 22.2464 - val_mae: 3.9183\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 60.4917 - mse: 60.4917 - mae: 5.6084 - val_loss: 23.7868 - val_mse: 23.7868 - val_mae: 4.1575\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 60.4095 - mse: 60.4095 - mae: 5.4208 - val_loss: 21.9655 - val_mse: 21.9655 - val_mae: 3.8953\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 59.7664 - mse: 59.7664 - mae: 5.3719 - val_loss: 22.0104 - val_mse: 22.0104 - val_mae: 3.9169\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 59.5561 - mse: 59.5561 - mae: 5.3875 - val_loss: 22.5540 - val_mse: 22.5540 - val_mae: 4.0128\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 58.9580 - mse: 58.9580 - mae: 5.4240 - val_loss: 21.5474 - val_mse: 21.5474 - val_mae: 3.8934\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 59.4655 - mse: 59.4655 - mae: 5.1939 - val_loss: 22.4458 - val_mse: 22.4458 - val_mae: 4.0196\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 58.5047 - mse: 58.5047 - mae: 5.3535 - val_loss: 21.5310 - val_mse: 21.5310 - val_mae: 3.9204\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 58.2592 - mse: 58.2592 - mae: 5.2729 - val_loss: 21.8134 - val_mse: 21.8134 - val_mae: 3.9414\n",
      "Epoch 29: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=KerasRegressor(callbacks=[<keras.callbacks.EarlyStopping object at 0x7f8ea1a3d6d0>], model=<function build_model at 0x7f8eab1efca0>),\n",
       "                   n_iter=30,\n",
       "                   param_distributions={'model__learning_rate': [1e-06, 1e-05,\n",
       "                                                                 0.0001],\n",
       "                                        'model__momentum': [0.1, 0.5, 0.9],\n",
       "                                        'model__n_hidden': [0, 1, 2, 3],\n",
       "                                        'model__n_neurons': [5, 25, 125],\n",
       "                                        'model__optimizer': ['sgd', 'nesterov',\n",
       "                                                             'momentum',\n",
       "                                                             'adam']},\n",
       "                   verbose=2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg,\n",
    "                                   param_distribs,\n",
    "                                   n_iter=30,\n",
    "                                   cv=3,\n",
    "                                   verbose=2)\n",
    "rnd_search_cv.fit(X_train, y_train, epochs=100, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd75a27-d0e4-4d12-9faa-91b5870e8f81",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Best params i zapis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd513537-21df-4a09-9720-81d0e5f16f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model__optimizer': 'adam', 'model__n_neurons': 125, 'model__n_hidden': 3, 'model__momentum': 0.1, 'model__learning_rate': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "print(rnd_search_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "267c29c3-5773-418c-9fdf-83ed22ba08af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model__optimizer': 'adam', 'model__n_neurons': 125, 'model__n_hidden': 3, 'model__momentum': 0.1, 'model__learning_rate': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('rnd_search.pkl','wb') as f: pickle.dump(rnd_search_cv.best_params_, f)\n",
    "\n",
    "#sprawdzenie zawartosci pliku\n",
    "with open('rnd_search.pkl','rb') as f: print(pickle.load(f))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
